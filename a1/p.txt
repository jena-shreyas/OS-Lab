nmt-master/nmt/attention_model.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/attention_model.py:2:#
nmt-master/nmt/attention_model.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/attention_model.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/attention_model.py:5:# You may obtain a copy of the License at
nmt-master/nmt/attention_model.py:6:#
nmt-master/nmt/attention_model.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/attention_model.py:8:#
nmt-master/nmt/attention_model.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/attention_model.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/attention_model.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/attention_model.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/attention_model.py:13:# limitations under the License.
nmt-master/nmt/attention_model.py:14:# ==============================================================================
nmt-master/nmt/attention_model.py:15:"""Attention-based sequence-to-sequence model with dynamic RNN support."""
nmt-master/nmt/attention_model.py:16:from __future__ import absolute_import
nmt-master/nmt/attention_model.py:17:from __future__ import division
nmt-master/nmt/attention_model.py:18:from __future__ import print_function
nmt-master/nmt/attention_model.py:20:import tensorflow as tf
nmt-master/nmt/attention_model.py:22:from . import model
nmt-master/nmt/attention_model.py:23:from . import model_helper
nmt-master/nmt/attention_model.py:25:__all__ = ["AttentionModel"]
nmt-master/nmt/attention_model.py:28:class AttentionModel(model.Model):
nmt-master/nmt/attention_model.py:29:  """Sequence-to-sequence dynamic model with attention.
nmt-master/nmt/attention_model.py:31:  This class implements a multi-layer recurrent neural network as encoder,
nmt-master/nmt/attention_model.py:32:  and an attention-based decoder. This is the same as the model described in
nmt-master/nmt/attention_model.py:33:  (Luong et al., EMNLP'2015) paper: https://arxiv.org/pdf/1508.04025v5.pdf.
nmt-master/nmt/attention_model.py:34:  This class also allows to use GRU cells in addition to LSTM cells with
nmt-master/nmt/attention_model.py:35:  support for dropout.
nmt-master/nmt/attention_model.py:36:  """
nmt-master/nmt/attention_model.py:38:  def __init__(self,
nmt-master/nmt/attention_model.py:39:               hparams,
nmt-master/nmt/attention_model.py:40:               mode,
nmt-master/nmt/attention_model.py:41:               iterator,
nmt-master/nmt/attention_model.py:42:               source_vocab_table,
nmt-master/nmt/attention_model.py:43:               target_vocab_table,
nmt-master/nmt/attention_model.py:44:               reverse_target_vocab_table=None,
nmt-master/nmt/attention_model.py:45:               scope=None,
nmt-master/nmt/attention_model.py:46:               extra_args=None):
nmt-master/nmt/attention_model.py:47:    self.has_attention = hparams.attention_architecture and hparams.attention
nmt-master/nmt/attention_model.py:49:    # Set attention_mechanism_fn
nmt-master/nmt/attention_model.py:50:    if self.has_attention:
nmt-master/nmt/attention_model.py:51:      if extra_args and extra_args.attention_mechanism_fn:
nmt-master/nmt/attention_model.py:52:        self.attention_mechanism_fn = extra_args.attention_mechanism_fn
nmt-master/nmt/attention_model.py:53:      else:
nmt-master/nmt/attention_model.py:54:        self.attention_mechanism_fn = create_attention_mechanism
nmt-master/nmt/attention_model.py:56:    super(AttentionModel, self).__init__(
nmt-master/nmt/attention_model.py:57:        hparams=hparams,
nmt-master/nmt/attention_model.py:58:        mode=mode,
nmt-master/nmt/attention_model.py:59:        iterator=iterator,
nmt-master/nmt/attention_model.py:60:        source_vocab_table=source_vocab_table,
nmt-master/nmt/attention_model.py:61:        target_vocab_table=target_vocab_table,
nmt-master/nmt/attention_model.py:62:        reverse_target_vocab_table=reverse_target_vocab_table,
nmt-master/nmt/attention_model.py:63:        scope=scope,
nmt-master/nmt/attention_model.py:64:        extra_args=extra_args)
nmt-master/nmt/attention_model.py:66:  def _prepare_beam_search_decoder_inputs(
nmt-master/nmt/attention_model.py:67:      self, beam_width, memory, source_sequence_length, encoder_state):
nmt-master/nmt/attention_model.py:68:    memory = tf.contrib.seq2seq.tile_batch(
nmt-master/nmt/attention_model.py:69:        memory, multiplier=beam_width)
nmt-master/nmt/attention_model.py:70:    source_sequence_length = tf.contrib.seq2seq.tile_batch(
nmt-master/nmt/attention_model.py:71:        source_sequence_length, multiplier=beam_width)
nmt-master/nmt/attention_model.py:72:    encoder_state = tf.contrib.seq2seq.tile_batch(
nmt-master/nmt/attention_model.py:73:        encoder_state, multiplier=beam_width)
nmt-master/nmt/attention_model.py:74:    batch_size = self.batch_size * beam_width
nmt-master/nmt/attention_model.py:75:    return memory, source_sequence_length, encoder_state, batch_size
nmt-master/nmt/attention_model.py:77:  def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state,
nmt-master/nmt/attention_model.py:78:                          source_sequence_length):
nmt-master/nmt/attention_model.py:79:    """Build a RNN cell with attention mechanism that can be used by decoder."""
nmt-master/nmt/attention_model.py:80:    # No Attention
nmt-master/nmt/attention_model.py:81:    if not self.has_attention:
nmt-master/nmt/attention_model.py:82:      return super(AttentionModel, self)._build_decoder_cell(
nmt-master/nmt/attention_model.py:83:          hparams, encoder_outputs, encoder_state, source_sequence_length)
nmt-master/nmt/attention_model.py:84:    elif hparams.attention_architecture != "standard":
nmt-master/nmt/attention_model.py:85:      raise ValueError(
nmt-master/nmt/attention_model.py:86:          "Unknown attention architecture %s" % hparams.attention_architecture)
nmt-master/nmt/attention_model.py:88:    num_units = hparams.num_units
nmt-master/nmt/attention_model.py:89:    num_layers = self.num_decoder_layers
nmt-master/nmt/attention_model.py:90:    num_residual_layers = self.num_decoder_residual_layers
nmt-master/nmt/attention_model.py:91:    infer_mode = hparams.infer_mode
nmt-master/nmt/attention_model.py:93:    dtype = tf.float32
nmt-master/nmt/attention_model.py:95:    # Ensure memory is batch-major
nmt-master/nmt/attention_model.py:96:    if self.time_major:
nmt-master/nmt/attention_model.py:97:      memory = tf.transpose(encoder_outputs, [1, 0, 2])
nmt-master/nmt/attention_model.py:98:    else:
nmt-master/nmt/attention_model.py:99:      memory = encoder_outputs
nmt-master/nmt/attention_model.py:101:    if (self.mode == tf.contrib.learn.ModeKeys.INFER and
nmt-master/nmt/attention_model.py:102:        infer_mode == "beam_search"):
nmt-master/nmt/attention_model.py:103:      memory, source_sequence_length, encoder_state, batch_size = (
nmt-master/nmt/attention_model.py:104:          self._prepare_beam_search_decoder_inputs(
nmt-master/nmt/attention_model.py:105:              hparams.beam_width, memory, source_sequence_length,
nmt-master/nmt/attention_model.py:106:              encoder_state))
nmt-master/nmt/attention_model.py:107:    else:
nmt-master/nmt/attention_model.py:108:      batch_size = self.batch_size
nmt-master/nmt/attention_model.py:110:    # Attention
nmt-master/nmt/attention_model.py:111:    attention_mechanism = self.attention_mechanism_fn(
nmt-master/nmt/attention_model.py:112:        hparams.attention, num_units, memory, source_sequence_length, self.mode)
nmt-master/nmt/attention_model.py:114:    cell = model_helper.create_rnn_cell(
nmt-master/nmt/attention_model.py:115:        unit_type=hparams.unit_type,
nmt-master/nmt/attention_model.py:116:        num_units=num_units,
nmt-master/nmt/attention_model.py:117:        num_layers=num_layers,
nmt-master/nmt/attention_model.py:118:        num_residual_layers=num_residual_layers,
nmt-master/nmt/attention_model.py:119:        forget_bias=hparams.forget_bias,
nmt-master/nmt/attention_model.py:120:        dropout=hparams.dropout,
nmt-master/nmt/attention_model.py:121:        num_gpus=self.num_gpus,
nmt-master/nmt/attention_model.py:122:        mode=self.mode,
nmt-master/nmt/attention_model.py:123:        single_cell_fn=self.single_cell_fn)
nmt-master/nmt/attention_model.py:125:    # Only generate alignment in greedy INFER mode.
nmt-master/nmt/attention_model.py:126:    alignment_history = (self.mode == tf.contrib.learn.ModeKeys.INFER and
nmt-master/nmt/attention_model.py:127:                         infer_mode != "beam_search")
nmt-master/nmt/attention_model.py:128:    cell = tf.contrib.seq2seq.AttentionWrapper(
nmt-master/nmt/attention_model.py:129:        cell,
nmt-master/nmt/attention_model.py:130:        attention_mechanism,
nmt-master/nmt/attention_model.py:131:        attention_layer_size=num_units,
nmt-master/nmt/attention_model.py:132:        alignment_history=alignment_history,
nmt-master/nmt/attention_model.py:133:        output_attention=hparams.output_attention,
nmt-master/nmt/attention_model.py:134:        name="attention")
nmt-master/nmt/attention_model.py:136:    # TODO(thangluong): do we need num_layers, num_gpus?
nmt-master/nmt/attention_model.py:137:    cell = tf.contrib.rnn.DeviceWrapper(cell,
nmt-master/nmt/attention_model.py:138:                                        model_helper.get_device_str(
nmt-master/nmt/attention_model.py:139:                                            num_layers - 1, self.num_gpus))
nmt-master/nmt/attention_model.py:141:    if hparams.pass_hidden_state:
nmt-master/nmt/attention_model.py:142:      decoder_initial_state = cell.zero_state(batch_size, dtype).clone(
nmt-master/nmt/attention_model.py:143:          cell_state=encoder_state)
nmt-master/nmt/attention_model.py:144:    else:
nmt-master/nmt/attention_model.py:145:      decoder_initial_state = cell.zero_state(batch_size, dtype)
nmt-master/nmt/attention_model.py:147:    return cell, decoder_initial_state
nmt-master/nmt/attention_model.py:149:  def _get_infer_summary(self, hparams):
nmt-master/nmt/attention_model.py:150:    if not self.has_attention or hparams.infer_mode == "beam_search":
nmt-master/nmt/attention_model.py:151:      return tf.no_op()
nmt-master/nmt/attention_model.py:152:    return _create_attention_images_summary(self.final_context_state)
nmt-master/nmt/attention_model.py:155:def create_attention_mechanism(attention_option, num_units, memory,
nmt-master/nmt/attention_model.py:156:                               source_sequence_length, mode):
nmt-master/nmt/attention_model.py:157:  """Create attention mechanism based on the attention_option."""
nmt-master/nmt/attention_model.py:158:  del mode  # unused
nmt-master/nmt/attention_model.py:160:  # Mechanism
nmt-master/nmt/attention_model.py:161:  if attention_option == "luong":
nmt-master/nmt/attention_model.py:162:    attention_mechanism = tf.contrib.seq2seq.LuongAttention(
nmt-master/nmt/attention_model.py:163:        num_units, memory, memory_sequence_length=source_sequence_length)
nmt-master/nmt/attention_model.py:164:  elif attention_option == "scaled_luong":
nmt-master/nmt/attention_model.py:165:    attention_mechanism = tf.contrib.seq2seq.LuongAttention(
nmt-master/nmt/attention_model.py:166:        num_units,
nmt-master/nmt/attention_model.py:167:        memory,
nmt-master/nmt/attention_model.py:168:        memory_sequence_length=source_sequence_length,
nmt-master/nmt/attention_model.py:169:        scale=True)
nmt-master/nmt/attention_model.py:170:  elif attention_option == "bahdanau":
nmt-master/nmt/attention_model.py:171:    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(
nmt-master/nmt/attention_model.py:172:        num_units, memory, memory_sequence_length=source_sequence_length)
nmt-master/nmt/attention_model.py:173:  elif attention_option == "normed_bahdanau":
nmt-master/nmt/attention_model.py:174:    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(
nmt-master/nmt/attention_model.py:175:        num_units,
nmt-master/nmt/attention_model.py:176:        memory,
nmt-master/nmt/attention_model.py:177:        memory_sequence_length=source_sequence_length,
nmt-master/nmt/attention_model.py:178:        normalize=True)
nmt-master/nmt/attention_model.py:179:  else:
nmt-master/nmt/attention_model.py:180:    raise ValueError("Unknown attention option %s" % attention_option)
nmt-master/nmt/attention_model.py:182:  return attention_mechanism
nmt-master/nmt/attention_model.py:185:def _create_attention_images_summary(final_context_state):
nmt-master/nmt/attention_model.py:186:  """create attention image and attention summary."""
nmt-master/nmt/attention_model.py:187:  attention_images = (final_context_state.alignment_history.stack())
nmt-master/nmt/attention_model.py:188:  # Reshape to (batch, src_seq_len, tgt_seq_len,1)
nmt-master/nmt/attention_model.py:189:  attention_images = tf.expand_dims(
nmt-master/nmt/attention_model.py:190:      tf.transpose(attention_images, [1, 2, 0]), -1)
nmt-master/nmt/attention_model.py:191:  # Scale to range [0, 255]
nmt-master/nmt/attention_model.py:192:  attention_images *= 255
nmt-master/nmt/attention_model.py:193:  attention_summary = tf.summary.image("attention_images", attention_images)
nmt-master/nmt/attention_model.py:194:  return attention_summary
nmt-master/nmt/gnmt_model.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/gnmt_model.py:2:#
nmt-master/nmt/gnmt_model.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/gnmt_model.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/gnmt_model.py:5:# You may obtain a copy of the License at
nmt-master/nmt/gnmt_model.py:6:#
nmt-master/nmt/gnmt_model.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/gnmt_model.py:8:#
nmt-master/nmt/gnmt_model.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/gnmt_model.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/gnmt_model.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/gnmt_model.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/gnmt_model.py:13:# limitations under the License.
nmt-master/nmt/gnmt_model.py:14:# ==============================================================================
nmt-master/nmt/gnmt_model.py:16:"""GNMT attention sequence-to-sequence model with dynamic RNN support."""
nmt-master/nmt/gnmt_model.py:17:from __future__ import absolute_import
nmt-master/nmt/gnmt_model.py:18:from __future__ import division
nmt-master/nmt/gnmt_model.py:19:from __future__ import print_function
nmt-master/nmt/gnmt_model.py:21:import tensorflow as tf
nmt-master/nmt/gnmt_model.py:23:from . import attention_model
nmt-master/nmt/gnmt_model.py:24:from . import model_helper
nmt-master/nmt/gnmt_model.py:25:from .utils import misc_utils as utils
nmt-master/nmt/gnmt_model.py:26:from .utils import vocab_utils
nmt-master/nmt/gnmt_model.py:28:__all__ = ["GNMTModel"]
nmt-master/nmt/gnmt_model.py:31:class GNMTModel(attention_model.AttentionModel):
nmt-master/nmt/gnmt_model.py:32:  """Sequence-to-sequence dynamic model with GNMT attention architecture.
nmt-master/nmt/gnmt_model.py:33:  """
nmt-master/nmt/gnmt_model.py:35:  def __init__(self,
nmt-master/nmt/gnmt_model.py:36:               hparams,
nmt-master/nmt/gnmt_model.py:37:               mode,
nmt-master/nmt/gnmt_model.py:38:               iterator,
nmt-master/nmt/gnmt_model.py:39:               source_vocab_table,
nmt-master/nmt/gnmt_model.py:40:               target_vocab_table,
nmt-master/nmt/gnmt_model.py:41:               reverse_target_vocab_table=None,
nmt-master/nmt/gnmt_model.py:42:               scope=None,
nmt-master/nmt/gnmt_model.py:43:               extra_args=None):
nmt-master/nmt/gnmt_model.py:44:    self.is_gnmt_attention = (
nmt-master/nmt/gnmt_model.py:45:        hparams.attention_architecture in ["gnmt", "gnmt_v2"])
nmt-master/nmt/gnmt_model.py:47:    super(GNMTModel, self).__init__(
nmt-master/nmt/gnmt_model.py:48:        hparams=hparams,
nmt-master/nmt/gnmt_model.py:49:        mode=mode,
nmt-master/nmt/gnmt_model.py:50:        iterator=iterator,
nmt-master/nmt/gnmt_model.py:51:        source_vocab_table=source_vocab_table,
nmt-master/nmt/gnmt_model.py:52:        target_vocab_table=target_vocab_table,
nmt-master/nmt/gnmt_model.py:53:        reverse_target_vocab_table=reverse_target_vocab_table,
nmt-master/nmt/gnmt_model.py:54:        scope=scope,
nmt-master/nmt/gnmt_model.py:55:        extra_args=extra_args)
nmt-master/nmt/gnmt_model.py:57:  def _build_encoder(self, hparams):
nmt-master/nmt/gnmt_model.py:58:    """Build a GNMT encoder."""
nmt-master/nmt/gnmt_model.py:59:    if hparams.encoder_type == "uni" or hparams.encoder_type == "bi":
nmt-master/nmt/gnmt_model.py:60:      return super(GNMTModel, self)._build_encoder(hparams)
nmt-master/nmt/gnmt_model.py:62:    if hparams.encoder_type != "gnmt":
nmt-master/nmt/gnmt_model.py:63:      raise ValueError("Unknown encoder_type %s" % hparams.encoder_type)
nmt-master/nmt/gnmt_model.py:65:    # Build GNMT encoder.
nmt-master/nmt/gnmt_model.py:66:    num_bi_layers = 1
nmt-master/nmt/gnmt_model.py:67:    num_uni_layers = self.num_encoder_layers - num_bi_layers
nmt-master/nmt/gnmt_model.py:68:    utils.print_out("# Build a GNMT encoder")
nmt-master/nmt/gnmt_model.py:69:    utils.print_out("  num_bi_layers = %d" % num_bi_layers)
nmt-master/nmt/gnmt_model.py:70:    utils.print_out("  num_uni_layers = %d" % num_uni_layers)
nmt-master/nmt/gnmt_model.py:72:    iterator = self.iterator
nmt-master/nmt/gnmt_model.py:73:    source = iterator.source
nmt-master/nmt/gnmt_model.py:74:    if self.time_major:
nmt-master/nmt/gnmt_model.py:75:      source = tf.transpose(source)
nmt-master/nmt/gnmt_model.py:77:    with tf.variable_scope("encoder") as scope:
nmt-master/nmt/gnmt_model.py:78:      dtype = scope.dtype
nmt-master/nmt/gnmt_model.py:80:      self.encoder_emb_inp = self.encoder_emb_lookup_fn(
nmt-master/nmt/gnmt_model.py:81:          self.embedding_encoder, source)
nmt-master/nmt/gnmt_model.py:83:      # Execute _build_bidirectional_rnn from Model class
nmt-master/nmt/gnmt_model.py:84:      bi_encoder_outputs, bi_encoder_state = self._build_bidirectional_rnn(
nmt-master/nmt/gnmt_model.py:85:          inputs=self.encoder_emb_inp,
nmt-master/nmt/gnmt_model.py:86:          sequence_length=iterator.source_sequence_length,
nmt-master/nmt/gnmt_model.py:87:          dtype=dtype,
nmt-master/nmt/gnmt_model.py:88:          hparams=hparams,
nmt-master/nmt/gnmt_model.py:89:          num_bi_layers=num_bi_layers,
nmt-master/nmt/gnmt_model.py:90:          num_bi_residual_layers=0,  # no residual connection
nmt-master/nmt/gnmt_model.py:91:      )
nmt-master/nmt/gnmt_model.py:93:      # Build unidirectional layers
nmt-master/nmt/gnmt_model.py:94:      if self.extract_encoder_layers:
nmt-master/nmt/gnmt_model.py:95:        encoder_state, encoder_outputs = self._build_individual_encoder_layers(
nmt-master/nmt/gnmt_model.py:96:            bi_encoder_outputs, num_uni_layers, dtype, hparams)
nmt-master/nmt/gnmt_model.py:97:      else:
nmt-master/nmt/gnmt_model.py:98:        encoder_state, encoder_outputs = self._build_all_encoder_layers(
nmt-master/nmt/gnmt_model.py:99:            bi_encoder_outputs, num_uni_layers, dtype, hparams)
nmt-master/nmt/gnmt_model.py:101:      # Pass all encoder states to the decoder
nmt-master/nmt/gnmt_model.py:102:      #   except the first bi-directional layer
nmt-master/nmt/gnmt_model.py:103:      encoder_state = (bi_encoder_state[1],) + (
nmt-master/nmt/gnmt_model.py:104:          (encoder_state,) if num_uni_layers == 1 else encoder_state)
nmt-master/nmt/gnmt_model.py:106:    return encoder_outputs, encoder_state
nmt-master/nmt/gnmt_model.py:108:  def _build_all_encoder_layers(self, bi_encoder_outputs,
nmt-master/nmt/gnmt_model.py:109:                                num_uni_layers, dtype, hparams):
nmt-master/nmt/gnmt_model.py:110:    """Build encoder layers all at once."""
nmt-master/nmt/gnmt_model.py:111:    uni_cell = model_helper.create_rnn_cell(
nmt-master/nmt/gnmt_model.py:112:        unit_type=hparams.unit_type,
nmt-master/nmt/gnmt_model.py:113:        num_units=hparams.num_units,
nmt-master/nmt/gnmt_model.py:114:        num_layers=num_uni_layers,
nmt-master/nmt/gnmt_model.py:115:        num_residual_layers=self.num_encoder_residual_layers,
nmt-master/nmt/gnmt_model.py:116:        forget_bias=hparams.forget_bias,
nmt-master/nmt/gnmt_model.py:117:        dropout=hparams.dropout,
nmt-master/nmt/gnmt_model.py:118:        num_gpus=self.num_gpus,
nmt-master/nmt/gnmt_model.py:119:        base_gpu=1,
nmt-master/nmt/gnmt_model.py:120:        mode=self.mode,
nmt-master/nmt/gnmt_model.py:121:        single_cell_fn=self.single_cell_fn)
nmt-master/nmt/gnmt_model.py:122:    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(
nmt-master/nmt/gnmt_model.py:123:        uni_cell,
nmt-master/nmt/gnmt_model.py:124:        bi_encoder_outputs,
nmt-master/nmt/gnmt_model.py:125:        dtype=dtype,
nmt-master/nmt/gnmt_model.py:126:        sequence_length=self.iterator.source_sequence_length,
nmt-master/nmt/gnmt_model.py:127:        time_major=self.time_major)
nmt-master/nmt/gnmt_model.py:129:    # Use the top layer for now
nmt-master/nmt/gnmt_model.py:130:    self.encoder_state_list = [encoder_outputs]
nmt-master/nmt/gnmt_model.py:132:    return encoder_state, encoder_outputs
nmt-master/nmt/gnmt_model.py:134:  def _build_individual_encoder_layers(self, bi_encoder_outputs,
nmt-master/nmt/gnmt_model.py:135:                                       num_uni_layers, dtype, hparams):
nmt-master/nmt/gnmt_model.py:136:    """Run each of the encoder layer separately, not used in general seq2seq."""
nmt-master/nmt/gnmt_model.py:137:    uni_cell_lists = model_helper._cell_list(
nmt-master/nmt/gnmt_model.py:138:        unit_type=hparams.unit_type,
nmt-master/nmt/gnmt_model.py:139:        num_units=hparams.num_units,
nmt-master/nmt/gnmt_model.py:140:        num_layers=num_uni_layers,
nmt-master/nmt/gnmt_model.py:141:        num_residual_layers=self.num_encoder_residual_layers,
nmt-master/nmt/gnmt_model.py:142:        forget_bias=hparams.forget_bias,
nmt-master/nmt/gnmt_model.py:143:        dropout=hparams.dropout,
nmt-master/nmt/gnmt_model.py:144:        num_gpus=self.num_gpus,
nmt-master/nmt/gnmt_model.py:145:        base_gpu=1,
nmt-master/nmt/gnmt_model.py:146:        mode=self.mode,
nmt-master/nmt/gnmt_model.py:147:        single_cell_fn=self.single_cell_fn)
nmt-master/nmt/gnmt_model.py:149:    encoder_inp = bi_encoder_outputs
nmt-master/nmt/gnmt_model.py:150:    encoder_states = []
nmt-master/nmt/gnmt_model.py:151:    self.encoder_state_list = [bi_encoder_outputs[:, :, :hparams.num_units],
nmt-master/nmt/gnmt_model.py:152:                               bi_encoder_outputs[:, :, hparams.num_units:]]
nmt-master/nmt/gnmt_model.py:153:    with tf.variable_scope("rnn/multi_rnn_cell"):
nmt-master/nmt/gnmt_model.py:154:      for i, cell in enumerate(uni_cell_lists):
nmt-master/nmt/gnmt_model.py:155:        with tf.variable_scope("cell_%d" % i) as scope:
nmt-master/nmt/gnmt_model.py:156:          encoder_inp, encoder_state = tf.nn.dynamic_rnn(
nmt-master/nmt/gnmt_model.py:157:              cell,
nmt-master/nmt/gnmt_model.py:158:              encoder_inp,
nmt-master/nmt/gnmt_model.py:159:              dtype=dtype,
nmt-master/nmt/gnmt_model.py:160:              sequence_length=self.iterator.source_sequence_length,
nmt-master/nmt/gnmt_model.py:161:              time_major=self.time_major,
nmt-master/nmt/gnmt_model.py:162:              scope=scope)
nmt-master/nmt/gnmt_model.py:163:          encoder_states.append(encoder_state)
nmt-master/nmt/gnmt_model.py:164:          self.encoder_state_list.append(encoder_inp)
nmt-master/nmt/gnmt_model.py:166:    encoder_state = tuple(encoder_states)
nmt-master/nmt/gnmt_model.py:167:    encoder_outputs = self.encoder_state_list[-1]
nmt-master/nmt/gnmt_model.py:168:    return encoder_state, encoder_outputs
nmt-master/nmt/gnmt_model.py:170:  def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state,
nmt-master/nmt/gnmt_model.py:171:                          source_sequence_length):
nmt-master/nmt/gnmt_model.py:172:    """Build a RNN cell with GNMT attention architecture."""
nmt-master/nmt/gnmt_model.py:173:    # Standard attention
nmt-master/nmt/gnmt_model.py:174:    if not self.is_gnmt_attention:
nmt-master/nmt/gnmt_model.py:175:      return super(GNMTModel, self)._build_decoder_cell(
nmt-master/nmt/gnmt_model.py:176:          hparams, encoder_outputs, encoder_state, source_sequence_length)
nmt-master/nmt/gnmt_model.py:178:    # GNMT attention
nmt-master/nmt/gnmt_model.py:179:    attention_option = hparams.attention
nmt-master/nmt/gnmt_model.py:180:    attention_architecture = hparams.attention_architecture
nmt-master/nmt/gnmt_model.py:181:    num_units = hparams.num_units
nmt-master/nmt/gnmt_model.py:182:    infer_mode = hparams.infer_mode
nmt-master/nmt/gnmt_model.py:184:    dtype = tf.float32
nmt-master/nmt/gnmt_model.py:186:    if self.time_major:
nmt-master/nmt/gnmt_model.py:187:      memory = tf.transpose(encoder_outputs, [1, 0, 2])
nmt-master/nmt/gnmt_model.py:188:    else:
nmt-master/nmt/gnmt_model.py:189:      memory = encoder_outputs
nmt-master/nmt/gnmt_model.py:191:    if (self.mode == tf.contrib.learn.ModeKeys.INFER and
nmt-master/nmt/gnmt_model.py:192:        infer_mode == "beam_search"):
nmt-master/nmt/gnmt_model.py:193:      memory, source_sequence_length, encoder_state, batch_size = (
nmt-master/nmt/gnmt_model.py:194:          self._prepare_beam_search_decoder_inputs(
nmt-master/nmt/gnmt_model.py:195:              hparams.beam_width, memory, source_sequence_length,
nmt-master/nmt/gnmt_model.py:196:              encoder_state))
nmt-master/nmt/gnmt_model.py:197:    else:
nmt-master/nmt/gnmt_model.py:198:      batch_size = self.batch_size
nmt-master/nmt/gnmt_model.py:200:    attention_mechanism = self.attention_mechanism_fn(
nmt-master/nmt/gnmt_model.py:201:        attention_option, num_units, memory, source_sequence_length, self.mode)
nmt-master/nmt/gnmt_model.py:203:    cell_list = model_helper._cell_list(  # pylint: disable=protected-access
nmt-master/nmt/gnmt_model.py:204:        unit_type=hparams.unit_type,
nmt-master/nmt/gnmt_model.py:205:        num_units=num_units,
nmt-master/nmt/gnmt_model.py:206:        num_layers=self.num_decoder_layers,
nmt-master/nmt/gnmt_model.py:207:        num_residual_layers=self.num_decoder_residual_layers,
nmt-master/nmt/gnmt_model.py:208:        forget_bias=hparams.forget_bias,
nmt-master/nmt/gnmt_model.py:209:        dropout=hparams.dropout,
nmt-master/nmt/gnmt_model.py:210:        num_gpus=self.num_gpus,
nmt-master/nmt/gnmt_model.py:211:        mode=self.mode,
nmt-master/nmt/gnmt_model.py:212:        single_cell_fn=self.single_cell_fn,
nmt-master/nmt/gnmt_model.py:213:        residual_fn=gnmt_residual_fn
nmt-master/nmt/gnmt_model.py:214:    )
nmt-master/nmt/gnmt_model.py:216:    # Only wrap the bottom layer with the attention mechanism.
nmt-master/nmt/gnmt_model.py:217:    attention_cell = cell_list.pop(0)
nmt-master/nmt/gnmt_model.py:219:    # Only generate alignment in greedy INFER mode.
nmt-master/nmt/gnmt_model.py:220:    alignment_history = (self.mode == tf.contrib.learn.ModeKeys.INFER and
nmt-master/nmt/gnmt_model.py:221:                         infer_mode != "beam_search")
nmt-master/nmt/gnmt_model.py:222:    attention_cell = tf.contrib.seq2seq.AttentionWrapper(
nmt-master/nmt/gnmt_model.py:223:        attention_cell,
nmt-master/nmt/gnmt_model.py:224:        attention_mechanism,
nmt-master/nmt/gnmt_model.py:225:        attention_layer_size=None,  # don't use attention layer.
nmt-master/nmt/gnmt_model.py:226:        output_attention=False,
nmt-master/nmt/gnmt_model.py:227:        alignment_history=alignment_history,
nmt-master/nmt/gnmt_model.py:228:        name="attention")
nmt-master/nmt/gnmt_model.py:230:    if attention_architecture == "gnmt":
nmt-master/nmt/gnmt_model.py:231:      cell = GNMTAttentionMultiCell(
nmt-master/nmt/gnmt_model.py:232:          attention_cell, cell_list)
nmt-master/nmt/gnmt_model.py:233:    elif attention_architecture == "gnmt_v2":
nmt-master/nmt/gnmt_model.py:234:      cell = GNMTAttentionMultiCell(
nmt-master/nmt/gnmt_model.py:235:          attention_cell, cell_list, use_new_attention=True)
nmt-master/nmt/gnmt_model.py:236:    else:
nmt-master/nmt/gnmt_model.py:237:      raise ValueError(
nmt-master/nmt/gnmt_model.py:238:          "Unknown attention_architecture %s" % attention_architecture)
nmt-master/nmt/gnmt_model.py:240:    if hparams.pass_hidden_state:
nmt-master/nmt/gnmt_model.py:241:      decoder_initial_state = tuple(
nmt-master/nmt/gnmt_model.py:242:          zs.clone(cell_state=es)
nmt-master/nmt/gnmt_model.py:243:          if isinstance(zs, tf.contrib.seq2seq.AttentionWrapperState) else es
nmt-master/nmt/gnmt_model.py:244:          for zs, es in zip(
nmt-master/nmt/gnmt_model.py:245:              cell.zero_state(batch_size, dtype), encoder_state))
nmt-master/nmt/gnmt_model.py:246:    else:
nmt-master/nmt/gnmt_model.py:247:      decoder_initial_state = cell.zero_state(batch_size, dtype)
nmt-master/nmt/gnmt_model.py:249:    return cell, decoder_initial_state
nmt-master/nmt/gnmt_model.py:251:  def _get_infer_summary(self, hparams):
nmt-master/nmt/gnmt_model.py:252:    if hparams.infer_mode == "beam_search":
nmt-master/nmt/gnmt_model.py:253:      return tf.no_op()
nmt-master/nmt/gnmt_model.py:254:    elif self.is_gnmt_attention:
nmt-master/nmt/gnmt_model.py:255:      return attention_model._create_attention_images_summary(
nmt-master/nmt/gnmt_model.py:256:          self.final_context_state[0])
nmt-master/nmt/gnmt_model.py:257:    else:
nmt-master/nmt/gnmt_model.py:258:      return super(GNMTModel, self)._get_infer_summary(hparams)
nmt-master/nmt/gnmt_model.py:261:class GNMTAttentionMultiCell(tf.nn.rnn_cell.MultiRNNCell):
nmt-master/nmt/gnmt_model.py:262:  """A MultiCell with GNMT attention style."""
nmt-master/nmt/gnmt_model.py:264:  def __init__(self, attention_cell, cells, use_new_attention=False):
nmt-master/nmt/gnmt_model.py:265:    """Creates a GNMTAttentionMultiCell.
nmt-master/nmt/gnmt_model.py:267:    Args:
nmt-master/nmt/gnmt_model.py:268:      attention_cell: An instance of AttentionWrapper.
nmt-master/nmt/gnmt_model.py:269:      cells: A list of RNNCell wrapped with AttentionInputWrapper.
nmt-master/nmt/gnmt_model.py:270:      use_new_attention: Whether to use the attention generated from current
nmt-master/nmt/gnmt_model.py:271:        step bottom layer's output. Default is False.
nmt-master/nmt/gnmt_model.py:272:    """
nmt-master/nmt/gnmt_model.py:273:    cells = [attention_cell] + cells
nmt-master/nmt/gnmt_model.py:274:    self.use_new_attention = use_new_attention
nmt-master/nmt/gnmt_model.py:275:    super(GNMTAttentionMultiCell, self).__init__(cells, state_is_tuple=True)
nmt-master/nmt/gnmt_model.py:277:  def __call__(self, inputs, state, scope=None):
nmt-master/nmt/gnmt_model.py:278:    """Run the cell with bottom layer's attention copied to all upper layers."""
nmt-master/nmt/gnmt_model.py:279:    if not tf.contrib.framework.nest.is_sequence(state):
nmt-master/nmt/gnmt_model.py:280:      raise ValueError(
nmt-master/nmt/gnmt_model.py:281:          "Expected state to be a tuple of length %d, but received: %s"
nmt-master/nmt/gnmt_model.py:282:          % (len(self.state_size), state))
nmt-master/nmt/gnmt_model.py:284:    with tf.variable_scope(scope or "multi_rnn_cell"):
nmt-master/nmt/gnmt_model.py:285:      new_states = []
nmt-master/nmt/gnmt_model.py:287:      with tf.variable_scope("cell_0_attention"):
nmt-master/nmt/gnmt_model.py:288:        attention_cell = self._cells[0]
nmt-master/nmt/gnmt_model.py:289:        attention_state = state[0]
nmt-master/nmt/gnmt_model.py:290:        cur_inp, new_attention_state = attention_cell(inputs, attention_state)
nmt-master/nmt/gnmt_model.py:291:        new_states.append(new_attention_state)
nmt-master/nmt/gnmt_model.py:293:      for i in range(1, len(self._cells)):
nmt-master/nmt/gnmt_model.py:294:        with tf.variable_scope("cell_%d" % i):
nmt-master/nmt/gnmt_model.py:296:          cell = self._cells[i]
nmt-master/nmt/gnmt_model.py:297:          cur_state = state[i]
nmt-master/nmt/gnmt_model.py:299:          if self.use_new_attention:
nmt-master/nmt/gnmt_model.py:300:            cur_inp = tf.concat([cur_inp, new_attention_state.attention], -1)
nmt-master/nmt/gnmt_model.py:301:          else:
nmt-master/nmt/gnmt_model.py:302:            cur_inp = tf.concat([cur_inp, attention_state.attention], -1)
nmt-master/nmt/gnmt_model.py:304:          cur_inp, new_state = cell(cur_inp, cur_state)
nmt-master/nmt/gnmt_model.py:305:          new_states.append(new_state)
nmt-master/nmt/gnmt_model.py:307:    return cur_inp, tuple(new_states)
nmt-master/nmt/gnmt_model.py:310:def gnmt_residual_fn(inputs, outputs):
nmt-master/nmt/gnmt_model.py:311:  """Residual function that handles different inputs and outputs inner dims.
nmt-master/nmt/gnmt_model.py:313:  Args:
nmt-master/nmt/gnmt_model.py:314:    inputs: cell inputs, this is actual inputs concatenated with the attention
nmt-master/nmt/gnmt_model.py:315:      vector.
nmt-master/nmt/gnmt_model.py:316:    outputs: cell outputs
nmt-master/nmt/gnmt_model.py:318:  Returns:
nmt-master/nmt/gnmt_model.py:319:    outputs + actual inputs
nmt-master/nmt/gnmt_model.py:320:  """
nmt-master/nmt/gnmt_model.py:321:  def split_input(inp, out):
nmt-master/nmt/gnmt_model.py:322:    out_dim = out.get_shape().as_list()[-1]
nmt-master/nmt/gnmt_model.py:323:    inp_dim = inp.get_shape().as_list()[-1]
nmt-master/nmt/gnmt_model.py:324:    return tf.split(inp, [out_dim, inp_dim - out_dim], axis=-1)
nmt-master/nmt/gnmt_model.py:325:  actual_inputs, _ = tf.contrib.framework.nest.map_structure(
nmt-master/nmt/gnmt_model.py:326:      split_input, inputs, outputs)
nmt-master/nmt/gnmt_model.py:327:  def assert_shape_match(inp, out):
nmt-master/nmt/gnmt_model.py:328:    inp.get_shape().assert_is_compatible_with(out.get_shape())
nmt-master/nmt/gnmt_model.py:329:  tf.contrib.framework.nest.assert_same_structure(actual_inputs, outputs)
nmt-master/nmt/gnmt_model.py:330:  tf.contrib.framework.nest.map_structure(
nmt-master/nmt/gnmt_model.py:331:      assert_shape_match, actual_inputs, outputs)
nmt-master/nmt/gnmt_model.py:332:  return tf.contrib.framework.nest.map_structure(
nmt-master/nmt/gnmt_model.py:333:      lambda inp, out: inp + out, actual_inputs, outputs)
nmt-master/nmt/inference.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/inference.py:2:#
nmt-master/nmt/inference.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/inference.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/inference.py:5:# You may obtain a copy of the License at
nmt-master/nmt/inference.py:6:#
nmt-master/nmt/inference.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/inference.py:8:#
nmt-master/nmt/inference.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/inference.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/inference.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/inference.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/inference.py:13:# limitations under the License.
nmt-master/nmt/inference.py:14:# ==============================================================================
nmt-master/nmt/inference.py:16:"""To perform inference on test set given a trained model."""
nmt-master/nmt/inference.py:17:from __future__ import print_function
nmt-master/nmt/inference.py:19:import codecs
nmt-master/nmt/inference.py:20:import time
nmt-master/nmt/inference.py:22:import tensorflow as tf
nmt-master/nmt/inference.py:24:from . import attention_model
nmt-master/nmt/inference.py:25:from . import gnmt_model
nmt-master/nmt/inference.py:26:from . import model as nmt_model
nmt-master/nmt/inference.py:27:from . import model_helper
nmt-master/nmt/inference.py:28:from .utils import misc_utils as utils
nmt-master/nmt/inference.py:29:from .utils import nmt_utils
nmt-master/nmt/inference.py:31:__all__ = ["load_data", "inference",
nmt-master/nmt/inference.py:32:           "single_worker_inference", "multi_worker_inference"]
nmt-master/nmt/inference.py:35:def _decode_inference_indices(model, sess, output_infer,
nmt-master/nmt/inference.py:36:                              output_infer_summary_prefix,
nmt-master/nmt/inference.py:37:                              inference_indices,
nmt-master/nmt/inference.py:38:                              tgt_eos,
nmt-master/nmt/inference.py:39:                              subword_option):
nmt-master/nmt/inference.py:40:  """Decoding only a specific set of sentences."""
nmt-master/nmt/inference.py:41:  utils.print_out("  decoding to output %s , num sents %d." %
nmt-master/nmt/inference.py:42:                  (output_infer, len(inference_indices)))
nmt-master/nmt/inference.py:43:  start_time = time.time()
nmt-master/nmt/inference.py:44:  with codecs.getwriter("utf-8")(
nmt-master/nmt/inference.py:45:      tf.gfile.GFile(output_infer, mode="wb")) as trans_f:
nmt-master/nmt/inference.py:46:    trans_f.write("")  # Write empty string to ensure file is created.
nmt-master/nmt/inference.py:47:    for decode_id in inference_indices:
nmt-master/nmt/inference.py:48:      nmt_outputs, infer_summary = model.decode(sess)
nmt-master/nmt/inference.py:50:      # get text translation
nmt-master/nmt/inference.py:51:      assert nmt_outputs.shape[0] == 1
nmt-master/nmt/inference.py:52:      translation = nmt_utils.get_translation(
nmt-master/nmt/inference.py:53:          nmt_outputs,
nmt-master/nmt/inference.py:54:          sent_id=0,
nmt-master/nmt/inference.py:55:          tgt_eos=tgt_eos,
nmt-master/nmt/inference.py:56:          subword_option=subword_option)
nmt-master/nmt/inference.py:58:      if infer_summary is not None:  # Attention models
nmt-master/nmt/inference.py:59:        image_file = output_infer_summary_prefix + str(decode_id) + ".png"
nmt-master/nmt/inference.py:60:        utils.print_out("  save attention image to %s*" % image_file)
nmt-master/nmt/inference.py:61:        image_summ = tf.Summary()
nmt-master/nmt/inference.py:62:        image_summ.ParseFromString(infer_summary)
nmt-master/nmt/inference.py:63:        with tf.gfile.GFile(image_file, mode="w") as img_f:
nmt-master/nmt/inference.py:64:          img_f.write(image_summ.value[0].image.encoded_image_string)
nmt-master/nmt/inference.py:66:      trans_f.write("%s\n" % translation)
nmt-master/nmt/inference.py:67:      utils.print_out(translation + b"\n")
nmt-master/nmt/inference.py:68:  utils.print_time("  done", start_time)
nmt-master/nmt/inference.py:71:def load_data(inference_input_file, hparams=None):
nmt-master/nmt/inference.py:72:  """Load inference data."""
nmt-master/nmt/inference.py:73:  with codecs.getreader("utf-8")(
nmt-master/nmt/inference.py:74:      tf.gfile.GFile(inference_input_file, mode="rb")) as f:
nmt-master/nmt/inference.py:75:    inference_data = f.read().splitlines()
nmt-master/nmt/inference.py:77:  if hparams and hparams.inference_indices:
nmt-master/nmt/inference.py:78:    inference_data = [inference_data[i] for i in hparams.inference_indices]
nmt-master/nmt/inference.py:80:  return inference_data
nmt-master/nmt/inference.py:83:def get_model_creator(hparams):
nmt-master/nmt/inference.py:84:  """Get the right model class depending on configuration."""
nmt-master/nmt/inference.py:85:  if (hparams.encoder_type == "gnmt" or
nmt-master/nmt/inference.py:86:      hparams.attention_architecture in ["gnmt", "gnmt_v2"]):
nmt-master/nmt/inference.py:87:    model_creator = gnmt_model.GNMTModel
nmt-master/nmt/inference.py:88:  elif hparams.attention_architecture == "standard":
nmt-master/nmt/inference.py:89:    model_creator = attention_model.AttentionModel
nmt-master/nmt/inference.py:90:  elif not hparams.attention:
nmt-master/nmt/inference.py:91:    model_creator = nmt_model.Model
nmt-master/nmt/inference.py:92:  else:
nmt-master/nmt/inference.py:93:    raise ValueError("Unknown attention architecture %s" %
nmt-master/nmt/inference.py:94:                     hparams.attention_architecture)
nmt-master/nmt/inference.py:95:  return model_creator
nmt-master/nmt/inference.py:98:def start_sess_and_load_model(infer_model, ckpt_path):
nmt-master/nmt/inference.py:99:  """Start session and load model."""
nmt-master/nmt/inference.py:100:  sess = tf.Session(
nmt-master/nmt/inference.py:101:      graph=infer_model.graph, config=utils.get_config_proto())
nmt-master/nmt/inference.py:102:  with infer_model.graph.as_default():
nmt-master/nmt/inference.py:103:    loaded_infer_model = model_helper.load_model(
nmt-master/nmt/inference.py:104:        infer_model.model, ckpt_path, sess, "infer")
nmt-master/nmt/inference.py:105:  return sess, loaded_infer_model
nmt-master/nmt/inference.py:108:def inference(ckpt_path,
nmt-master/nmt/inference.py:109:              inference_input_file,
nmt-master/nmt/inference.py:110:              inference_output_file,
nmt-master/nmt/inference.py:111:              hparams,
nmt-master/nmt/inference.py:112:              num_workers=1,
nmt-master/nmt/inference.py:113:              jobid=0,
nmt-master/nmt/inference.py:114:              scope=None):
nmt-master/nmt/inference.py:115:  """Perform translation."""
nmt-master/nmt/inference.py:116:  if hparams.inference_indices:
nmt-master/nmt/inference.py:117:    assert num_workers == 1
nmt-master/nmt/inference.py:119:  model_creator = get_model_creator(hparams)
nmt-master/nmt/inference.py:120:  infer_model = model_helper.create_infer_model(model_creator, hparams, scope)
nmt-master/nmt/inference.py:121:  sess, loaded_infer_model = start_sess_and_load_model(infer_model, ckpt_path)
nmt-master/nmt/inference.py:123:  if num_workers == 1:
nmt-master/nmt/inference.py:124:    single_worker_inference(
nmt-master/nmt/inference.py:125:        sess,
nmt-master/nmt/inference.py:126:        infer_model,
nmt-master/nmt/inference.py:127:        loaded_infer_model,
nmt-master/nmt/inference.py:128:        inference_input_file,
nmt-master/nmt/inference.py:129:        inference_output_file,
nmt-master/nmt/inference.py:130:        hparams)
nmt-master/nmt/inference.py:131:  else:
nmt-master/nmt/inference.py:132:    multi_worker_inference(
nmt-master/nmt/inference.py:133:        sess,
nmt-master/nmt/inference.py:134:        infer_model,
nmt-master/nmt/inference.py:135:        loaded_infer_model,
nmt-master/nmt/inference.py:136:        inference_input_file,
nmt-master/nmt/inference.py:137:        inference_output_file,
nmt-master/nmt/inference.py:138:        hparams,
nmt-master/nmt/inference.py:139:        num_workers=num_workers,
nmt-master/nmt/inference.py:140:        jobid=jobid)
nmt-master/nmt/inference.py:141:  sess.close()
nmt-master/nmt/inference.py:144:def single_worker_inference(sess,
nmt-master/nmt/inference.py:145:                            infer_model,
nmt-master/nmt/inference.py:146:                            loaded_infer_model,
nmt-master/nmt/inference.py:147:                            inference_input_file,
nmt-master/nmt/inference.py:148:                            inference_output_file,
nmt-master/nmt/inference.py:149:                            hparams):
nmt-master/nmt/inference.py:150:  """Inference with a single worker."""
nmt-master/nmt/inference.py:151:  output_infer = inference_output_file
nmt-master/nmt/inference.py:153:  # Read data
nmt-master/nmt/inference.py:154:  infer_data = load_data(inference_input_file, hparams)
nmt-master/nmt/inference.py:156:  with infer_model.graph.as_default():
nmt-master/nmt/inference.py:157:    sess.run(
nmt-master/nmt/inference.py:158:        infer_model.iterator.initializer,
nmt-master/nmt/inference.py:159:        feed_dict={
nmt-master/nmt/inference.py:160:            infer_model.src_placeholder: infer_data,
nmt-master/nmt/inference.py:161:            infer_model.batch_size_placeholder: hparams.infer_batch_size
nmt-master/nmt/inference.py:162:        })
nmt-master/nmt/inference.py:163:    # Decode
nmt-master/nmt/inference.py:164:    utils.print_out("# Start decoding")
nmt-master/nmt/inference.py:165:    if hparams.inference_indices:
nmt-master/nmt/inference.py:166:      _decode_inference_indices(
nmt-master/nmt/inference.py:167:          loaded_infer_model,
nmt-master/nmt/inference.py:168:          sess,
nmt-master/nmt/inference.py:169:          output_infer=output_infer,
nmt-master/nmt/inference.py:170:          output_infer_summary_prefix=output_infer,
nmt-master/nmt/inference.py:171:          inference_indices=hparams.inference_indices,
nmt-master/nmt/inference.py:172:          tgt_eos=hparams.eos,
nmt-master/nmt/inference.py:173:          subword_option=hparams.subword_option)
nmt-master/nmt/inference.py:174:    else:
nmt-master/nmt/inference.py:175:      nmt_utils.decode_and_evaluate(
nmt-master/nmt/inference.py:176:          "infer",
nmt-master/nmt/inference.py:177:          loaded_infer_model,
nmt-master/nmt/inference.py:178:          sess,
nmt-master/nmt/inference.py:179:          output_infer,
nmt-master/nmt/inference.py:180:          ref_file=None,
nmt-master/nmt/inference.py:181:          metrics=hparams.metrics,
nmt-master/nmt/inference.py:182:          subword_option=hparams.subword_option,
nmt-master/nmt/inference.py:183:          beam_width=hparams.beam_width,
nmt-master/nmt/inference.py:184:          tgt_eos=hparams.eos,
nmt-master/nmt/inference.py:185:          num_translations_per_input=hparams.num_translations_per_input,
nmt-master/nmt/inference.py:186:          infer_mode=hparams.infer_mode)
nmt-master/nmt/inference.py:189:def multi_worker_inference(sess,
nmt-master/nmt/inference.py:190:                           infer_model,
nmt-master/nmt/inference.py:191:                           loaded_infer_model,
nmt-master/nmt/inference.py:192:                           inference_input_file,
nmt-master/nmt/inference.py:193:                           inference_output_file,
nmt-master/nmt/inference.py:194:                           hparams,
nmt-master/nmt/inference.py:195:                           num_workers,
nmt-master/nmt/inference.py:196:                           jobid):
nmt-master/nmt/inference.py:197:  """Inference using multiple workers."""
nmt-master/nmt/inference.py:198:  assert num_workers > 1
nmt-master/nmt/inference.py:200:  final_output_infer = inference_output_file
nmt-master/nmt/inference.py:201:  output_infer = "%s_%d" % (inference_output_file, jobid)
nmt-master/nmt/inference.py:202:  output_infer_done = "%s_done_%d" % (inference_output_file, jobid)
nmt-master/nmt/inference.py:204:  # Read data
nmt-master/nmt/inference.py:205:  infer_data = load_data(inference_input_file, hparams)
nmt-master/nmt/inference.py:207:  # Split data to multiple workers
nmt-master/nmt/inference.py:208:  total_load = len(infer_data)
nmt-master/nmt/inference.py:209:  load_per_worker = int((total_load - 1) / num_workers) + 1
nmt-master/nmt/inference.py:210:  start_position = jobid * load_per_worker
nmt-master/nmt/inference.py:211:  end_position = min(start_position + load_per_worker, total_load)
nmt-master/nmt/inference.py:212:  infer_data = infer_data[start_position:end_position]
nmt-master/nmt/inference.py:214:  with infer_model.graph.as_default():
nmt-master/nmt/inference.py:215:    sess.run(infer_model.iterator.initializer,
nmt-master/nmt/inference.py:216:             {
nmt-master/nmt/inference.py:217:                 infer_model.src_placeholder: infer_data,
nmt-master/nmt/inference.py:218:                 infer_model.batch_size_placeholder: hparams.infer_batch_size
nmt-master/nmt/inference.py:219:             })
nmt-master/nmt/inference.py:220:    # Decode
nmt-master/nmt/inference.py:221:    utils.print_out("# Start decoding")
nmt-master/nmt/inference.py:222:    nmt_utils.decode_and_evaluate(
nmt-master/nmt/inference.py:223:        "infer",
nmt-master/nmt/inference.py:224:        loaded_infer_model,
nmt-master/nmt/inference.py:225:        sess,
nmt-master/nmt/inference.py:226:        output_infer,
nmt-master/nmt/inference.py:227:        ref_file=None,
nmt-master/nmt/inference.py:228:        metrics=hparams.metrics,
nmt-master/nmt/inference.py:229:        subword_option=hparams.subword_option,
nmt-master/nmt/inference.py:230:        beam_width=hparams.beam_width,
nmt-master/nmt/inference.py:231:        tgt_eos=hparams.eos,
nmt-master/nmt/inference.py:232:        num_translations_per_input=hparams.num_translations_per_input,
nmt-master/nmt/inference.py:233:        infer_mode=hparams.infer_mode)
nmt-master/nmt/inference.py:235:    # Change file name to indicate the file writing is completed.
nmt-master/nmt/inference.py:236:    tf.gfile.Rename(output_infer, output_infer_done, overwrite=True)
nmt-master/nmt/inference.py:238:    # Job 0 is responsible for the clean up.
nmt-master/nmt/inference.py:239:    if jobid != 0: return
nmt-master/nmt/inference.py:241:    # Now write all translations
nmt-master/nmt/inference.py:242:    with codecs.getwriter("utf-8")(
nmt-master/nmt/inference.py:243:        tf.gfile.GFile(final_output_infer, mode="wb")) as final_f:
nmt-master/nmt/inference.py:244:      for worker_id in range(num_workers):
nmt-master/nmt/inference.py:245:        worker_infer_done = "%s_done_%d" % (inference_output_file, worker_id)
nmt-master/nmt/inference.py:246:        while not tf.gfile.Exists(worker_infer_done):
nmt-master/nmt/inference.py:247:          utils.print_out("  waiting job %d to complete." % worker_id)
nmt-master/nmt/inference.py:248:          time.sleep(10)
nmt-master/nmt/inference.py:250:        with codecs.getreader("utf-8")(
nmt-master/nmt/inference.py:251:            tf.gfile.GFile(worker_infer_done, mode="rb")) as f:
nmt-master/nmt/inference.py:252:          for translation in f:
nmt-master/nmt/inference.py:253:            final_f.write("%s" % translation)
nmt-master/nmt/inference.py:255:      for worker_id in range(num_workers):
nmt-master/nmt/inference.py:256:        worker_infer_done = "%s_done_%d" % (inference_output_file, worker_id)
nmt-master/nmt/inference.py:257:        tf.gfile.Remove(worker_infer_done)
nmt-master/nmt/inference_test.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/inference_test.py:2:#
nmt-master/nmt/inference_test.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/inference_test.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/inference_test.py:5:# You may obtain a copy of the License at
nmt-master/nmt/inference_test.py:6:#
nmt-master/nmt/inference_test.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/inference_test.py:8:#
nmt-master/nmt/inference_test.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/inference_test.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/inference_test.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/inference_test.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/inference_test.py:13:# limitations under the License.
nmt-master/nmt/inference_test.py:14:# ==============================================================================
nmt-master/nmt/inference_test.py:16:"""Tests for model inference."""
nmt-master/nmt/inference_test.py:18:from __future__ import absolute_import
nmt-master/nmt/inference_test.py:19:from __future__ import division
nmt-master/nmt/inference_test.py:20:from __future__ import print_function
nmt-master/nmt/inference_test.py:22:import os
nmt-master/nmt/inference_test.py:23:import numpy as np
nmt-master/nmt/inference_test.py:24:import tensorflow as tf
nmt-master/nmt/inference_test.py:26:from . import inference
nmt-master/nmt/inference_test.py:27:from . import model_helper
nmt-master/nmt/inference_test.py:28:from .utils import common_test_utils
nmt-master/nmt/inference_test.py:30:float32 = np.float32
nmt-master/nmt/inference_test.py:31:int32 = np.int32
nmt-master/nmt/inference_test.py:32:array = np.array
nmt-master/nmt/inference_test.py:35:class InferenceTest(tf.test.TestCase):
nmt-master/nmt/inference_test.py:37:  def _createTestInferCheckpoint(self, hparams, name):
nmt-master/nmt/inference_test.py:38:    # Prepare
nmt-master/nmt/inference_test.py:39:    hparams.vocab_prefix = (
nmt-master/nmt/inference_test.py:40:        "nmt/testdata/test_infer_vocab")
nmt-master/nmt/inference_test.py:41:    hparams.src_vocab_file = hparams.vocab_prefix + "." + hparams.src
nmt-master/nmt/inference_test.py:42:    hparams.tgt_vocab_file = hparams.vocab_prefix + "." + hparams.tgt
nmt-master/nmt/inference_test.py:43:    out_dir = os.path.join(tf.test.get_temp_dir(), name)
nmt-master/nmt/inference_test.py:44:    os.makedirs(out_dir)
nmt-master/nmt/inference_test.py:45:    hparams.out_dir = out_dir
nmt-master/nmt/inference_test.py:47:    # Create check point
nmt-master/nmt/inference_test.py:48:    model_creator = inference.get_model_creator(hparams)
nmt-master/nmt/inference_test.py:49:    infer_model = model_helper.create_infer_model(model_creator, hparams)
nmt-master/nmt/inference_test.py:50:    with self.test_session(graph=infer_model.graph) as sess:
nmt-master/nmt/inference_test.py:51:      loaded_model, global_step = model_helper.create_or_load_model(
nmt-master/nmt/inference_test.py:52:          infer_model.model, out_dir, sess, "infer_name")
nmt-master/nmt/inference_test.py:53:      ckpt_path = loaded_model.saver.save(
nmt-master/nmt/inference_test.py:54:          sess, os.path.join(out_dir, "translate.ckpt"),
nmt-master/nmt/inference_test.py:55:          global_step=global_step)
nmt-master/nmt/inference_test.py:56:    return ckpt_path
nmt-master/nmt/inference_test.py:58:  def testBasicModel(self):
nmt-master/nmt/inference_test.py:59:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/inference_test.py:60:        encoder_type="uni",
nmt-master/nmt/inference_test.py:61:        num_layers=1,
nmt-master/nmt/inference_test.py:62:        attention="",
nmt-master/nmt/inference_test.py:63:        attention_architecture="",
nmt-master/nmt/inference_test.py:64:        use_residual=False,)
nmt-master/nmt/inference_test.py:65:    ckpt_path = self._createTestInferCheckpoint(hparams, "basic_infer")
nmt-master/nmt/inference_test.py:66:    infer_file = "nmt/testdata/test_infer_file"
nmt-master/nmt/inference_test.py:67:    output_infer = os.path.join(hparams.out_dir, "output_infer")
nmt-master/nmt/inference_test.py:68:    inference.inference(ckpt_path, infer_file, output_infer, hparams)
nmt-master/nmt/inference_test.py:69:    with open(output_infer) as f:
nmt-master/nmt/inference_test.py:70:      self.assertEqual(5, len(list(f)))
nmt-master/nmt/inference_test.py:72:  def testBasicModelWithMultipleTranslations(self):
nmt-master/nmt/inference_test.py:73:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/inference_test.py:74:        encoder_type="uni",
nmt-master/nmt/inference_test.py:75:        num_layers=1,
nmt-master/nmt/inference_test.py:76:        attention="",
nmt-master/nmt/inference_test.py:77:        attention_architecture="",
nmt-master/nmt/inference_test.py:78:        use_residual=False,
nmt-master/nmt/inference_test.py:79:        num_translations_per_input=2,
nmt-master/nmt/inference_test.py:80:        beam_width=2,
nmt-master/nmt/inference_test.py:81:    )
nmt-master/nmt/inference_test.py:82:    hparams.infer_mode = "beam_search"
nmt-master/nmt/inference_test.py:84:    ckpt_path = self._createTestInferCheckpoint(hparams, "multi_basic_infer")
nmt-master/nmt/inference_test.py:85:    infer_file = "nmt/testdata/test_infer_file"
nmt-master/nmt/inference_test.py:86:    output_infer = os.path.join(hparams.out_dir, "output_infer")
nmt-master/nmt/inference_test.py:87:    inference.inference(ckpt_path, infer_file, output_infer, hparams)
nmt-master/nmt/inference_test.py:88:    with open(output_infer) as f:
nmt-master/nmt/inference_test.py:89:      self.assertEqual(10, len(list(f)))
nmt-master/nmt/inference_test.py:91:  def testAttentionModel(self):
nmt-master/nmt/inference_test.py:92:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/inference_test.py:93:        encoder_type="uni",
nmt-master/nmt/inference_test.py:94:        num_layers=1,
nmt-master/nmt/inference_test.py:95:        attention="scaled_luong",
nmt-master/nmt/inference_test.py:96:        attention_architecture="standard",
nmt-master/nmt/inference_test.py:97:        use_residual=False,)
nmt-master/nmt/inference_test.py:98:    ckpt_path = self._createTestInferCheckpoint(hparams, "attention_infer")
nmt-master/nmt/inference_test.py:99:    infer_file = "nmt/testdata/test_infer_file"
nmt-master/nmt/inference_test.py:100:    output_infer = os.path.join(hparams.out_dir, "output_infer")
nmt-master/nmt/inference_test.py:101:    inference.inference(ckpt_path, infer_file, output_infer, hparams)
nmt-master/nmt/inference_test.py:102:    with open(output_infer) as f:
nmt-master/nmt/inference_test.py:103:      self.assertEqual(5, len(list(f)))
nmt-master/nmt/inference_test.py:105:  def testMultiWorkers(self):
nmt-master/nmt/inference_test.py:106:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/inference_test.py:107:        encoder_type="uni",
nmt-master/nmt/inference_test.py:108:        num_layers=2,
nmt-master/nmt/inference_test.py:109:        attention="scaled_luong",
nmt-master/nmt/inference_test.py:110:        attention_architecture="standard",
nmt-master/nmt/inference_test.py:111:        use_residual=False,)
nmt-master/nmt/inference_test.py:113:    num_workers = 3
nmt-master/nmt/inference_test.py:115:    # There are 5 examples, make batch_size=3 makes job0 has 3 examples, job1
nmt-master/nmt/inference_test.py:116:    # has 2 examples, and job2 has 0 example. This helps testing some edge
nmt-master/nmt/inference_test.py:117:    # cases.
nmt-master/nmt/inference_test.py:118:    hparams.batch_size = 3
nmt-master/nmt/inference_test.py:120:    ckpt_path = self._createTestInferCheckpoint(hparams, "multi_worker_infer")
nmt-master/nmt/inference_test.py:121:    infer_file = "nmt/testdata/test_infer_file"
nmt-master/nmt/inference_test.py:122:    output_infer = os.path.join(hparams.out_dir, "output_infer")
nmt-master/nmt/inference_test.py:123:    inference.inference(
nmt-master/nmt/inference_test.py:124:        ckpt_path, infer_file, output_infer, hparams, num_workers, jobid=1)
nmt-master/nmt/inference_test.py:126:    inference.inference(
nmt-master/nmt/inference_test.py:127:        ckpt_path, infer_file, output_infer, hparams, num_workers, jobid=2)
nmt-master/nmt/inference_test.py:129:    # Note: Need to start job 0 at the end; otherwise, it will block the testing
nmt-master/nmt/inference_test.py:130:    # thread.
nmt-master/nmt/inference_test.py:131:    inference.inference(
nmt-master/nmt/inference_test.py:132:        ckpt_path, infer_file, output_infer, hparams, num_workers, jobid=0)
nmt-master/nmt/inference_test.py:134:    with open(output_infer) as f:
nmt-master/nmt/inference_test.py:135:      self.assertEqual(5, len(list(f)))
nmt-master/nmt/inference_test.py:137:  def testBasicModelWithInferIndices(self):
nmt-master/nmt/inference_test.py:138:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/inference_test.py:139:        encoder_type="uni",
nmt-master/nmt/inference_test.py:140:        num_layers=1,
nmt-master/nmt/inference_test.py:141:        attention="",
nmt-master/nmt/inference_test.py:142:        attention_architecture="",
nmt-master/nmt/inference_test.py:143:        use_residual=False,
nmt-master/nmt/inference_test.py:144:        inference_indices=[0])
nmt-master/nmt/inference_test.py:145:    ckpt_path = self._createTestInferCheckpoint(hparams,
nmt-master/nmt/inference_test.py:146:                                                "basic_infer_with_indices")
nmt-master/nmt/inference_test.py:147:    infer_file = "nmt/testdata/test_infer_file"
nmt-master/nmt/inference_test.py:148:    output_infer = os.path.join(hparams.out_dir, "output_infer")
nmt-master/nmt/inference_test.py:149:    inference.inference(ckpt_path, infer_file, output_infer, hparams)
nmt-master/nmt/inference_test.py:150:    with open(output_infer) as f:
nmt-master/nmt/inference_test.py:151:      self.assertEqual(1, len(list(f)))
nmt-master/nmt/inference_test.py:153:  def testAttentionModelWithInferIndices(self):
nmt-master/nmt/inference_test.py:154:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/inference_test.py:155:        encoder_type="uni",
nmt-master/nmt/inference_test.py:156:        num_layers=1,
nmt-master/nmt/inference_test.py:157:        attention="scaled_luong",
nmt-master/nmt/inference_test.py:158:        attention_architecture="standard",
nmt-master/nmt/inference_test.py:159:        use_residual=False,
nmt-master/nmt/inference_test.py:160:        inference_indices=[1, 2])
nmt-master/nmt/inference_test.py:161:    # TODO(rzhao): Make infer indices support batch_size > 1.
nmt-master/nmt/inference_test.py:162:    hparams.infer_batch_size = 1
nmt-master/nmt/inference_test.py:163:    ckpt_path = self._createTestInferCheckpoint(hparams,
nmt-master/nmt/inference_test.py:164:                                                "attention_infer_with_indices")
nmt-master/nmt/inference_test.py:165:    infer_file = "nmt/testdata/test_infer_file"
nmt-master/nmt/inference_test.py:166:    output_infer = os.path.join(hparams.out_dir, "output_infer")
nmt-master/nmt/inference_test.py:167:    inference.inference(ckpt_path, infer_file, output_infer, hparams)
nmt-master/nmt/inference_test.py:168:    with open(output_infer) as f:
nmt-master/nmt/inference_test.py:169:      self.assertEqual(2, len(list(f)))
nmt-master/nmt/inference_test.py:170:    self.assertTrue(os.path.exists(output_infer+str(1)+".png"))
nmt-master/nmt/inference_test.py:171:    self.assertTrue(os.path.exists(output_infer+str(2)+".png"))
nmt-master/nmt/inference_test.py:174:if __name__ == "__main__":
nmt-master/nmt/inference_test.py:175:  tf.test.main()
nmt-master/nmt/model.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/model.py:2:#
nmt-master/nmt/model.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/model.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/model.py:5:# You may obtain a copy of the License at
nmt-master/nmt/model.py:6:#
nmt-master/nmt/model.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/model.py:8:#
nmt-master/nmt/model.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/model.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/model.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/model.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/model.py:13:# limitations under the License.
nmt-master/nmt/model.py:14:# ==============================================================================
nmt-master/nmt/model.py:16:"""Basic sequence-to-sequence model with dynamic RNN support."""
nmt-master/nmt/model.py:17:from __future__ import absolute_import
nmt-master/nmt/model.py:18:from __future__ import division
nmt-master/nmt/model.py:19:from __future__ import print_function
nmt-master/nmt/model.py:21:import abc
nmt-master/nmt/model.py:22:import collections
nmt-master/nmt/model.py:23:import numpy as np
nmt-master/nmt/model.py:25:import tensorflow as tf
nmt-master/nmt/model.py:27:from . import model_helper
nmt-master/nmt/model.py:28:from .utils import iterator_utils
nmt-master/nmt/model.py:29:from .utils import misc_utils as utils
nmt-master/nmt/model.py:30:from .utils import vocab_utils
nmt-master/nmt/model.py:32:utils.check_tensorflow_version()
nmt-master/nmt/model.py:34:__all__ = ["BaseModel", "Model"]
nmt-master/nmt/model.py:37:class TrainOutputTuple(collections.namedtuple(
nmt-master/nmt/model.py:38:    "TrainOutputTuple", ("train_summary", "train_loss", "predict_count",
nmt-master/nmt/model.py:39:                         "global_step", "word_count", "batch_size", "grad_norm",
nmt-master/nmt/model.py:40:                         "learning_rate"))):
nmt-master/nmt/model.py:41:  """To allow for flexibily in returing different outputs."""
nmt-master/nmt/model.py:42:  pass
nmt-master/nmt/model.py:45:class EvalOutputTuple(collections.namedtuple(
nmt-master/nmt/model.py:46:    "EvalOutputTuple", ("eval_loss", "predict_count", "batch_size"))):
nmt-master/nmt/model.py:47:  """To allow for flexibily in returing different outputs."""
nmt-master/nmt/model.py:48:  pass
nmt-master/nmt/model.py:51:class InferOutputTuple(collections.namedtuple(
nmt-master/nmt/model.py:52:    "InferOutputTuple", ("infer_logits", "infer_summary", "sample_id",
nmt-master/nmt/model.py:53:                         "sample_words"))):
nmt-master/nmt/model.py:54:  """To allow for flexibily in returing different outputs."""
nmt-master/nmt/model.py:55:  pass
nmt-master/nmt/model.py:58:class BaseModel(object):
nmt-master/nmt/model.py:59:  """Sequence-to-sequence base class.
nmt-master/nmt/model.py:60:  """
nmt-master/nmt/model.py:62:  def __init__(self,
nmt-master/nmt/model.py:63:               hparams,
nmt-master/nmt/model.py:64:               mode,
nmt-master/nmt/model.py:65:               iterator,
nmt-master/nmt/model.py:66:               source_vocab_table,
nmt-master/nmt/model.py:67:               target_vocab_table,
nmt-master/nmt/model.py:68:               reverse_target_vocab_table=None,
nmt-master/nmt/model.py:69:               scope=None,
nmt-master/nmt/model.py:70:               extra_args=None):
nmt-master/nmt/model.py:71:    """Create the model.
nmt-master/nmt/model.py:73:    Args:
nmt-master/nmt/model.py:74:      hparams: Hyperparameter configurations.
nmt-master/nmt/model.py:75:      mode: TRAIN | EVAL | INFER
nmt-master/nmt/model.py:76:      iterator: Dataset Iterator that feeds data.
nmt-master/nmt/model.py:77:      source_vocab_table: Lookup table mapping source words to ids.
nmt-master/nmt/model.py:78:      target_vocab_table: Lookup table mapping target words to ids.
nmt-master/nmt/model.py:79:      reverse_target_vocab_table: Lookup table mapping ids to target words. Only
nmt-master/nmt/model.py:80:        required in INFER mode. Defaults to None.
nmt-master/nmt/model.py:81:      scope: scope of the model.
nmt-master/nmt/model.py:82:      extra_args: model_helper.ExtraArgs, for passing customizable functions.
nmt-master/nmt/model.py:84:    """
nmt-master/nmt/model.py:85:    # Set params
nmt-master/nmt/model.py:86:    self._set_params_initializer(hparams, mode, iterator,
nmt-master/nmt/model.py:87:                                 source_vocab_table, target_vocab_table,
nmt-master/nmt/model.py:88:                                 scope, extra_args)
nmt-master/nmt/model.py:90:    # Not used in general seq2seq models; when True, ignore decoder & training
nmt-master/nmt/model.py:91:    self.extract_encoder_layers = (hasattr(hparams, "extract_encoder_layers")
nmt-master/nmt/model.py:92:                                   and hparams.extract_encoder_layers)
nmt-master/nmt/model.py:94:    # Train graph
nmt-master/nmt/model.py:95:    res = self.build_graph(hparams, scope=scope)
nmt-master/nmt/model.py:96:    if not self.extract_encoder_layers:
nmt-master/nmt/model.py:97:      self._set_train_or_infer(res, reverse_target_vocab_table, hparams)
nmt-master/nmt/model.py:99:    # Saver
nmt-master/nmt/model.py:100:    self.saver = tf.train.Saver(
nmt-master/nmt/model.py:101:        tf.global_variables(), max_to_keep=hparams.num_keep_ckpts)
nmt-master/nmt/model.py:103:  def _set_params_initializer(self,
nmt-master/nmt/model.py:104:                              hparams,
nmt-master/nmt/model.py:105:                              mode,
nmt-master/nmt/model.py:106:                              iterator,
nmt-master/nmt/model.py:107:                              source_vocab_table,
nmt-master/nmt/model.py:108:                              target_vocab_table,
nmt-master/nmt/model.py:109:                              scope,
nmt-master/nmt/model.py:110:                              extra_args=None):
nmt-master/nmt/model.py:111:    """Set various params for self and initialize."""
nmt-master/nmt/model.py:112:    assert isinstance(iterator, iterator_utils.BatchedInput)
nmt-master/nmt/model.py:113:    self.iterator = iterator
nmt-master/nmt/model.py:114:    self.mode = mode
nmt-master/nmt/model.py:115:    self.src_vocab_table = source_vocab_table
nmt-master/nmt/model.py:116:    self.tgt_vocab_table = target_vocab_table
nmt-master/nmt/model.py:118:    self.src_vocab_size = hparams.src_vocab_size
nmt-master/nmt/model.py:119:    self.tgt_vocab_size = hparams.tgt_vocab_size
nmt-master/nmt/model.py:120:    self.num_gpus = hparams.num_gpus
nmt-master/nmt/model.py:121:    self.time_major = hparams.time_major
nmt-master/nmt/model.py:123:    if hparams.use_char_encode:
nmt-master/nmt/model.py:124:      assert (not self.time_major), ("Can't use time major for"
nmt-master/nmt/model.py:125:                                     " char-level inputs.")
nmt-master/nmt/model.py:127:    self.dtype = tf.float32
nmt-master/nmt/model.py:128:    self.num_sampled_softmax = hparams.num_sampled_softmax
nmt-master/nmt/model.py:130:    # extra_args: to make it flexible for adding external customizable code
nmt-master/nmt/model.py:131:    self.single_cell_fn = None
nmt-master/nmt/model.py:132:    if extra_args:
nmt-master/nmt/model.py:133:      self.single_cell_fn = extra_args.single_cell_fn
nmt-master/nmt/model.py:135:    # Set num units
nmt-master/nmt/model.py:136:    self.num_units = hparams.num_units
nmt-master/nmt/model.py:138:    # Set num layers
nmt-master/nmt/model.py:139:    self.num_encoder_layers = hparams.num_encoder_layers
nmt-master/nmt/model.py:140:    self.num_decoder_layers = hparams.num_decoder_layers
nmt-master/nmt/model.py:141:    assert self.num_encoder_layers
nmt-master/nmt/model.py:142:    assert self.num_decoder_layers
nmt-master/nmt/model.py:144:    # Set num residual layers
nmt-master/nmt/model.py:145:    if hasattr(hparams, "num_residual_layers"):  # compatible common_test_utils
nmt-master/nmt/model.py:146:      self.num_encoder_residual_layers = hparams.num_residual_layers
nmt-master/nmt/model.py:147:      self.num_decoder_residual_layers = hparams.num_residual_layers
nmt-master/nmt/model.py:148:    else:
nmt-master/nmt/model.py:149:      self.num_encoder_residual_layers = hparams.num_encoder_residual_layers
nmt-master/nmt/model.py:150:      self.num_decoder_residual_layers = hparams.num_decoder_residual_layers
nmt-master/nmt/model.py:152:    # Batch size
nmt-master/nmt/model.py:153:    self.batch_size = tf.size(self.iterator.source_sequence_length)
nmt-master/nmt/model.py:155:    # Global step
nmt-master/nmt/model.py:156:    self.global_step = tf.Variable(0, trainable=False)
nmt-master/nmt/model.py:158:    # Initializer
nmt-master/nmt/model.py:159:    self.random_seed = hparams.random_seed
nmt-master/nmt/model.py:160:    initializer = model_helper.get_initializer(
nmt-master/nmt/model.py:161:        hparams.init_op, self.random_seed, hparams.init_weight)
nmt-master/nmt/model.py:162:    tf.get_variable_scope().set_initializer(initializer)
nmt-master/nmt/model.py:164:    # Embeddings
nmt-master/nmt/model.py:165:    if extra_args and extra_args.encoder_emb_lookup_fn:
nmt-master/nmt/model.py:166:      self.encoder_emb_lookup_fn = extra_args.encoder_emb_lookup_fn
nmt-master/nmt/model.py:167:    else:
nmt-master/nmt/model.py:168:      self.encoder_emb_lookup_fn = tf.nn.embedding_lookup
nmt-master/nmt/model.py:169:    self.init_embeddings(hparams, scope)
nmt-master/nmt/model.py:171:  def _set_train_or_infer(self, res, reverse_target_vocab_table, hparams):
nmt-master/nmt/model.py:172:    """Set up training and inference."""
nmt-master/nmt/model.py:173:    if self.mode == tf.contrib.learn.ModeKeys.TRAIN:
nmt-master/nmt/model.py:174:      self.train_loss = res[1]
nmt-master/nmt/model.py:175:      self.word_count = tf.reduce_sum(
nmt-master/nmt/model.py:176:          self.iterator.source_sequence_length) + tf.reduce_sum(
nmt-master/nmt/model.py:177:              self.iterator.target_sequence_length)
nmt-master/nmt/model.py:178:    elif self.mode == tf.contrib.learn.ModeKeys.EVAL:
nmt-master/nmt/model.py:179:      self.eval_loss = res[1]
nmt-master/nmt/model.py:180:    elif self.mode == tf.contrib.learn.ModeKeys.INFER:
nmt-master/nmt/model.py:181:      self.infer_logits, _, self.final_context_state, self.sample_id = res
nmt-master/nmt/model.py:182:      self.sample_words = reverse_target_vocab_table.lookup(
nmt-master/nmt/model.py:183:          tf.to_int64(self.sample_id))
nmt-master/nmt/model.py:185:    if self.mode != tf.contrib.learn.ModeKeys.INFER:
nmt-master/nmt/model.py:186:      ## Count the number of predicted words for compute ppl.
nmt-master/nmt/model.py:187:      self.predict_count = tf.reduce_sum(
nmt-master/nmt/model.py:188:          self.iterator.target_sequence_length)
nmt-master/nmt/model.py:190:    params = tf.trainable_variables()
nmt-master/nmt/model.py:192:    # Gradients and SGD update operation for training the model.
nmt-master/nmt/model.py:193:    # Arrange for the embedding vars to appear at the beginning.
nmt-master/nmt/model.py:194:    if self.mode == tf.contrib.learn.ModeKeys.TRAIN:
nmt-master/nmt/model.py:195:      self.learning_rate = tf.constant(hparams.learning_rate)
nmt-master/nmt/model.py:196:      # warm-up
nmt-master/nmt/model.py:197:      self.learning_rate = self._get_learning_rate_warmup(hparams)
nmt-master/nmt/model.py:198:      # decay
nmt-master/nmt/model.py:199:      self.learning_rate = self._get_learning_rate_decay(hparams)
nmt-master/nmt/model.py:201:      # Optimizer
nmt-master/nmt/model.py:202:      if hparams.optimizer == "sgd":
nmt-master/nmt/model.py:203:        opt = tf.train.GradientDescentOptimizer(self.learning_rate)
nmt-master/nmt/model.py:204:      elif hparams.optimizer == "adam":
nmt-master/nmt/model.py:205:        opt = tf.train.AdamOptimizer(self.learning_rate)
nmt-master/nmt/model.py:206:      else:
nmt-master/nmt/model.py:207:        raise ValueError("Unknown optimizer type %s" % hparams.optimizer)
nmt-master/nmt/model.py:209:      # Gradients
nmt-master/nmt/model.py:210:      gradients = tf.gradients(
nmt-master/nmt/model.py:211:          self.train_loss,
nmt-master/nmt/model.py:212:          params,
nmt-master/nmt/model.py:213:          colocate_gradients_with_ops=hparams.colocate_gradients_with_ops)
nmt-master/nmt/model.py:215:      clipped_grads, grad_norm_summary, grad_norm = model_helper.gradient_clip(
nmt-master/nmt/model.py:216:          gradients, max_gradient_norm=hparams.max_gradient_norm)
nmt-master/nmt/model.py:217:      self.grad_norm_summary = grad_norm_summary
nmt-master/nmt/model.py:218:      self.grad_norm = grad_norm
nmt-master/nmt/model.py:220:      self.update = opt.apply_gradients(
nmt-master/nmt/model.py:221:          zip(clipped_grads, params), global_step=self.global_step)
nmt-master/nmt/model.py:223:      # Summary
nmt-master/nmt/model.py:224:      self.train_summary = self._get_train_summary()
nmt-master/nmt/model.py:225:    elif self.mode == tf.contrib.learn.ModeKeys.INFER:
nmt-master/nmt/model.py:226:      self.infer_summary = self._get_infer_summary(hparams)
nmt-master/nmt/model.py:228:    # Print trainable variables
nmt-master/nmt/model.py:229:    utils.print_out("# Trainable variables")
nmt-master/nmt/model.py:230:    utils.print_out("Format: <name>, <shape>, <(soft) device placement>")
nmt-master/nmt/model.py:231:    for param in params:
nmt-master/nmt/model.py:232:      utils.print_out("  %s, %s, %s" % (param.name, str(param.get_shape()),
nmt-master/nmt/model.py:233:                                        param.op.device))
nmt-master/nmt/model.py:235:  def _get_learning_rate_warmup(self, hparams):
nmt-master/nmt/model.py:236:    """Get learning rate warmup."""
nmt-master/nmt/model.py:237:    warmup_steps = hparams.warmup_steps
nmt-master/nmt/model.py:238:    warmup_scheme = hparams.warmup_scheme
nmt-master/nmt/model.py:239:    utils.print_out("  learning_rate=%g, warmup_steps=%d, warmup_scheme=%s" %
nmt-master/nmt/model.py:240:                    (hparams.learning_rate, warmup_steps, warmup_scheme))
nmt-master/nmt/model.py:242:    # Apply inverse decay if global steps less than warmup steps.
nmt-master/nmt/model.py:243:    # Inspired by https://arxiv.org/pdf/1706.03762.pdf (Section 5.3)
nmt-master/nmt/model.py:244:    # When step < warmup_steps,
nmt-master/nmt/model.py:245:    #   learing_rate *= warmup_factor ** (warmup_steps - step)
nmt-master/nmt/model.py:246:    if warmup_scheme == "t2t":
nmt-master/nmt/model.py:247:      # 0.01^(1/warmup_steps): we start with a lr, 100 times smaller
nmt-master/nmt/model.py:248:      warmup_factor = tf.exp(tf.log(0.01) / warmup_steps)
nmt-master/nmt/model.py:249:      inv_decay = warmup_factor**(
nmt-master/nmt/model.py:250:          tf.to_float(warmup_steps - self.global_step))
nmt-master/nmt/model.py:251:    else:
nmt-master/nmt/model.py:252:      raise ValueError("Unknown warmup scheme %s" % warmup_scheme)
nmt-master/nmt/model.py:254:    return tf.cond(
nmt-master/nmt/model.py:255:        self.global_step < hparams.warmup_steps,
nmt-master/nmt/model.py:256:        lambda: inv_decay * self.learning_rate,
nmt-master/nmt/model.py:257:        lambda: self.learning_rate,
nmt-master/nmt/model.py:258:        name="learning_rate_warmup_cond")
nmt-master/nmt/model.py:260:  def _get_decay_info(self, hparams):
nmt-master/nmt/model.py:261:    """Return decay info based on decay_scheme."""
nmt-master/nmt/model.py:262:    if hparams.decay_scheme in ["luong5", "luong10", "luong234"]:
nmt-master/nmt/model.py:263:      decay_factor = 0.5
nmt-master/nmt/model.py:264:      if hparams.decay_scheme == "luong5":
nmt-master/nmt/model.py:265:        start_decay_step = int(hparams.num_train_steps / 2)
nmt-master/nmt/model.py:266:        decay_times = 5
nmt-master/nmt/model.py:267:      elif hparams.decay_scheme == "luong10":
nmt-master/nmt/model.py:268:        start_decay_step = int(hparams.num_train_steps / 2)
nmt-master/nmt/model.py:269:        decay_times = 10
nmt-master/nmt/model.py:270:      elif hparams.decay_scheme == "luong234":
nmt-master/nmt/model.py:271:        start_decay_step = int(hparams.num_train_steps * 2 / 3)
nmt-master/nmt/model.py:272:        decay_times = 4
nmt-master/nmt/model.py:273:      remain_steps = hparams.num_train_steps - start_decay_step
nmt-master/nmt/model.py:274:      decay_steps = int(remain_steps / decay_times)
nmt-master/nmt/model.py:275:    elif not hparams.decay_scheme:  # no decay
nmt-master/nmt/model.py:276:      start_decay_step = hparams.num_train_steps
nmt-master/nmt/model.py:277:      decay_steps = 0
nmt-master/nmt/model.py:278:      decay_factor = 1.0
nmt-master/nmt/model.py:279:    elif hparams.decay_scheme:
nmt-master/nmt/model.py:280:      raise ValueError("Unknown decay scheme %s" % hparams.decay_scheme)
nmt-master/nmt/model.py:281:    return start_decay_step, decay_steps, decay_factor
nmt-master/nmt/model.py:283:  def _get_learning_rate_decay(self, hparams):
nmt-master/nmt/model.py:284:    """Get learning rate decay."""
nmt-master/nmt/model.py:285:    start_decay_step, decay_steps, decay_factor = self._get_decay_info(hparams)
nmt-master/nmt/model.py:286:    utils.print_out("  decay_scheme=%s, start_decay_step=%d, decay_steps %d, "
nmt-master/nmt/model.py:287:                    "decay_factor %g" % (hparams.decay_scheme,
nmt-master/nmt/model.py:288:                                         start_decay_step,
nmt-master/nmt/model.py:289:                                         decay_steps,
nmt-master/nmt/model.py:290:                                         decay_factor))
nmt-master/nmt/model.py:292:    return tf.cond(
nmt-master/nmt/model.py:293:        self.global_step < start_decay_step,
nmt-master/nmt/model.py:294:        lambda: self.learning_rate,
nmt-master/nmt/model.py:295:        lambda: tf.train.exponential_decay(
nmt-master/nmt/model.py:296:            self.learning_rate,
nmt-master/nmt/model.py:297:            (self.global_step - start_decay_step),
nmt-master/nmt/model.py:298:            decay_steps, decay_factor, staircase=True),
nmt-master/nmt/model.py:299:        name="learning_rate_decay_cond")
nmt-master/nmt/model.py:301:  def init_embeddings(self, hparams, scope):
nmt-master/nmt/model.py:302:    """Init embeddings."""
nmt-master/nmt/model.py:303:    self.embedding_encoder, self.embedding_decoder = (
nmt-master/nmt/model.py:304:        model_helper.create_emb_for_encoder_and_decoder(
nmt-master/nmt/model.py:305:            share_vocab=hparams.share_vocab,
nmt-master/nmt/model.py:306:            src_vocab_size=self.src_vocab_size,
nmt-master/nmt/model.py:307:            tgt_vocab_size=self.tgt_vocab_size,
nmt-master/nmt/model.py:308:            src_embed_size=self.num_units,
nmt-master/nmt/model.py:309:            tgt_embed_size=self.num_units,
nmt-master/nmt/model.py:310:            num_enc_partitions=hparams.num_enc_emb_partitions,
nmt-master/nmt/model.py:311:            num_dec_partitions=hparams.num_dec_emb_partitions,
nmt-master/nmt/model.py:312:            src_vocab_file=hparams.src_vocab_file,
nmt-master/nmt/model.py:313:            tgt_vocab_file=hparams.tgt_vocab_file,
nmt-master/nmt/model.py:314:            src_embed_file=hparams.src_embed_file,
nmt-master/nmt/model.py:315:            tgt_embed_file=hparams.tgt_embed_file,
nmt-master/nmt/model.py:316:            use_char_encode=hparams.use_char_encode,
nmt-master/nmt/model.py:317:            scope=scope,))
nmt-master/nmt/model.py:319:  def _get_train_summary(self):
nmt-master/nmt/model.py:320:    """Get train summary."""
nmt-master/nmt/model.py:321:    train_summary = tf.summary.merge(
nmt-master/nmt/model.py:322:        [tf.summary.scalar("lr", self.learning_rate),
nmt-master/nmt/model.py:323:         tf.summary.scalar("train_loss", self.train_loss)] +
nmt-master/nmt/model.py:324:        self.grad_norm_summary)
nmt-master/nmt/model.py:325:    return train_summary
nmt-master/nmt/model.py:327:  def train(self, sess):
nmt-master/nmt/model.py:328:    """Execute train graph."""
nmt-master/nmt/model.py:329:    assert self.mode == tf.contrib.learn.ModeKeys.TRAIN
nmt-master/nmt/model.py:330:    output_tuple = TrainOutputTuple(train_summary=self.train_summary,
nmt-master/nmt/model.py:331:                                    train_loss=self.train_loss,
nmt-master/nmt/model.py:332:                                    predict_count=self.predict_count,
nmt-master/nmt/model.py:333:                                    global_step=self.global_step,
nmt-master/nmt/model.py:334:                                    word_count=self.word_count,
nmt-master/nmt/model.py:335:                                    batch_size=self.batch_size,
nmt-master/nmt/model.py:336:                                    grad_norm=self.grad_norm,
nmt-master/nmt/model.py:337:                                    learning_rate=self.learning_rate)
nmt-master/nmt/model.py:338:    return sess.run([self.update, output_tuple])
nmt-master/nmt/model.py:340:  def eval(self, sess):
nmt-master/nmt/model.py:341:    """Execute eval graph."""
nmt-master/nmt/model.py:342:    assert self.mode == tf.contrib.learn.ModeKeys.EVAL
nmt-master/nmt/model.py:343:    output_tuple = EvalOutputTuple(eval_loss=self.eval_loss,
nmt-master/nmt/model.py:344:                                   predict_count=self.predict_count,
nmt-master/nmt/model.py:345:                                   batch_size=self.batch_size)
nmt-master/nmt/model.py:346:    return sess.run(output_tuple)
nmt-master/nmt/model.py:348:  def build_graph(self, hparams, scope=None):
nmt-master/nmt/model.py:349:    """Subclass must implement this method.
nmt-master/nmt/model.py:351:    Creates a sequence-to-sequence model with dynamic RNN decoder API.
nmt-master/nmt/model.py:352:    Args:
nmt-master/nmt/model.py:353:      hparams: Hyperparameter configurations.
nmt-master/nmt/model.py:354:      scope: VariableScope for the created subgraph; default "dynamic_seq2seq".
nmt-master/nmt/model.py:356:    Returns:
nmt-master/nmt/model.py:357:      A tuple of the form (logits, loss_tuple, final_context_state, sample_id),
nmt-master/nmt/model.py:358:      where:
nmt-master/nmt/model.py:359:        logits: float32 Tensor [batch_size x num_decoder_symbols].
nmt-master/nmt/model.py:360:        loss: loss = the total loss / batch_size.
nmt-master/nmt/model.py:361:        final_context_state: the final state of decoder RNN.
nmt-master/nmt/model.py:362:        sample_id: sampling indices.
nmt-master/nmt/model.py:364:    Raises:
nmt-master/nmt/model.py:365:      ValueError: if encoder_type differs from mono and bi, or
nmt-master/nmt/model.py:366:        attention_option is not (luong | scaled_luong |
nmt-master/nmt/model.py:367:        bahdanau | normed_bahdanau).
nmt-master/nmt/model.py:368:    """
nmt-master/nmt/model.py:369:    utils.print_out("# Creating %s graph ..." % self.mode)
nmt-master/nmt/model.py:371:    # Projection
nmt-master/nmt/model.py:372:    if not self.extract_encoder_layers:
nmt-master/nmt/model.py:373:      with tf.variable_scope(scope or "build_network"):
nmt-master/nmt/model.py:374:        with tf.variable_scope("decoder/output_projection"):
nmt-master/nmt/model.py:375:          self.output_layer = tf.layers.Dense(
nmt-master/nmt/model.py:376:              self.tgt_vocab_size, use_bias=False, name="output_projection")
nmt-master/nmt/model.py:378:    with tf.variable_scope(scope or "dynamic_seq2seq", dtype=self.dtype):
nmt-master/nmt/model.py:379:      # Encoder
nmt-master/nmt/model.py:380:      if hparams.language_model:  # no encoder for language modeling
nmt-master/nmt/model.py:381:        utils.print_out("  language modeling: no encoder")
nmt-master/nmt/model.py:382:        self.encoder_outputs = None
nmt-master/nmt/model.py:383:        encoder_state = None
nmt-master/nmt/model.py:384:      else:
nmt-master/nmt/model.py:385:        self.encoder_outputs, encoder_state = self._build_encoder(hparams)
nmt-master/nmt/model.py:387:      # Skip decoder if extracting only encoder layers
nmt-master/nmt/model.py:388:      if self.extract_encoder_layers:
nmt-master/nmt/model.py:389:        return
nmt-master/nmt/model.py:391:      ## Decoder
nmt-master/nmt/model.py:392:      logits, decoder_cell_outputs, sample_id, final_context_state = (
nmt-master/nmt/model.py:393:          self._build_decoder(self.encoder_outputs, encoder_state, hparams))
nmt-master/nmt/model.py:395:      ## Loss
nmt-master/nmt/model.py:396:      if self.mode != tf.contrib.learn.ModeKeys.INFER:
nmt-master/nmt/model.py:397:        with tf.device(model_helper.get_device_str(self.num_encoder_layers - 1,
nmt-master/nmt/model.py:398:                                                   self.num_gpus)):
nmt-master/nmt/model.py:399:          loss = self._compute_loss(logits, decoder_cell_outputs)
nmt-master/nmt/model.py:400:      else:
nmt-master/nmt/model.py:401:        loss = tf.constant(0.0)
nmt-master/nmt/model.py:403:      return logits, loss, final_context_state, sample_id
nmt-master/nmt/model.py:405:  @abc.abstractmethod
nmt-master/nmt/model.py:406:  def _build_encoder(self, hparams):
nmt-master/nmt/model.py:407:    """Subclass must implement this.
nmt-master/nmt/model.py:409:    Build and run an RNN encoder.
nmt-master/nmt/model.py:411:    Args:
nmt-master/nmt/model.py:412:      hparams: Hyperparameters configurations.
nmt-master/nmt/model.py:414:    Returns:
nmt-master/nmt/model.py:415:      A tuple of encoder_outputs and encoder_state.
nmt-master/nmt/model.py:416:    """
nmt-master/nmt/model.py:417:    pass
nmt-master/nmt/model.py:419:  def _build_encoder_cell(self, hparams, num_layers, num_residual_layers,
nmt-master/nmt/model.py:420:                          base_gpu=0):
nmt-master/nmt/model.py:421:    """Build a multi-layer RNN cell that can be used by encoder."""
nmt-master/nmt/model.py:423:    return model_helper.create_rnn_cell(
nmt-master/nmt/model.py:424:        unit_type=hparams.unit_type,
nmt-master/nmt/model.py:425:        num_units=self.num_units,
nmt-master/nmt/model.py:426:        num_layers=num_layers,
nmt-master/nmt/model.py:427:        num_residual_layers=num_residual_layers,
nmt-master/nmt/model.py:428:        forget_bias=hparams.forget_bias,
nmt-master/nmt/model.py:429:        dropout=hparams.dropout,
nmt-master/nmt/model.py:430:        num_gpus=hparams.num_gpus,
nmt-master/nmt/model.py:431:        mode=self.mode,
nmt-master/nmt/model.py:432:        base_gpu=base_gpu,
nmt-master/nmt/model.py:433:        single_cell_fn=self.single_cell_fn)
nmt-master/nmt/model.py:435:  def _get_infer_maximum_iterations(self, hparams, source_sequence_length):
nmt-master/nmt/model.py:436:    """Maximum decoding steps at inference time."""
nmt-master/nmt/model.py:437:    if hparams.tgt_max_len_infer:
nmt-master/nmt/model.py:438:      maximum_iterations = hparams.tgt_max_len_infer
nmt-master/nmt/model.py:439:      utils.print_out("  decoding maximum_iterations %d" % maximum_iterations)
nmt-master/nmt/model.py:440:    else:
nmt-master/nmt/model.py:441:      # TODO(thangluong): add decoding_length_factor flag
nmt-master/nmt/model.py:442:      decoding_length_factor = 2.0
nmt-master/nmt/model.py:443:      max_encoder_length = tf.reduce_max(source_sequence_length)
nmt-master/nmt/model.py:444:      maximum_iterations = tf.to_int32(tf.round(
nmt-master/nmt/model.py:445:          tf.to_float(max_encoder_length) * decoding_length_factor))
nmt-master/nmt/model.py:446:    return maximum_iterations
nmt-master/nmt/model.py:448:  def _build_decoder(self, encoder_outputs, encoder_state, hparams):
nmt-master/nmt/model.py:449:    """Build and run a RNN decoder with a final projection layer.
nmt-master/nmt/model.py:451:    Args:
nmt-master/nmt/model.py:452:      encoder_outputs: The outputs of encoder for every time step.
nmt-master/nmt/model.py:453:      encoder_state: The final state of the encoder.
nmt-master/nmt/model.py:454:      hparams: The Hyperparameters configurations.
nmt-master/nmt/model.py:456:    Returns:
nmt-master/nmt/model.py:457:      A tuple of final logits and final decoder state:
nmt-master/nmt/model.py:458:        logits: size [time, batch_size, vocab_size] when time_major=True.
nmt-master/nmt/model.py:459:    """
nmt-master/nmt/model.py:460:    tgt_sos_id = tf.cast(self.tgt_vocab_table.lookup(tf.constant(hparams.sos)),
nmt-master/nmt/model.py:461:                         tf.int32)
nmt-master/nmt/model.py:462:    tgt_eos_id = tf.cast(self.tgt_vocab_table.lookup(tf.constant(hparams.eos)),
nmt-master/nmt/model.py:463:                         tf.int32)
nmt-master/nmt/model.py:464:    iterator = self.iterator
nmt-master/nmt/model.py:466:    # maximum_iteration: The maximum decoding steps.
nmt-master/nmt/model.py:467:    maximum_iterations = self._get_infer_maximum_iterations(
nmt-master/nmt/model.py:468:        hparams, iterator.source_sequence_length)
nmt-master/nmt/model.py:470:    ## Decoder.
nmt-master/nmt/model.py:471:    with tf.variable_scope("decoder") as decoder_scope:
nmt-master/nmt/model.py:472:      cell, decoder_initial_state = self._build_decoder_cell(
nmt-master/nmt/model.py:473:          hparams, encoder_outputs, encoder_state,
nmt-master/nmt/model.py:474:          iterator.source_sequence_length)
nmt-master/nmt/model.py:476:      # Optional ops depends on which mode we are in and which loss function we
nmt-master/nmt/model.py:477:      # are using.
nmt-master/nmt/model.py:478:      logits = tf.no_op()
nmt-master/nmt/model.py:479:      decoder_cell_outputs = None
nmt-master/nmt/model.py:481:      ## Train or eval
nmt-master/nmt/model.py:482:      if self.mode != tf.contrib.learn.ModeKeys.INFER:
nmt-master/nmt/model.py:483:        # decoder_emp_inp: [max_time, batch_size, num_units]
nmt-master/nmt/model.py:484:        target_input = iterator.target_input
nmt-master/nmt/model.py:485:        if self.time_major:
nmt-master/nmt/model.py:486:          target_input = tf.transpose(target_input)
nmt-master/nmt/model.py:487:        decoder_emb_inp = tf.nn.embedding_lookup(
nmt-master/nmt/model.py:488:            self.embedding_decoder, target_input)
nmt-master/nmt/model.py:490:        # Helper
nmt-master/nmt/model.py:491:        helper = tf.contrib.seq2seq.TrainingHelper(
nmt-master/nmt/model.py:492:            decoder_emb_inp, iterator.target_sequence_length,
nmt-master/nmt/model.py:493:            time_major=self.time_major)
nmt-master/nmt/model.py:495:        # Decoder
nmt-master/nmt/model.py:496:        my_decoder = tf.contrib.seq2seq.BasicDecoder(
nmt-master/nmt/model.py:497:            cell,
nmt-master/nmt/model.py:498:            helper,
nmt-master/nmt/model.py:499:            decoder_initial_state,)
nmt-master/nmt/model.py:501:        # Dynamic decoding
nmt-master/nmt/model.py:502:        outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(
nmt-master/nmt/model.py:503:            my_decoder,
nmt-master/nmt/model.py:504:            output_time_major=self.time_major,
nmt-master/nmt/model.py:505:            swap_memory=True,
nmt-master/nmt/model.py:506:            scope=decoder_scope)
nmt-master/nmt/model.py:508:        sample_id = outputs.sample_id
nmt-master/nmt/model.py:510:        if self.num_sampled_softmax > 0:
nmt-master/nmt/model.py:511:          # Note: this is required when using sampled_softmax_loss.
nmt-master/nmt/model.py:512:          decoder_cell_outputs = outputs.rnn_output
nmt-master/nmt/model.py:514:        # Note: there's a subtle difference here between train and inference.
nmt-master/nmt/model.py:515:        # We could have set output_layer when create my_decoder
nmt-master/nmt/model.py:516:        #   and shared more code between train and inference.
nmt-master/nmt/model.py:517:        # We chose to apply the output_layer to all timesteps for speed:
nmt-master/nmt/model.py:518:        #   10% improvements for small models & 20% for larger ones.
nmt-master/nmt/model.py:519:        # If memory is a concern, we should apply output_layer per timestep.
nmt-master/nmt/model.py:520:        num_layers = self.num_decoder_layers
nmt-master/nmt/model.py:521:        num_gpus = self.num_gpus
nmt-master/nmt/model.py:522:        device_id = num_layers if num_layers < num_gpus else (num_layers - 1)
nmt-master/nmt/model.py:523:        # Colocate output layer with the last RNN cell if there is no extra GPU
nmt-master/nmt/model.py:524:        # available. Otherwise, put last layer on a separate GPU.
nmt-master/nmt/model.py:525:        with tf.device(model_helper.get_device_str(device_id, num_gpus)):
nmt-master/nmt/model.py:526:          logits = self.output_layer(outputs.rnn_output)
nmt-master/nmt/model.py:528:        if self.num_sampled_softmax > 0:
nmt-master/nmt/model.py:529:          logits = tf.no_op()  # unused when using sampled softmax loss.
nmt-master/nmt/model.py:531:      ## Inference
nmt-master/nmt/model.py:532:      else:
nmt-master/nmt/model.py:533:        infer_mode = hparams.infer_mode
nmt-master/nmt/model.py:534:        start_tokens = tf.fill([self.batch_size], tgt_sos_id)
nmt-master/nmt/model.py:535:        end_token = tgt_eos_id
nmt-master/nmt/model.py:536:        utils.print_out(
nmt-master/nmt/model.py:537:            "  decoder: infer_mode=%sbeam_width=%d, "
nmt-master/nmt/model.py:538:            "length_penalty=%f, coverage_penalty=%f"
nmt-master/nmt/model.py:539:            % (infer_mode, hparams.beam_width, hparams.length_penalty_weight,
nmt-master/nmt/model.py:540:               hparams.coverage_penalty_weight))
nmt-master/nmt/model.py:542:        if infer_mode == "beam_search":
nmt-master/nmt/model.py:543:          beam_width = hparams.beam_width
nmt-master/nmt/model.py:544:          length_penalty_weight = hparams.length_penalty_weight
nmt-master/nmt/model.py:545:          coverage_penalty_weight = hparams.coverage_penalty_weight
nmt-master/nmt/model.py:547:          my_decoder = tf.contrib.seq2seq.BeamSearchDecoder(
nmt-master/nmt/model.py:548:              cell=cell,
nmt-master/nmt/model.py:549:              embedding=self.embedding_decoder,
nmt-master/nmt/model.py:550:              start_tokens=start_tokens,
nmt-master/nmt/model.py:551:              end_token=end_token,
nmt-master/nmt/model.py:552:              initial_state=decoder_initial_state,
nmt-master/nmt/model.py:553:              beam_width=beam_width,
nmt-master/nmt/model.py:554:              output_layer=self.output_layer,
nmt-master/nmt/model.py:555:              length_penalty_weight=length_penalty_weight,
nmt-master/nmt/model.py:556:              coverage_penalty_weight=coverage_penalty_weight)
nmt-master/nmt/model.py:557:        elif infer_mode == "sample":
nmt-master/nmt/model.py:558:          # Helper
nmt-master/nmt/model.py:559:          sampling_temperature = hparams.sampling_temperature
nmt-master/nmt/model.py:560:          assert sampling_temperature > 0.0, (
nmt-master/nmt/model.py:561:              "sampling_temperature must greater than 0.0 when using sample"
nmt-master/nmt/model.py:562:              " decoder.")
nmt-master/nmt/model.py:563:          helper = tf.contrib.seq2seq.SampleEmbeddingHelper(
nmt-master/nmt/model.py:564:              self.embedding_decoder, start_tokens, end_token,
nmt-master/nmt/model.py:565:              softmax_temperature=sampling_temperature,
nmt-master/nmt/model.py:566:              seed=self.random_seed)
nmt-master/nmt/model.py:567:        elif infer_mode == "greedy":
nmt-master/nmt/model.py:568:          helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
nmt-master/nmt/model.py:569:              self.embedding_decoder, start_tokens, end_token)
nmt-master/nmt/model.py:570:        else:
nmt-master/nmt/model.py:571:          raise ValueError("Unknown infer_mode '%s'", infer_mode)
nmt-master/nmt/model.py:573:        if infer_mode != "beam_search":
nmt-master/nmt/model.py:574:          my_decoder = tf.contrib.seq2seq.BasicDecoder(
nmt-master/nmt/model.py:575:              cell,
nmt-master/nmt/model.py:576:              helper,
nmt-master/nmt/model.py:577:              decoder_initial_state,
nmt-master/nmt/model.py:578:              output_layer=self.output_layer  # applied per timestep
nmt-master/nmt/model.py:579:          )
nmt-master/nmt/model.py:581:        # Dynamic decoding
nmt-master/nmt/model.py:582:        outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(
nmt-master/nmt/model.py:583:            my_decoder,
nmt-master/nmt/model.py:584:            maximum_iterations=maximum_iterations,
nmt-master/nmt/model.py:585:            output_time_major=self.time_major,
nmt-master/nmt/model.py:586:            swap_memory=True,
nmt-master/nmt/model.py:587:            scope=decoder_scope)
nmt-master/nmt/model.py:589:        if infer_mode == "beam_search":
nmt-master/nmt/model.py:590:          sample_id = outputs.predicted_ids
nmt-master/nmt/model.py:591:        else:
nmt-master/nmt/model.py:592:          logits = outputs.rnn_output
nmt-master/nmt/model.py:593:          sample_id = outputs.sample_id
nmt-master/nmt/model.py:595:    return logits, decoder_cell_outputs, sample_id, final_context_state
nmt-master/nmt/model.py:597:  def get_max_time(self, tensor):
nmt-master/nmt/model.py:598:    time_axis = 0 if self.time_major else 1
nmt-master/nmt/model.py:599:    return tensor.shape[time_axis].value or tf.shape(tensor)[time_axis]
nmt-master/nmt/model.py:601:  @abc.abstractmethod
nmt-master/nmt/model.py:602:  def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state,
nmt-master/nmt/model.py:603:                          source_sequence_length):
nmt-master/nmt/model.py:604:    """Subclass must implement this.
nmt-master/nmt/model.py:606:    Args:
nmt-master/nmt/model.py:607:      hparams: Hyperparameters configurations.
nmt-master/nmt/model.py:608:      encoder_outputs: The outputs of encoder for every time step.
nmt-master/nmt/model.py:609:      encoder_state: The final state of the encoder.
nmt-master/nmt/model.py:610:      source_sequence_length: sequence length of encoder_outputs.
nmt-master/nmt/model.py:612:    Returns:
nmt-master/nmt/model.py:613:      A tuple of a multi-layer RNN cell used by decoder and the intial state of
nmt-master/nmt/model.py:614:      the decoder RNN.
nmt-master/nmt/model.py:615:    """
nmt-master/nmt/model.py:616:    pass
nmt-master/nmt/model.py:618:  def _softmax_cross_entropy_loss(
nmt-master/nmt/model.py:619:      self, logits, decoder_cell_outputs, labels):
nmt-master/nmt/model.py:620:    """Compute softmax loss or sampled softmax loss."""
nmt-master/nmt/model.py:621:    if self.num_sampled_softmax > 0:
nmt-master/nmt/model.py:623:      is_sequence = (decoder_cell_outputs.shape.ndims == 3)
nmt-master/nmt/model.py:625:      if is_sequence:
nmt-master/nmt/model.py:626:        labels = tf.reshape(labels, [-1, 1])
nmt-master/nmt/model.py:627:        inputs = tf.reshape(decoder_cell_outputs, [-1, self.num_units])
nmt-master/nmt/model.py:629:      crossent = tf.nn.sampled_softmax_loss(
nmt-master/nmt/model.py:630:          weights=tf.transpose(self.output_layer.kernel),
nmt-master/nmt/model.py:631:          biases=self.output_layer.bias or tf.zeros([self.tgt_vocab_size]),
nmt-master/nmt/model.py:632:          labels=labels,
nmt-master/nmt/model.py:633:          inputs=inputs,
nmt-master/nmt/model.py:634:          num_sampled=self.num_sampled_softmax,
nmt-master/nmt/model.py:635:          num_classes=self.tgt_vocab_size,
nmt-master/nmt/model.py:636:          partition_strategy="div",
nmt-master/nmt/model.py:637:          seed=self.random_seed)
nmt-master/nmt/model.py:639:      if is_sequence:
nmt-master/nmt/model.py:640:        if self.time_major:
nmt-master/nmt/model.py:641:          crossent = tf.reshape(crossent, [-1, self.batch_size])
nmt-master/nmt/model.py:642:        else:
nmt-master/nmt/model.py:643:          crossent = tf.reshape(crossent, [self.batch_size, -1])
nmt-master/nmt/model.py:645:    else:
nmt-master/nmt/model.py:646:      crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(
nmt-master/nmt/model.py:647:          labels=labels, logits=logits)
nmt-master/nmt/model.py:649:    return crossent
nmt-master/nmt/model.py:651:  def _compute_loss(self, logits, decoder_cell_outputs):
nmt-master/nmt/model.py:652:    """Compute optimization loss."""
nmt-master/nmt/model.py:653:    target_output = self.iterator.target_output
nmt-master/nmt/model.py:654:    if self.time_major:
nmt-master/nmt/model.py:655:      target_output = tf.transpose(target_output)
nmt-master/nmt/model.py:656:    max_time = self.get_max_time(target_output)
nmt-master/nmt/model.py:658:    crossent = self._softmax_cross_entropy_loss(
nmt-master/nmt/model.py:659:        logits, decoder_cell_outputs, target_output)
nmt-master/nmt/model.py:661:    target_weights = tf.sequence_mask(
nmt-master/nmt/model.py:662:        self.iterator.target_sequence_length, max_time, dtype=self.dtype)
nmt-master/nmt/model.py:663:    if self.time_major:
nmt-master/nmt/model.py:664:      target_weights = tf.transpose(target_weights)
nmt-master/nmt/model.py:666:    loss = tf.reduce_sum(
nmt-master/nmt/model.py:667:        crossent * target_weights) / tf.to_float(self.batch_size)
nmt-master/nmt/model.py:668:    return loss
nmt-master/nmt/model.py:670:  def _get_infer_summary(self, hparams):
nmt-master/nmt/model.py:671:    del hparams
nmt-master/nmt/model.py:672:    return tf.no_op()
nmt-master/nmt/model.py:674:  def infer(self, sess):
nmt-master/nmt/model.py:675:    assert self.mode == tf.contrib.learn.ModeKeys.INFER
nmt-master/nmt/model.py:676:    output_tuple = InferOutputTuple(infer_logits=self.infer_logits,
nmt-master/nmt/model.py:677:                                    infer_summary=self.infer_summary,
nmt-master/nmt/model.py:678:                                    sample_id=self.sample_id,
nmt-master/nmt/model.py:679:                                    sample_words=self.sample_words)
nmt-master/nmt/model.py:680:    return sess.run(output_tuple)
nmt-master/nmt/model.py:682:  def decode(self, sess):
nmt-master/nmt/model.py:683:    """Decode a batch.
nmt-master/nmt/model.py:685:    Args:
nmt-master/nmt/model.py:686:      sess: tensorflow session to use.
nmt-master/nmt/model.py:688:    Returns:
nmt-master/nmt/model.py:689:      A tuple consiting of outputs, infer_summary.
nmt-master/nmt/model.py:690:        outputs: of size [batch_size, time]
nmt-master/nmt/model.py:691:    """
nmt-master/nmt/model.py:692:    output_tuple = self.infer(sess)
nmt-master/nmt/model.py:693:    sample_words = output_tuple.sample_words
nmt-master/nmt/model.py:694:    infer_summary = output_tuple.infer_summary
nmt-master/nmt/model.py:696:    # make sure outputs is of shape [batch_size, time] or [beam_width,
nmt-master/nmt/model.py:697:    # batch_size, time] when using beam search.
nmt-master/nmt/model.py:698:    if self.time_major:
nmt-master/nmt/model.py:699:      sample_words = sample_words.transpose()
nmt-master/nmt/model.py:700:    elif sample_words.ndim == 3:
nmt-master/nmt/model.py:701:      # beam search output in [batch_size, time, beam_width] shape.
nmt-master/nmt/model.py:702:      sample_words = sample_words.transpose([2, 0, 1])
nmt-master/nmt/model.py:703:    return sample_words, infer_summary
nmt-master/nmt/model.py:705:  def build_encoder_states(self, include_embeddings=False):
nmt-master/nmt/model.py:706:    """Stack encoder states and return tensor [batch, length, layer, size]."""
nmt-master/nmt/model.py:707:    assert self.mode == tf.contrib.learn.ModeKeys.INFER
nmt-master/nmt/model.py:708:    if include_embeddings:
nmt-master/nmt/model.py:709:      stack_state_list = tf.stack(
nmt-master/nmt/model.py:710:          [self.encoder_emb_inp] + self.encoder_state_list, 2)
nmt-master/nmt/model.py:711:    else:
nmt-master/nmt/model.py:712:      stack_state_list = tf.stack(self.encoder_state_list, 2)
nmt-master/nmt/model.py:714:    # transform from [length, batch, ...] -> [batch, length, ...]
nmt-master/nmt/model.py:715:    if self.time_major:
nmt-master/nmt/model.py:716:      stack_state_list = tf.transpose(stack_state_list, [1, 0, 2, 3])
nmt-master/nmt/model.py:718:    return stack_state_list
nmt-master/nmt/model.py:721:class Model(BaseModel):
nmt-master/nmt/model.py:722:  """Sequence-to-sequence dynamic model.
nmt-master/nmt/model.py:724:  This class implements a multi-layer recurrent neural network as encoder,
nmt-master/nmt/model.py:725:  and a multi-layer recurrent neural network decoder.
nmt-master/nmt/model.py:726:  """
nmt-master/nmt/model.py:727:  def _build_encoder_from_sequence(self, hparams, sequence, sequence_length):
nmt-master/nmt/model.py:728:    """Build an encoder from a sequence.
nmt-master/nmt/model.py:730:    Args:
nmt-master/nmt/model.py:731:      hparams: hyperparameters.
nmt-master/nmt/model.py:732:      sequence: tensor with input sequence data.
nmt-master/nmt/model.py:733:      sequence_length: tensor with length of the input sequence.
nmt-master/nmt/model.py:735:    Returns:
nmt-master/nmt/model.py:736:      encoder_outputs: RNN encoder outputs.
nmt-master/nmt/model.py:737:      encoder_state: RNN encoder state.
nmt-master/nmt/model.py:739:    Raises:
nmt-master/nmt/model.py:740:      ValueError: if encoder_type is neither "uni" nor "bi".
nmt-master/nmt/model.py:741:    """
nmt-master/nmt/model.py:742:    num_layers = self.num_encoder_layers
nmt-master/nmt/model.py:743:    num_residual_layers = self.num_encoder_residual_layers
nmt-master/nmt/model.py:745:    if self.time_major:
nmt-master/nmt/model.py:746:      sequence = tf.transpose(sequence)
nmt-master/nmt/model.py:748:    with tf.variable_scope("encoder") as scope:
nmt-master/nmt/model.py:749:      dtype = scope.dtype
nmt-master/nmt/model.py:751:      self.encoder_emb_inp = self.encoder_emb_lookup_fn(
nmt-master/nmt/model.py:752:          self.embedding_encoder, sequence)
nmt-master/nmt/model.py:754:      # Encoder_outputs: [max_time, batch_size, num_units]
nmt-master/nmt/model.py:755:      if hparams.encoder_type == "uni":
nmt-master/nmt/model.py:756:        utils.print_out("  num_layers = %d, num_residual_layers=%d" %
nmt-master/nmt/model.py:757:                        (num_layers, num_residual_layers))
nmt-master/nmt/model.py:758:        cell = self._build_encoder_cell(hparams, num_layers,
nmt-master/nmt/model.py:759:                                        num_residual_layers)
nmt-master/nmt/model.py:761:        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(
nmt-master/nmt/model.py:762:            cell,
nmt-master/nmt/model.py:763:            self.encoder_emb_inp,
nmt-master/nmt/model.py:764:            dtype=dtype,
nmt-master/nmt/model.py:765:            sequence_length=sequence_length,
nmt-master/nmt/model.py:766:            time_major=self.time_major,
nmt-master/nmt/model.py:767:            swap_memory=True)
nmt-master/nmt/model.py:768:      elif hparams.encoder_type == "bi":
nmt-master/nmt/model.py:769:        num_bi_layers = int(num_layers / 2)
nmt-master/nmt/model.py:770:        num_bi_residual_layers = int(num_residual_layers / 2)
nmt-master/nmt/model.py:771:        utils.print_out("  num_bi_layers = %d, num_bi_residual_layers=%d" %
nmt-master/nmt/model.py:772:                        (num_bi_layers, num_bi_residual_layers))
nmt-master/nmt/model.py:774:        encoder_outputs, bi_encoder_state = (
nmt-master/nmt/model.py:775:            self._build_bidirectional_rnn(
nmt-master/nmt/model.py:776:                inputs=self.encoder_emb_inp,
nmt-master/nmt/model.py:777:                sequence_length=sequence_length,
nmt-master/nmt/model.py:778:                dtype=dtype,
nmt-master/nmt/model.py:779:                hparams=hparams,
nmt-master/nmt/model.py:780:                num_bi_layers=num_bi_layers,
nmt-master/nmt/model.py:781:                num_bi_residual_layers=num_bi_residual_layers))
nmt-master/nmt/model.py:783:        if num_bi_layers == 1:
nmt-master/nmt/model.py:784:          encoder_state = bi_encoder_state
nmt-master/nmt/model.py:785:        else:
nmt-master/nmt/model.py:786:          # alternatively concat forward and backward states
nmt-master/nmt/model.py:787:          encoder_state = []
nmt-master/nmt/model.py:788:          for layer_id in range(num_bi_layers):
nmt-master/nmt/model.py:789:            encoder_state.append(bi_encoder_state[0][layer_id])  # forward
nmt-master/nmt/model.py:790:            encoder_state.append(bi_encoder_state[1][layer_id])  # backward
nmt-master/nmt/model.py:791:          encoder_state = tuple(encoder_state)
nmt-master/nmt/model.py:792:      else:
nmt-master/nmt/model.py:793:        raise ValueError("Unknown encoder_type %s" % hparams.encoder_type)
nmt-master/nmt/model.py:795:    # Use the top layer for now
nmt-master/nmt/model.py:796:    self.encoder_state_list = [encoder_outputs]
nmt-master/nmt/model.py:798:    return encoder_outputs, encoder_state
nmt-master/nmt/model.py:800:  def _build_encoder(self, hparams):
nmt-master/nmt/model.py:801:    """Build encoder from source."""
nmt-master/nmt/model.py:802:    utils.print_out("# Build a basic encoder")
nmt-master/nmt/model.py:803:    return self._build_encoder_from_sequence(
nmt-master/nmt/model.py:804:        hparams, self.iterator.source, self.iterator.source_sequence_length)
nmt-master/nmt/model.py:806:  def _build_bidirectional_rnn(self, inputs, sequence_length,
nmt-master/nmt/model.py:807:                               dtype, hparams,
nmt-master/nmt/model.py:808:                               num_bi_layers,
nmt-master/nmt/model.py:809:                               num_bi_residual_layers,
nmt-master/nmt/model.py:810:                               base_gpu=0):
nmt-master/nmt/model.py:811:    """Create and call biddirectional RNN cells.
nmt-master/nmt/model.py:813:    Args:
nmt-master/nmt/model.py:814:      num_residual_layers: Number of residual layers from top to bottom. For
nmt-master/nmt/model.py:815:        example, if `num_bi_layers=4` and `num_residual_layers=2`, the last 2 RNN
nmt-master/nmt/model.py:816:        layers in each RNN cell will be wrapped with `ResidualWrapper`.
nmt-master/nmt/model.py:817:      base_gpu: The gpu device id to use for the first forward RNN layer. The
nmt-master/nmt/model.py:818:        i-th forward RNN layer will use `(base_gpu + i) % num_gpus` as its
nmt-master/nmt/model.py:819:        device id. The `base_gpu` for backward RNN cell is `(base_gpu +
nmt-master/nmt/model.py:820:        num_bi_layers)`.
nmt-master/nmt/model.py:822:    Returns:
nmt-master/nmt/model.py:823:      The concatenated bidirectional output and the bidirectional RNN cell"s
nmt-master/nmt/model.py:824:      state.
nmt-master/nmt/model.py:825:    """
nmt-master/nmt/model.py:826:    # Construct forward and backward cells
nmt-master/nmt/model.py:827:    fw_cell = self._build_encoder_cell(hparams,
nmt-master/nmt/model.py:828:                                       num_bi_layers,
nmt-master/nmt/model.py:829:                                       num_bi_residual_layers,
nmt-master/nmt/model.py:830:                                       base_gpu=base_gpu)
nmt-master/nmt/model.py:831:    bw_cell = self._build_encoder_cell(hparams,
nmt-master/nmt/model.py:832:                                       num_bi_layers,
nmt-master/nmt/model.py:833:                                       num_bi_residual_layers,
nmt-master/nmt/model.py:834:                                       base_gpu=(base_gpu + num_bi_layers))
nmt-master/nmt/model.py:836:    bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(
nmt-master/nmt/model.py:837:        fw_cell,
nmt-master/nmt/model.py:838:        bw_cell,
nmt-master/nmt/model.py:839:        inputs,
nmt-master/nmt/model.py:840:        dtype=dtype,
nmt-master/nmt/model.py:841:        sequence_length=sequence_length,
nmt-master/nmt/model.py:842:        time_major=self.time_major,
nmt-master/nmt/model.py:843:        swap_memory=True)
nmt-master/nmt/model.py:845:    return tf.concat(bi_outputs, -1), bi_state
nmt-master/nmt/model.py:847:  def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state,
nmt-master/nmt/model.py:848:                          source_sequence_length, base_gpu=0):
nmt-master/nmt/model.py:849:    """Build an RNN cell that can be used by decoder."""
nmt-master/nmt/model.py:850:    # We only make use of encoder_outputs in attention-based models
nmt-master/nmt/model.py:851:    if hparams.attention:
nmt-master/nmt/model.py:852:      raise ValueError("BasicModel doesn't support attention.")
nmt-master/nmt/model.py:854:    cell = model_helper.create_rnn_cell(
nmt-master/nmt/model.py:855:        unit_type=hparams.unit_type,
nmt-master/nmt/model.py:856:        num_units=self.num_units,
nmt-master/nmt/model.py:857:        num_layers=self.num_decoder_layers,
nmt-master/nmt/model.py:858:        num_residual_layers=self.num_decoder_residual_layers,
nmt-master/nmt/model.py:859:        forget_bias=hparams.forget_bias,
nmt-master/nmt/model.py:860:        dropout=hparams.dropout,
nmt-master/nmt/model.py:861:        num_gpus=self.num_gpus,
nmt-master/nmt/model.py:862:        mode=self.mode,
nmt-master/nmt/model.py:863:        single_cell_fn=self.single_cell_fn,
nmt-master/nmt/model.py:864:        base_gpu=base_gpu
nmt-master/nmt/model.py:865:    )
nmt-master/nmt/model.py:867:    if hparams.language_model:
nmt-master/nmt/model.py:868:      encoder_state = cell.zero_state(self.batch_size, self.dtype)
nmt-master/nmt/model.py:869:    elif not hparams.pass_hidden_state:
nmt-master/nmt/model.py:870:      raise ValueError("For non-attentional model, "
nmt-master/nmt/model.py:871:                       "pass_hidden_state needs to be set to True")
nmt-master/nmt/model.py:873:    # For beam search, we need to replicate encoder infos beam_width times
nmt-master/nmt/model.py:874:    if (self.mode == tf.contrib.learn.ModeKeys.INFER and
nmt-master/nmt/model.py:875:        hparams.infer_mode == "beam_search"):
nmt-master/nmt/model.py:876:      decoder_initial_state = tf.contrib.seq2seq.tile_batch(
nmt-master/nmt/model.py:877:          encoder_state, multiplier=hparams.beam_width)
nmt-master/nmt/model.py:878:    else:
nmt-master/nmt/model.py:879:      decoder_initial_state = encoder_state
nmt-master/nmt/model.py:881:    return cell, decoder_initial_state
nmt-master/nmt/model_helper.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/model_helper.py:2:#
nmt-master/nmt/model_helper.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/model_helper.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/model_helper.py:5:# You may obtain a copy of the License at
nmt-master/nmt/model_helper.py:6:#
nmt-master/nmt/model_helper.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/model_helper.py:8:#
nmt-master/nmt/model_helper.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/model_helper.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/model_helper.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/model_helper.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/model_helper.py:13:# limitations under the License.
nmt-master/nmt/model_helper.py:14:# ==============================================================================
nmt-master/nmt/model_helper.py:16:"""Utility functions for building models."""
nmt-master/nmt/model_helper.py:17:from __future__ import print_function
nmt-master/nmt/model_helper.py:19:import collections
nmt-master/nmt/model_helper.py:20:import os
nmt-master/nmt/model_helper.py:21:import time
nmt-master/nmt/model_helper.py:22:import numpy as np
nmt-master/nmt/model_helper.py:23:import six
nmt-master/nmt/model_helper.py:24:import tensorflow as tf
nmt-master/nmt/model_helper.py:26:from tensorflow.python.ops import lookup_ops
nmt-master/nmt/model_helper.py:27:from .utils import iterator_utils
nmt-master/nmt/model_helper.py:28:from .utils import misc_utils as utils
nmt-master/nmt/model_helper.py:29:from .utils import vocab_utils
nmt-master/nmt/model_helper.py:31:__all__ = [
nmt-master/nmt/model_helper.py:32:    "get_initializer", "get_device_str", "create_train_model",
nmt-master/nmt/model_helper.py:33:    "create_eval_model", "create_infer_model",
nmt-master/nmt/model_helper.py:34:    "create_emb_for_encoder_and_decoder", "create_rnn_cell", "gradient_clip",
nmt-master/nmt/model_helper.py:35:    "create_or_load_model", "load_model", "avg_checkpoints",
nmt-master/nmt/model_helper.py:36:    "compute_perplexity"
nmt-master/nmt/model_helper.py:37:]
nmt-master/nmt/model_helper.py:39:# If a vocab size is greater than this value, put the embedding on cpu instead
nmt-master/nmt/model_helper.py:40:VOCAB_SIZE_THRESHOLD_CPU = 50000
nmt-master/nmt/model_helper.py:43:def get_initializer(init_op, seed=None, init_weight=None):
nmt-master/nmt/model_helper.py:44:  """Create an initializer. init_weight is only for uniform."""
nmt-master/nmt/model_helper.py:45:  if init_op == "uniform":
nmt-master/nmt/model_helper.py:46:    assert init_weight
nmt-master/nmt/model_helper.py:47:    return tf.random_uniform_initializer(
nmt-master/nmt/model_helper.py:48:        -init_weight, init_weight, seed=seed)
nmt-master/nmt/model_helper.py:49:  elif init_op == "glorot_normal":
nmt-master/nmt/model_helper.py:50:    return tf.keras.initializers.glorot_normal(
nmt-master/nmt/model_helper.py:51:        seed=seed)
nmt-master/nmt/model_helper.py:52:  elif init_op == "glorot_uniform":
nmt-master/nmt/model_helper.py:53:    return tf.keras.initializers.glorot_uniform(
nmt-master/nmt/model_helper.py:54:        seed=seed)
nmt-master/nmt/model_helper.py:55:  else:
nmt-master/nmt/model_helper.py:56:    raise ValueError("Unknown init_op %s" % init_op)
nmt-master/nmt/model_helper.py:59:def get_device_str(device_id, num_gpus):
nmt-master/nmt/model_helper.py:60:  """Return a device string for multi-GPU setup."""
nmt-master/nmt/model_helper.py:61:  if num_gpus == 0:
nmt-master/nmt/model_helper.py:62:    return "/cpu:0"
nmt-master/nmt/model_helper.py:63:  device_str_output = "/gpu:%d" % (device_id % num_gpus)
nmt-master/nmt/model_helper.py:64:  return device_str_output
nmt-master/nmt/model_helper.py:67:class ExtraArgs(collections.namedtuple(
nmt-master/nmt/model_helper.py:68:    "ExtraArgs", ("single_cell_fn", "model_device_fn",
nmt-master/nmt/model_helper.py:69:                  "attention_mechanism_fn", "encoder_emb_lookup_fn"))):
nmt-master/nmt/model_helper.py:70:  pass
nmt-master/nmt/model_helper.py:73:class TrainModel(
nmt-master/nmt/model_helper.py:74:    collections.namedtuple("TrainModel", ("graph", "model", "iterator",
nmt-master/nmt/model_helper.py:75:                                          "skip_count_placeholder"))):
nmt-master/nmt/model_helper.py:76:  pass
nmt-master/nmt/model_helper.py:79:def create_train_model(
nmt-master/nmt/model_helper.py:80:    model_creator, hparams, scope=None, num_workers=1, jobid=0,
nmt-master/nmt/model_helper.py:81:    extra_args=None):
nmt-master/nmt/model_helper.py:82:  """Create train graph, model, and iterator."""
nmt-master/nmt/model_helper.py:83:  src_file = "%s.%s" % (hparams.train_prefix, hparams.src)
nmt-master/nmt/model_helper.py:84:  tgt_file = "%s.%s" % (hparams.train_prefix, hparams.tgt)
nmt-master/nmt/model_helper.py:85:  src_vocab_file = hparams.src_vocab_file
nmt-master/nmt/model_helper.py:86:  tgt_vocab_file = hparams.tgt_vocab_file
nmt-master/nmt/model_helper.py:88:  graph = tf.Graph()
nmt-master/nmt/model_helper.py:90:  with graph.as_default(), tf.container(scope or "train"):
nmt-master/nmt/model_helper.py:91:    src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables(
nmt-master/nmt/model_helper.py:92:        src_vocab_file, tgt_vocab_file, hparams.share_vocab)
nmt-master/nmt/model_helper.py:94:    src_dataset = tf.data.TextLineDataset(tf.gfile.Glob(src_file))
nmt-master/nmt/model_helper.py:95:    tgt_dataset = tf.data.TextLineDataset(tf.gfile.Glob(tgt_file))
nmt-master/nmt/model_helper.py:96:    skip_count_placeholder = tf.placeholder(shape=(), dtype=tf.int64)
nmt-master/nmt/model_helper.py:98:    iterator = iterator_utils.get_iterator(
nmt-master/nmt/model_helper.py:99:        src_dataset,
nmt-master/nmt/model_helper.py:100:        tgt_dataset,
nmt-master/nmt/model_helper.py:101:        src_vocab_table,
nmt-master/nmt/model_helper.py:102:        tgt_vocab_table,
nmt-master/nmt/model_helper.py:103:        batch_size=hparams.batch_size,
nmt-master/nmt/model_helper.py:104:        sos=hparams.sos,
nmt-master/nmt/model_helper.py:105:        eos=hparams.eos,
nmt-master/nmt/model_helper.py:106:        random_seed=hparams.random_seed,
nmt-master/nmt/model_helper.py:107:        num_buckets=hparams.num_buckets,
nmt-master/nmt/model_helper.py:108:        src_max_len=hparams.src_max_len,
nmt-master/nmt/model_helper.py:109:        tgt_max_len=hparams.tgt_max_len,
nmt-master/nmt/model_helper.py:110:        skip_count=skip_count_placeholder,
nmt-master/nmt/model_helper.py:111:        num_shards=num_workers,
nmt-master/nmt/model_helper.py:112:        shard_index=jobid,
nmt-master/nmt/model_helper.py:113:        use_char_encode=hparams.use_char_encode)
nmt-master/nmt/model_helper.py:115:    # Note: One can set model_device_fn to
nmt-master/nmt/model_helper.py:116:    # `tf.train.replica_device_setter(ps_tasks)` for distributed training.
nmt-master/nmt/model_helper.py:117:    model_device_fn = None
nmt-master/nmt/model_helper.py:118:    if extra_args: model_device_fn = extra_args.model_device_fn
nmt-master/nmt/model_helper.py:119:    with tf.device(model_device_fn):
nmt-master/nmt/model_helper.py:120:      model = model_creator(
nmt-master/nmt/model_helper.py:121:          hparams,
nmt-master/nmt/model_helper.py:122:          iterator=iterator,
nmt-master/nmt/model_helper.py:123:          mode=tf.contrib.learn.ModeKeys.TRAIN,
nmt-master/nmt/model_helper.py:124:          source_vocab_table=src_vocab_table,
nmt-master/nmt/model_helper.py:125:          target_vocab_table=tgt_vocab_table,
nmt-master/nmt/model_helper.py:126:          scope=scope,
nmt-master/nmt/model_helper.py:127:          extra_args=extra_args)
nmt-master/nmt/model_helper.py:129:  return TrainModel(
nmt-master/nmt/model_helper.py:130:      graph=graph,
nmt-master/nmt/model_helper.py:131:      model=model,
nmt-master/nmt/model_helper.py:132:      iterator=iterator,
nmt-master/nmt/model_helper.py:133:      skip_count_placeholder=skip_count_placeholder)
nmt-master/nmt/model_helper.py:136:class EvalModel(
nmt-master/nmt/model_helper.py:137:    collections.namedtuple("EvalModel",
nmt-master/nmt/model_helper.py:138:                           ("graph", "model", "src_file_placeholder",
nmt-master/nmt/model_helper.py:139:                            "tgt_file_placeholder", "iterator"))):
nmt-master/nmt/model_helper.py:140:  pass
nmt-master/nmt/model_helper.py:143:def create_eval_model(model_creator, hparams, scope=None, extra_args=None):
nmt-master/nmt/model_helper.py:144:  """Create train graph, model, src/tgt file holders, and iterator."""
nmt-master/nmt/model_helper.py:145:  src_vocab_file = hparams.src_vocab_file
nmt-master/nmt/model_helper.py:146:  tgt_vocab_file = hparams.tgt_vocab_file
nmt-master/nmt/model_helper.py:147:  graph = tf.Graph()
nmt-master/nmt/model_helper.py:149:  with graph.as_default(), tf.container(scope or "eval"):
nmt-master/nmt/model_helper.py:150:    src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables(
nmt-master/nmt/model_helper.py:151:        src_vocab_file, tgt_vocab_file, hparams.share_vocab)
nmt-master/nmt/model_helper.py:152:    reverse_tgt_vocab_table = lookup_ops.index_to_string_table_from_file(
nmt-master/nmt/model_helper.py:153:        tgt_vocab_file, default_value=vocab_utils.UNK)
nmt-master/nmt/model_helper.py:155:    src_file_placeholder = tf.placeholder(shape=(), dtype=tf.string)
nmt-master/nmt/model_helper.py:156:    tgt_file_placeholder = tf.placeholder(shape=(), dtype=tf.string)
nmt-master/nmt/model_helper.py:157:    src_dataset = tf.data.TextLineDataset(src_file_placeholder)
nmt-master/nmt/model_helper.py:158:    tgt_dataset = tf.data.TextLineDataset(tgt_file_placeholder)
nmt-master/nmt/model_helper.py:159:    iterator = iterator_utils.get_iterator(
nmt-master/nmt/model_helper.py:160:        src_dataset,
nmt-master/nmt/model_helper.py:161:        tgt_dataset,
nmt-master/nmt/model_helper.py:162:        src_vocab_table,
nmt-master/nmt/model_helper.py:163:        tgt_vocab_table,
nmt-master/nmt/model_helper.py:164:        hparams.batch_size,
nmt-master/nmt/model_helper.py:165:        sos=hparams.sos,
nmt-master/nmt/model_helper.py:166:        eos=hparams.eos,
nmt-master/nmt/model_helper.py:167:        random_seed=hparams.random_seed,
nmt-master/nmt/model_helper.py:168:        num_buckets=hparams.num_buckets,
nmt-master/nmt/model_helper.py:169:        src_max_len=hparams.src_max_len_infer,
nmt-master/nmt/model_helper.py:170:        tgt_max_len=hparams.tgt_max_len_infer,
nmt-master/nmt/model_helper.py:171:        use_char_encode=hparams.use_char_encode)
nmt-master/nmt/model_helper.py:172:    model = model_creator(
nmt-master/nmt/model_helper.py:173:        hparams,
nmt-master/nmt/model_helper.py:174:        iterator=iterator,
nmt-master/nmt/model_helper.py:175:        mode=tf.contrib.learn.ModeKeys.EVAL,
nmt-master/nmt/model_helper.py:176:        source_vocab_table=src_vocab_table,
nmt-master/nmt/model_helper.py:177:        target_vocab_table=tgt_vocab_table,
nmt-master/nmt/model_helper.py:178:        reverse_target_vocab_table=reverse_tgt_vocab_table,
nmt-master/nmt/model_helper.py:179:        scope=scope,
nmt-master/nmt/model_helper.py:180:        extra_args=extra_args)
nmt-master/nmt/model_helper.py:181:  return EvalModel(
nmt-master/nmt/model_helper.py:182:      graph=graph,
nmt-master/nmt/model_helper.py:183:      model=model,
nmt-master/nmt/model_helper.py:184:      src_file_placeholder=src_file_placeholder,
nmt-master/nmt/model_helper.py:185:      tgt_file_placeholder=tgt_file_placeholder,
nmt-master/nmt/model_helper.py:186:      iterator=iterator)
nmt-master/nmt/model_helper.py:189:class InferModel(
nmt-master/nmt/model_helper.py:190:    collections.namedtuple("InferModel",
nmt-master/nmt/model_helper.py:191:                           ("graph", "model", "src_placeholder",
nmt-master/nmt/model_helper.py:192:                            "batch_size_placeholder", "iterator"))):
nmt-master/nmt/model_helper.py:193:  pass
nmt-master/nmt/model_helper.py:196:def create_infer_model(model_creator, hparams, scope=None, extra_args=None):
nmt-master/nmt/model_helper.py:197:  """Create inference model."""
nmt-master/nmt/model_helper.py:198:  graph = tf.Graph()
nmt-master/nmt/model_helper.py:199:  src_vocab_file = hparams.src_vocab_file
nmt-master/nmt/model_helper.py:200:  tgt_vocab_file = hparams.tgt_vocab_file
nmt-master/nmt/model_helper.py:202:  with graph.as_default(), tf.container(scope or "infer"):
nmt-master/nmt/model_helper.py:203:    src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables(
nmt-master/nmt/model_helper.py:204:        src_vocab_file, tgt_vocab_file, hparams.share_vocab)
nmt-master/nmt/model_helper.py:205:    reverse_tgt_vocab_table = lookup_ops.index_to_string_table_from_file(
nmt-master/nmt/model_helper.py:206:        tgt_vocab_file, default_value=vocab_utils.UNK)
nmt-master/nmt/model_helper.py:208:    src_placeholder = tf.placeholder(shape=[None], dtype=tf.string)
nmt-master/nmt/model_helper.py:209:    batch_size_placeholder = tf.placeholder(shape=[], dtype=tf.int64)
nmt-master/nmt/model_helper.py:211:    src_dataset = tf.data.Dataset.from_tensor_slices(
nmt-master/nmt/model_helper.py:212:        src_placeholder)
nmt-master/nmt/model_helper.py:213:    iterator = iterator_utils.get_infer_iterator(
nmt-master/nmt/model_helper.py:214:        src_dataset,
nmt-master/nmt/model_helper.py:215:        src_vocab_table,
nmt-master/nmt/model_helper.py:216:        batch_size=batch_size_placeholder,
nmt-master/nmt/model_helper.py:217:        eos=hparams.eos,
nmt-master/nmt/model_helper.py:218:        src_max_len=hparams.src_max_len_infer,
nmt-master/nmt/model_helper.py:219:        use_char_encode=hparams.use_char_encode)
nmt-master/nmt/model_helper.py:220:    model = model_creator(
nmt-master/nmt/model_helper.py:221:        hparams,
nmt-master/nmt/model_helper.py:222:        iterator=iterator,
nmt-master/nmt/model_helper.py:223:        mode=tf.contrib.learn.ModeKeys.INFER,
nmt-master/nmt/model_helper.py:224:        source_vocab_table=src_vocab_table,
nmt-master/nmt/model_helper.py:225:        target_vocab_table=tgt_vocab_table,
nmt-master/nmt/model_helper.py:226:        reverse_target_vocab_table=reverse_tgt_vocab_table,
nmt-master/nmt/model_helper.py:227:        scope=scope,
nmt-master/nmt/model_helper.py:228:        extra_args=extra_args)
nmt-master/nmt/model_helper.py:229:  return InferModel(
nmt-master/nmt/model_helper.py:230:      graph=graph,
nmt-master/nmt/model_helper.py:231:      model=model,
nmt-master/nmt/model_helper.py:232:      src_placeholder=src_placeholder,
nmt-master/nmt/model_helper.py:233:      batch_size_placeholder=batch_size_placeholder,
nmt-master/nmt/model_helper.py:234:      iterator=iterator)
nmt-master/nmt/model_helper.py:237:def _get_embed_device(vocab_size):
nmt-master/nmt/model_helper.py:238:  """Decide on which device to place an embed matrix given its vocab size."""
nmt-master/nmt/model_helper.py:239:  if vocab_size > VOCAB_SIZE_THRESHOLD_CPU:
nmt-master/nmt/model_helper.py:240:    return "/cpu:0"
nmt-master/nmt/model_helper.py:241:  else:
nmt-master/nmt/model_helper.py:242:    return "/gpu:0"
nmt-master/nmt/model_helper.py:245:def _create_pretrained_emb_from_txt(
nmt-master/nmt/model_helper.py:246:    vocab_file, embed_file, num_trainable_tokens=3, dtype=tf.float32,
nmt-master/nmt/model_helper.py:247:    scope=None):
nmt-master/nmt/model_helper.py:248:  """Load pretrain embeding from embed_file, and return an embedding matrix.
nmt-master/nmt/model_helper.py:250:  Args:
nmt-master/nmt/model_helper.py:251:    embed_file: Path to a Glove formated embedding txt file.
nmt-master/nmt/model_helper.py:252:    num_trainable_tokens: Make the first n tokens in the vocab file as trainable
nmt-master/nmt/model_helper.py:253:      variables. Default is 3, which is "<unk>", "<s>" and "</s>".
nmt-master/nmt/model_helper.py:254:  """
nmt-master/nmt/model_helper.py:255:  vocab, _ = vocab_utils.load_vocab(vocab_file)
nmt-master/nmt/model_helper.py:256:  trainable_tokens = vocab[:num_trainable_tokens]
nmt-master/nmt/model_helper.py:258:  utils.print_out("# Using pretrained embedding: %s." % embed_file)
nmt-master/nmt/model_helper.py:259:  utils.print_out("  with trainable tokens: ")
nmt-master/nmt/model_helper.py:261:  emb_dict, emb_size = vocab_utils.load_embed_txt(embed_file)
nmt-master/nmt/model_helper.py:262:  for token in trainable_tokens:
nmt-master/nmt/model_helper.py:263:    utils.print_out("    %s" % token)
nmt-master/nmt/model_helper.py:264:    if token not in emb_dict:
nmt-master/nmt/model_helper.py:265:      emb_dict[token] = [0.0] * emb_size
nmt-master/nmt/model_helper.py:267:  emb_mat = np.array(
nmt-master/nmt/model_helper.py:268:      [emb_dict[token] for token in vocab], dtype=dtype.as_numpy_dtype())
nmt-master/nmt/model_helper.py:269:  emb_mat = tf.constant(emb_mat)
nmt-master/nmt/model_helper.py:270:  emb_mat_const = tf.slice(emb_mat, [num_trainable_tokens, 0], [-1, -1])
nmt-master/nmt/model_helper.py:271:  with tf.variable_scope(scope or "pretrain_embeddings", dtype=dtype) as scope:
nmt-master/nmt/model_helper.py:272:    with tf.device(_get_embed_device(num_trainable_tokens)):
nmt-master/nmt/model_helper.py:273:      emb_mat_var = tf.get_variable(
nmt-master/nmt/model_helper.py:274:          "emb_mat_var", [num_trainable_tokens, emb_size])
nmt-master/nmt/model_helper.py:275:  return tf.concat([emb_mat_var, emb_mat_const], 0)
nmt-master/nmt/model_helper.py:278:def _create_or_load_embed(embed_name, vocab_file, embed_file,
nmt-master/nmt/model_helper.py:279:                          vocab_size, embed_size, dtype):
nmt-master/nmt/model_helper.py:280:  """Create a new or load an existing embedding matrix."""
nmt-master/nmt/model_helper.py:281:  if vocab_file and embed_file:
nmt-master/nmt/model_helper.py:282:    embedding = _create_pretrained_emb_from_txt(vocab_file, embed_file)
nmt-master/nmt/model_helper.py:283:  else:
nmt-master/nmt/model_helper.py:284:    with tf.device(_get_embed_device(vocab_size)):
nmt-master/nmt/model_helper.py:285:      embedding = tf.get_variable(
nmt-master/nmt/model_helper.py:286:          embed_name, [vocab_size, embed_size], dtype)
nmt-master/nmt/model_helper.py:287:  return embedding
nmt-master/nmt/model_helper.py:290:def create_emb_for_encoder_and_decoder(share_vocab,
nmt-master/nmt/model_helper.py:291:                                       src_vocab_size,
nmt-master/nmt/model_helper.py:292:                                       tgt_vocab_size,
nmt-master/nmt/model_helper.py:293:                                       src_embed_size,
nmt-master/nmt/model_helper.py:294:                                       tgt_embed_size,
nmt-master/nmt/model_helper.py:295:                                       dtype=tf.float32,
nmt-master/nmt/model_helper.py:296:                                       num_enc_partitions=0,
nmt-master/nmt/model_helper.py:297:                                       num_dec_partitions=0,
nmt-master/nmt/model_helper.py:298:                                       src_vocab_file=None,
nmt-master/nmt/model_helper.py:299:                                       tgt_vocab_file=None,
nmt-master/nmt/model_helper.py:300:                                       src_embed_file=None,
nmt-master/nmt/model_helper.py:301:                                       tgt_embed_file=None,
nmt-master/nmt/model_helper.py:302:                                       use_char_encode=False,
nmt-master/nmt/model_helper.py:303:                                       scope=None):
nmt-master/nmt/model_helper.py:304:  """Create embedding matrix for both encoder and decoder.
nmt-master/nmt/model_helper.py:306:  Args:
nmt-master/nmt/model_helper.py:307:    share_vocab: A boolean. Whether to share embedding matrix for both
nmt-master/nmt/model_helper.py:308:      encoder and decoder.
nmt-master/nmt/model_helper.py:309:    src_vocab_size: An integer. The source vocab size.
nmt-master/nmt/model_helper.py:310:    tgt_vocab_size: An integer. The target vocab size.
nmt-master/nmt/model_helper.py:311:    src_embed_size: An integer. The embedding dimension for the encoder's
nmt-master/nmt/model_helper.py:312:      embedding.
nmt-master/nmt/model_helper.py:313:    tgt_embed_size: An integer. The embedding dimension for the decoder's
nmt-master/nmt/model_helper.py:314:      embedding.
nmt-master/nmt/model_helper.py:315:    dtype: dtype of the embedding matrix. Default to float32.
nmt-master/nmt/model_helper.py:316:    num_enc_partitions: number of partitions used for the encoder's embedding
nmt-master/nmt/model_helper.py:317:      vars.
nmt-master/nmt/model_helper.py:318:    num_dec_partitions: number of partitions used for the decoder's embedding
nmt-master/nmt/model_helper.py:319:      vars.
nmt-master/nmt/model_helper.py:320:    scope: VariableScope for the created subgraph. Default to "embedding".
nmt-master/nmt/model_helper.py:322:  Returns:
nmt-master/nmt/model_helper.py:323:    embedding_encoder: Encoder's embedding matrix.
nmt-master/nmt/model_helper.py:324:    embedding_decoder: Decoder's embedding matrix.
nmt-master/nmt/model_helper.py:326:  Raises:
nmt-master/nmt/model_helper.py:327:    ValueError: if use share_vocab but source and target have different vocab
nmt-master/nmt/model_helper.py:328:      size.
nmt-master/nmt/model_helper.py:329:  """
nmt-master/nmt/model_helper.py:330:  if num_enc_partitions <= 1:
nmt-master/nmt/model_helper.py:331:    enc_partitioner = None
nmt-master/nmt/model_helper.py:332:  else:
nmt-master/nmt/model_helper.py:333:    # Note: num_partitions > 1 is required for distributed training due to
nmt-master/nmt/model_helper.py:334:    # embedding_lookup tries to colocate single partition-ed embedding variable
nmt-master/nmt/model_helper.py:335:    # with lookup ops. This may cause embedding variables being placed on worker
nmt-master/nmt/model_helper.py:336:    # jobs.
nmt-master/nmt/model_helper.py:337:    enc_partitioner = tf.fixed_size_partitioner(num_enc_partitions)
nmt-master/nmt/model_helper.py:339:  if num_dec_partitions <= 1:
nmt-master/nmt/model_helper.py:340:    dec_partitioner = None
nmt-master/nmt/model_helper.py:341:  else:
nmt-master/nmt/model_helper.py:342:    # Note: num_partitions > 1 is required for distributed training due to
nmt-master/nmt/model_helper.py:343:    # embedding_lookup tries to colocate single partition-ed embedding variable
nmt-master/nmt/model_helper.py:344:    # with lookup ops. This may cause embedding variables being placed on worker
nmt-master/nmt/model_helper.py:345:    # jobs.
nmt-master/nmt/model_helper.py:346:    dec_partitioner = tf.fixed_size_partitioner(num_dec_partitions)
nmt-master/nmt/model_helper.py:348:  if src_embed_file and enc_partitioner:
nmt-master/nmt/model_helper.py:349:    raise ValueError(
nmt-master/nmt/model_helper.py:350:        "Can't set num_enc_partitions > 1 when using pretrained encoder "
nmt-master/nmt/model_helper.py:351:        "embedding")
nmt-master/nmt/model_helper.py:353:  if tgt_embed_file and dec_partitioner:
nmt-master/nmt/model_helper.py:354:    raise ValueError(
nmt-master/nmt/model_helper.py:355:        "Can't set num_dec_partitions > 1 when using pretrained decdoer "
nmt-master/nmt/model_helper.py:356:        "embedding")
nmt-master/nmt/model_helper.py:358:  with tf.variable_scope(
nmt-master/nmt/model_helper.py:359:      scope or "embeddings", dtype=dtype, partitioner=enc_partitioner) as scope:
nmt-master/nmt/model_helper.py:360:    # Share embedding
nmt-master/nmt/model_helper.py:361:    if share_vocab:
nmt-master/nmt/model_helper.py:362:      if src_vocab_size != tgt_vocab_size:
nmt-master/nmt/model_helper.py:363:        raise ValueError("Share embedding but different src/tgt vocab sizes"
nmt-master/nmt/model_helper.py:364:                         " %d vs. %d" % (src_vocab_size, tgt_vocab_size))
nmt-master/nmt/model_helper.py:365:      assert src_embed_size == tgt_embed_size
nmt-master/nmt/model_helper.py:366:      utils.print_out("# Use the same embedding for source and target")
nmt-master/nmt/model_helper.py:367:      vocab_file = src_vocab_file or tgt_vocab_file
nmt-master/nmt/model_helper.py:368:      embed_file = src_embed_file or tgt_embed_file
nmt-master/nmt/model_helper.py:370:      embedding_encoder = _create_or_load_embed(
nmt-master/nmt/model_helper.py:371:          "embedding_share", vocab_file, embed_file,
nmt-master/nmt/model_helper.py:372:          src_vocab_size, src_embed_size, dtype)
nmt-master/nmt/model_helper.py:373:      embedding_decoder = embedding_encoder
nmt-master/nmt/model_helper.py:374:    else:
nmt-master/nmt/model_helper.py:375:      if not use_char_encode:
nmt-master/nmt/model_helper.py:376:        with tf.variable_scope("encoder", partitioner=enc_partitioner):
nmt-master/nmt/model_helper.py:377:          embedding_encoder = _create_or_load_embed(
nmt-master/nmt/model_helper.py:378:              "embedding_encoder", src_vocab_file, src_embed_file,
nmt-master/nmt/model_helper.py:379:              src_vocab_size, src_embed_size, dtype)
nmt-master/nmt/model_helper.py:380:      else:
nmt-master/nmt/model_helper.py:381:        embedding_encoder = None
nmt-master/nmt/model_helper.py:383:      with tf.variable_scope("decoder", partitioner=dec_partitioner):
nmt-master/nmt/model_helper.py:384:        embedding_decoder = _create_or_load_embed(
nmt-master/nmt/model_helper.py:385:            "embedding_decoder", tgt_vocab_file, tgt_embed_file,
nmt-master/nmt/model_helper.py:386:            tgt_vocab_size, tgt_embed_size, dtype)
nmt-master/nmt/model_helper.py:388:  return embedding_encoder, embedding_decoder
nmt-master/nmt/model_helper.py:391:def _single_cell(unit_type, num_units, forget_bias, dropout, mode,
nmt-master/nmt/model_helper.py:392:                 residual_connection=False, device_str=None, residual_fn=None):
nmt-master/nmt/model_helper.py:393:  """Create an instance of a single RNN cell."""
nmt-master/nmt/model_helper.py:394:  # dropout (= 1 - keep_prob) is set to 0 during eval and infer
nmt-master/nmt/model_helper.py:395:  dropout = dropout if mode == tf.contrib.learn.ModeKeys.TRAIN else 0.0
nmt-master/nmt/model_helper.py:397:  # Cell Type
nmt-master/nmt/model_helper.py:398:  if unit_type == "lstm":
nmt-master/nmt/model_helper.py:399:    utils.print_out("  LSTM, forget_bias=%g" % forget_bias, new_line=False)
nmt-master/nmt/model_helper.py:400:    single_cell = tf.contrib.rnn.BasicLSTMCell(
nmt-master/nmt/model_helper.py:401:        num_units,
nmt-master/nmt/model_helper.py:402:        forget_bias=forget_bias)
nmt-master/nmt/model_helper.py:403:  elif unit_type == "gru":
nmt-master/nmt/model_helper.py:404:    utils.print_out("  GRU", new_line=False)
nmt-master/nmt/model_helper.py:405:    single_cell = tf.contrib.rnn.GRUCell(num_units)
nmt-master/nmt/model_helper.py:406:  elif unit_type == "layer_norm_lstm":
nmt-master/nmt/model_helper.py:407:    utils.print_out("  Layer Normalized LSTM, forget_bias=%g" % forget_bias,
nmt-master/nmt/model_helper.py:408:                    new_line=False)
nmt-master/nmt/model_helper.py:409:    single_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(
nmt-master/nmt/model_helper.py:410:        num_units,
nmt-master/nmt/model_helper.py:411:        forget_bias=forget_bias,
nmt-master/nmt/model_helper.py:412:        layer_norm=True)
nmt-master/nmt/model_helper.py:413:  elif unit_type == "nas":
nmt-master/nmt/model_helper.py:414:    utils.print_out("  NASCell", new_line=False)
nmt-master/nmt/model_helper.py:415:    single_cell = tf.contrib.rnn.NASCell(num_units)
nmt-master/nmt/model_helper.py:416:  else:
nmt-master/nmt/model_helper.py:417:    raise ValueError("Unknown unit type %s!" % unit_type)
nmt-master/nmt/model_helper.py:419:  # Dropout (= 1 - keep_prob)
nmt-master/nmt/model_helper.py:420:  if dropout > 0.0:
nmt-master/nmt/model_helper.py:421:    single_cell = tf.contrib.rnn.DropoutWrapper(
nmt-master/nmt/model_helper.py:422:        cell=single_cell, input_keep_prob=(1.0 - dropout))
nmt-master/nmt/model_helper.py:423:    utils.print_out("  %s, dropout=%g " %(type(single_cell).__name__, dropout),
nmt-master/nmt/model_helper.py:424:                    new_line=False)
nmt-master/nmt/model_helper.py:426:  # Residual
nmt-master/nmt/model_helper.py:427:  if residual_connection:
nmt-master/nmt/model_helper.py:428:    single_cell = tf.contrib.rnn.ResidualWrapper(
nmt-master/nmt/model_helper.py:429:        single_cell, residual_fn=residual_fn)
nmt-master/nmt/model_helper.py:430:    utils.print_out("  %s" % type(single_cell).__name__, new_line=False)
nmt-master/nmt/model_helper.py:432:  # Device Wrapper
nmt-master/nmt/model_helper.py:433:  if device_str:
nmt-master/nmt/model_helper.py:434:    single_cell = tf.contrib.rnn.DeviceWrapper(single_cell, device_str)
nmt-master/nmt/model_helper.py:435:    utils.print_out("  %s, device=%s" %
nmt-master/nmt/model_helper.py:436:                    (type(single_cell).__name__, device_str), new_line=False)
nmt-master/nmt/model_helper.py:438:  return single_cell
nmt-master/nmt/model_helper.py:441:def _cell_list(unit_type, num_units, num_layers, num_residual_layers,
nmt-master/nmt/model_helper.py:442:               forget_bias, dropout, mode, num_gpus, base_gpu=0,
nmt-master/nmt/model_helper.py:443:               single_cell_fn=None, residual_fn=None):
nmt-master/nmt/model_helper.py:444:  """Create a list of RNN cells."""
nmt-master/nmt/model_helper.py:445:  if not single_cell_fn:
nmt-master/nmt/model_helper.py:446:    single_cell_fn = _single_cell
nmt-master/nmt/model_helper.py:448:  # Multi-GPU
nmt-master/nmt/model_helper.py:449:  cell_list = []
nmt-master/nmt/model_helper.py:450:  for i in range(num_layers):
nmt-master/nmt/model_helper.py:451:    utils.print_out("  cell %d" % i, new_line=False)
nmt-master/nmt/model_helper.py:452:    single_cell = single_cell_fn(
nmt-master/nmt/model_helper.py:453:        unit_type=unit_type,
nmt-master/nmt/model_helper.py:454:        num_units=num_units,
nmt-master/nmt/model_helper.py:455:        forget_bias=forget_bias,
nmt-master/nmt/model_helper.py:456:        dropout=dropout,
nmt-master/nmt/model_helper.py:457:        mode=mode,
nmt-master/nmt/model_helper.py:458:        residual_connection=(i >= num_layers - num_residual_layers),
nmt-master/nmt/model_helper.py:459:        device_str=get_device_str(i + base_gpu, num_gpus),
nmt-master/nmt/model_helper.py:460:        residual_fn=residual_fn
nmt-master/nmt/model_helper.py:461:    )
nmt-master/nmt/model_helper.py:462:    utils.print_out("")
nmt-master/nmt/model_helper.py:463:    cell_list.append(single_cell)
nmt-master/nmt/model_helper.py:465:  return cell_list
nmt-master/nmt/model_helper.py:468:def create_rnn_cell(unit_type, num_units, num_layers, num_residual_layers,
nmt-master/nmt/model_helper.py:469:                    forget_bias, dropout, mode, num_gpus, base_gpu=0,
nmt-master/nmt/model_helper.py:470:                    single_cell_fn=None):
nmt-master/nmt/model_helper.py:471:  """Create multi-layer RNN cell.
nmt-master/nmt/model_helper.py:473:  Args:
nmt-master/nmt/model_helper.py:474:    unit_type: string representing the unit type, i.e. "lstm".
nmt-master/nmt/model_helper.py:475:    num_units: the depth of each unit.
nmt-master/nmt/model_helper.py:476:    num_layers: number of cells.
nmt-master/nmt/model_helper.py:477:    num_residual_layers: Number of residual layers from top to bottom. For
nmt-master/nmt/model_helper.py:478:      example, if `num_layers=4` and `num_residual_layers=2`, the last 2 RNN
nmt-master/nmt/model_helper.py:479:      cells in the returned list will be wrapped with `ResidualWrapper`.
nmt-master/nmt/model_helper.py:480:    forget_bias: the initial forget bias of the RNNCell(s).
nmt-master/nmt/model_helper.py:481:    dropout: floating point value between 0.0 and 1.0:
nmt-master/nmt/model_helper.py:482:      the probability of dropout.  this is ignored if `mode != TRAIN`.
nmt-master/nmt/model_helper.py:483:    mode: either tf.contrib.learn.TRAIN/EVAL/INFER
nmt-master/nmt/model_helper.py:484:    num_gpus: The number of gpus to use when performing round-robin
nmt-master/nmt/model_helper.py:485:      placement of layers.
nmt-master/nmt/model_helper.py:486:    base_gpu: The gpu device id to use for the first RNN cell in the
nmt-master/nmt/model_helper.py:487:      returned list. The i-th RNN cell will use `(base_gpu + i) % num_gpus`
nmt-master/nmt/model_helper.py:488:      as its device id.
nmt-master/nmt/model_helper.py:489:    single_cell_fn: allow for adding customized cell.
nmt-master/nmt/model_helper.py:490:      When not specified, we default to model_helper._single_cell
nmt-master/nmt/model_helper.py:491:  Returns:
nmt-master/nmt/model_helper.py:492:    An `RNNCell` instance.
nmt-master/nmt/model_helper.py:493:  """
nmt-master/nmt/model_helper.py:494:  cell_list = _cell_list(unit_type=unit_type,
nmt-master/nmt/model_helper.py:495:                         num_units=num_units,
nmt-master/nmt/model_helper.py:496:                         num_layers=num_layers,
nmt-master/nmt/model_helper.py:497:                         num_residual_layers=num_residual_layers,
nmt-master/nmt/model_helper.py:498:                         forget_bias=forget_bias,
nmt-master/nmt/model_helper.py:499:                         dropout=dropout,
nmt-master/nmt/model_helper.py:500:                         mode=mode,
nmt-master/nmt/model_helper.py:501:                         num_gpus=num_gpus,
nmt-master/nmt/model_helper.py:502:                         base_gpu=base_gpu,
nmt-master/nmt/model_helper.py:503:                         single_cell_fn=single_cell_fn)
nmt-master/nmt/model_helper.py:505:  if len(cell_list) == 1:  # Single layer.
nmt-master/nmt/model_helper.py:506:    return cell_list[0]
nmt-master/nmt/model_helper.py:507:  else:  # Multi layers
nmt-master/nmt/model_helper.py:508:    return tf.contrib.rnn.MultiRNNCell(cell_list)
nmt-master/nmt/model_helper.py:511:def gradient_clip(gradients, max_gradient_norm):
nmt-master/nmt/model_helper.py:512:  """Clipping gradients of a model."""
nmt-master/nmt/model_helper.py:513:  clipped_gradients, gradient_norm = tf.clip_by_global_norm(
nmt-master/nmt/model_helper.py:514:      gradients, max_gradient_norm)
nmt-master/nmt/model_helper.py:515:  gradient_norm_summary = [tf.summary.scalar("grad_norm", gradient_norm)]
nmt-master/nmt/model_helper.py:516:  gradient_norm_summary.append(
nmt-master/nmt/model_helper.py:517:      tf.summary.scalar("clipped_gradient", tf.global_norm(clipped_gradients)))
nmt-master/nmt/model_helper.py:519:  return clipped_gradients, gradient_norm_summary, gradient_norm
nmt-master/nmt/model_helper.py:522:def print_variables_in_ckpt(ckpt_path):
nmt-master/nmt/model_helper.py:523:  """Print a list of variables in a checkpoint together with their shapes."""
nmt-master/nmt/model_helper.py:524:  utils.print_out("# Variables in ckpt %s" % ckpt_path)
nmt-master/nmt/model_helper.py:525:  reader = tf.train.NewCheckpointReader(ckpt_path)
nmt-master/nmt/model_helper.py:526:  variable_map = reader.get_variable_to_shape_map()
nmt-master/nmt/model_helper.py:527:  for key in sorted(variable_map.keys()):
nmt-master/nmt/model_helper.py:528:    utils.print_out("  %s: %s" % (key, variable_map[key]))
nmt-master/nmt/model_helper.py:531:def load_model(model, ckpt_path, session, name):
nmt-master/nmt/model_helper.py:532:  """Load model from a checkpoint."""
nmt-master/nmt/model_helper.py:533:  start_time = time.time()
nmt-master/nmt/model_helper.py:534:  try:
nmt-master/nmt/model_helper.py:535:    model.saver.restore(session, ckpt_path)
nmt-master/nmt/model_helper.py:536:  except tf.errors.NotFoundError as e:
nmt-master/nmt/model_helper.py:537:    utils.print_out("Can't load checkpoint")
nmt-master/nmt/model_helper.py:538:    print_variables_in_ckpt(ckpt_path)
nmt-master/nmt/model_helper.py:539:    utils.print_out("%s" % str(e))
nmt-master/nmt/model_helper.py:541:  session.run(tf.tables_initializer())
nmt-master/nmt/model_helper.py:542:  utils.print_out(
nmt-master/nmt/model_helper.py:543:      "  loaded %s model parameters from %s, time %.2fs" %
nmt-master/nmt/model_helper.py:544:      (name, ckpt_path, time.time() - start_time))
nmt-master/nmt/model_helper.py:545:  return model
nmt-master/nmt/model_helper.py:548:def avg_checkpoints(model_dir, num_last_checkpoints, global_step,
nmt-master/nmt/model_helper.py:549:                    global_step_name):
nmt-master/nmt/model_helper.py:550:  """Average the last N checkpoints in the model_dir."""
nmt-master/nmt/model_helper.py:551:  checkpoint_state = tf.train.get_checkpoint_state(model_dir)
nmt-master/nmt/model_helper.py:552:  if not checkpoint_state:
nmt-master/nmt/model_helper.py:553:    utils.print_out("# No checkpoint file found in directory: %s" % model_dir)
nmt-master/nmt/model_helper.py:554:    return None
nmt-master/nmt/model_helper.py:556:  # Checkpoints are ordered from oldest to newest.
nmt-master/nmt/model_helper.py:557:  checkpoints = (
nmt-master/nmt/model_helper.py:558:      checkpoint_state.all_model_checkpoint_paths[-num_last_checkpoints:])
nmt-master/nmt/model_helper.py:560:  if len(checkpoints) < num_last_checkpoints:
nmt-master/nmt/model_helper.py:561:    utils.print_out(
nmt-master/nmt/model_helper.py:562:        "# Skipping averaging checkpoints because not enough checkpoints is "
nmt-master/nmt/model_helper.py:563:        "avaliable."
nmt-master/nmt/model_helper.py:564:    )
nmt-master/nmt/model_helper.py:565:    return None
nmt-master/nmt/model_helper.py:567:  avg_model_dir = os.path.join(model_dir, "avg_checkpoints")
nmt-master/nmt/model_helper.py:568:  if not tf.gfile.Exists(avg_model_dir):
nmt-master/nmt/model_helper.py:569:    utils.print_out(
nmt-master/nmt/model_helper.py:570:        "# Creating new directory %s for saving averaged checkpoints." %
nmt-master/nmt/model_helper.py:571:        avg_model_dir)
nmt-master/nmt/model_helper.py:572:    tf.gfile.MakeDirs(avg_model_dir)
nmt-master/nmt/model_helper.py:574:  utils.print_out("# Reading and averaging variables in checkpoints:")
nmt-master/nmt/model_helper.py:575:  var_list = tf.contrib.framework.list_variables(checkpoints[0])
nmt-master/nmt/model_helper.py:576:  var_values, var_dtypes = {}, {}
nmt-master/nmt/model_helper.py:577:  for (name, shape) in var_list:
nmt-master/nmt/model_helper.py:578:    if name != global_step_name:
nmt-master/nmt/model_helper.py:579:      var_values[name] = np.zeros(shape)
nmt-master/nmt/model_helper.py:581:  for checkpoint in checkpoints:
nmt-master/nmt/model_helper.py:582:    utils.print_out("    %s" % checkpoint)
nmt-master/nmt/model_helper.py:583:    reader = tf.contrib.framework.load_checkpoint(checkpoint)
nmt-master/nmt/model_helper.py:584:    for name in var_values:
nmt-master/nmt/model_helper.py:585:      tensor = reader.get_tensor(name)
nmt-master/nmt/model_helper.py:586:      var_dtypes[name] = tensor.dtype
nmt-master/nmt/model_helper.py:587:      var_values[name] += tensor
nmt-master/nmt/model_helper.py:589:  for name in var_values:
nmt-master/nmt/model_helper.py:590:    var_values[name] /= len(checkpoints)
nmt-master/nmt/model_helper.py:592:  # Build a graph with same variables in the checkpoints, and save the averaged
nmt-master/nmt/model_helper.py:593:  # variables into the avg_model_dir.
nmt-master/nmt/model_helper.py:594:  with tf.Graph().as_default():
nmt-master/nmt/model_helper.py:595:    tf_vars = [
nmt-master/nmt/model_helper.py:596:        tf.get_variable(v, shape=var_values[v].shape, dtype=var_dtypes[name])
nmt-master/nmt/model_helper.py:597:        for v in var_values
nmt-master/nmt/model_helper.py:598:    ]
nmt-master/nmt/model_helper.py:600:    placeholders = [tf.placeholder(v.dtype, shape=v.shape) for v in tf_vars]
nmt-master/nmt/model_helper.py:601:    assign_ops = [tf.assign(v, p) for (v, p) in zip(tf_vars, placeholders)]
nmt-master/nmt/model_helper.py:602:    global_step_var = tf.Variable(
nmt-master/nmt/model_helper.py:603:        global_step, name=global_step_name, trainable=False)
nmt-master/nmt/model_helper.py:604:    saver = tf.train.Saver(tf.all_variables())
nmt-master/nmt/model_helper.py:606:    with tf.Session() as sess:
nmt-master/nmt/model_helper.py:607:      sess.run(tf.initialize_all_variables())
nmt-master/nmt/model_helper.py:608:      for p, assign_op, (name, value) in zip(placeholders, assign_ops,
nmt-master/nmt/model_helper.py:609:                                             six.iteritems(var_values)):
nmt-master/nmt/model_helper.py:610:        sess.run(assign_op, {p: value})
nmt-master/nmt/model_helper.py:612:      # Use the built saver to save the averaged checkpoint. Only keep 1
nmt-master/nmt/model_helper.py:613:      # checkpoint and the best checkpoint will be moved to avg_best_metric_dir.
nmt-master/nmt/model_helper.py:614:      saver.save(
nmt-master/nmt/model_helper.py:615:          sess,
nmt-master/nmt/model_helper.py:616:          os.path.join(avg_model_dir, "translate.ckpt"))
nmt-master/nmt/model_helper.py:618:  return avg_model_dir
nmt-master/nmt/model_helper.py:621:def create_or_load_model(model, model_dir, session, name):
nmt-master/nmt/model_helper.py:622:  """Create translation model and initialize or load parameters in session."""
nmt-master/nmt/model_helper.py:623:  latest_ckpt = tf.train.latest_checkpoint(model_dir)
nmt-master/nmt/model_helper.py:624:  if latest_ckpt:
nmt-master/nmt/model_helper.py:625:    model = load_model(model, latest_ckpt, session, name)
nmt-master/nmt/model_helper.py:626:  else:
nmt-master/nmt/model_helper.py:627:    start_time = time.time()
nmt-master/nmt/model_helper.py:628:    session.run(tf.global_variables_initializer())
nmt-master/nmt/model_helper.py:629:    session.run(tf.tables_initializer())
nmt-master/nmt/model_helper.py:630:    utils.print_out("  created %s model with fresh parameters, time %.2fs" %
nmt-master/nmt/model_helper.py:631:                    (name, time.time() - start_time))
nmt-master/nmt/model_helper.py:633:  global_step = model.global_step.eval(session=session)
nmt-master/nmt/model_helper.py:634:  return model, global_step
nmt-master/nmt/model_helper.py:637:def compute_perplexity(model, sess, name):
nmt-master/nmt/model_helper.py:638:  """Compute perplexity of the output of the model.
nmt-master/nmt/model_helper.py:640:  Args:
nmt-master/nmt/model_helper.py:641:    model: model for compute perplexity.
nmt-master/nmt/model_helper.py:642:    sess: tensorflow session to use.
nmt-master/nmt/model_helper.py:643:    name: name of the batch.
nmt-master/nmt/model_helper.py:645:  Returns:
nmt-master/nmt/model_helper.py:646:    The perplexity of the eval outputs.
nmt-master/nmt/model_helper.py:647:  """
nmt-master/nmt/model_helper.py:648:  total_loss = 0
nmt-master/nmt/model_helper.py:649:  total_predict_count = 0
nmt-master/nmt/model_helper.py:650:  start_time = time.time()
nmt-master/nmt/model_helper.py:652:  while True:
nmt-master/nmt/model_helper.py:653:    try:
nmt-master/nmt/model_helper.py:654:      output_tuple = model.eval(sess)
nmt-master/nmt/model_helper.py:655:      total_loss += output_tuple.eval_loss * output_tuple.batch_size
nmt-master/nmt/model_helper.py:656:      total_predict_count += output_tuple.predict_count
nmt-master/nmt/model_helper.py:657:    except tf.errors.OutOfRangeError:
nmt-master/nmt/model_helper.py:658:      break
nmt-master/nmt/model_helper.py:660:  perplexity = utils.safe_exp(total_loss / total_predict_count)
nmt-master/nmt/model_helper.py:661:  utils.print_time("  eval %s: perplexity %.2f" % (name, perplexity),
nmt-master/nmt/model_helper.py:662:                   start_time)
nmt-master/nmt/model_helper.py:663:  return perplexity
nmt-master/nmt/model_test.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/model_test.py:2:#
nmt-master/nmt/model_test.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/model_test.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/model_test.py:5:# You may obtain a copy of the License at
nmt-master/nmt/model_test.py:6:#
nmt-master/nmt/model_test.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/model_test.py:8:#
nmt-master/nmt/model_test.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/model_test.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/model_test.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/model_test.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/model_test.py:13:# limitations under the License.
nmt-master/nmt/model_test.py:14:# ==============================================================================
nmt-master/nmt/model_test.py:15:"""Tests for model.py."""
nmt-master/nmt/model_test.py:17:from __future__ import absolute_import
nmt-master/nmt/model_test.py:18:from __future__ import division
nmt-master/nmt/model_test.py:19:from __future__ import print_function
nmt-master/nmt/model_test.py:21:import pprint
nmt-master/nmt/model_test.py:22:import sys
nmt-master/nmt/model_test.py:23:import numpy as np
nmt-master/nmt/model_test.py:24:import tensorflow as tf
nmt-master/nmt/model_test.py:26:from . import attention_model
nmt-master/nmt/model_test.py:27:from . import gnmt_model
nmt-master/nmt/model_test.py:28:from . import model
nmt-master/nmt/model_test.py:29:from .utils import common_test_utils
nmt-master/nmt/model_test.py:30:from .utils import nmt_utils
nmt-master/nmt/model_test.py:33:float32 = np.float32
nmt-master/nmt/model_test.py:34:int32 = np.int32
nmt-master/nmt/model_test.py:35:array = np.array
nmt-master/nmt/model_test.py:37:SOS = '<s>'
nmt-master/nmt/model_test.py:38:EOS = '</s>'
nmt-master/nmt/model_test.py:41:class ModelTest(tf.test.TestCase):
nmt-master/nmt/model_test.py:43:  @classmethod
nmt-master/nmt/model_test.py:44:  def setUpClass(cls):
nmt-master/nmt/model_test.py:45:    cls.actual_vars_values = {}
nmt-master/nmt/model_test.py:46:    cls.expected_vars_values = {
nmt-master/nmt/model_test.py:47:        'AttentionMechanismBahdanau/att_layer_weight/shape': (10, 5),
nmt-master/nmt/model_test.py:48:        'AttentionMechanismBahdanau/att_layer_weight/sum':
nmt-master/nmt/model_test.py:49:            -0.64981574,
nmt-master/nmt/model_test.py:50:        'AttentionMechanismBahdanau/last_dec_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:51:        'AttentionMechanismBahdanau/last_dec_weight/sum':
nmt-master/nmt/model_test.py:52:            0.058069646,
nmt-master/nmt/model_test.py:53:        'AttentionMechanismBahdanau/last_enc_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:54:        'AttentionMechanismBahdanau/last_enc_weight/sum':
nmt-master/nmt/model_test.py:55:            0.058028102,
nmt-master/nmt/model_test.py:56:        'AttentionMechanismLuong/att_layer_weight/shape': (10, 5),
nmt-master/nmt/model_test.py:57:        'AttentionMechanismLuong/att_layer_weight/sum':
nmt-master/nmt/model_test.py:58:            -0.64981574,
nmt-master/nmt/model_test.py:59:        'AttentionMechanismLuong/last_dec_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:60:        'AttentionMechanismLuong/last_dec_weight/sum':
nmt-master/nmt/model_test.py:61:            0.058069646,
nmt-master/nmt/model_test.py:62:        'AttentionMechanismLuong/last_enc_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:63:        'AttentionMechanismLuong/last_enc_weight/sum':
nmt-master/nmt/model_test.py:64:            0.058028102,
nmt-master/nmt/model_test.py:65:        'AttentionMechanismNormedBahdanau/att_layer_weight/shape': (10, 5),
nmt-master/nmt/model_test.py:66:        'AttentionMechanismNormedBahdanau/att_layer_weight/sum':
nmt-master/nmt/model_test.py:67:            -0.64981973,
nmt-master/nmt/model_test.py:68:        'AttentionMechanismNormedBahdanau/last_dec_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:69:        'AttentionMechanismNormedBahdanau/last_dec_weight/sum':
nmt-master/nmt/model_test.py:70:            0.058067322,
nmt-master/nmt/model_test.py:71:        'AttentionMechanismNormedBahdanau/last_enc_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:72:        'AttentionMechanismNormedBahdanau/last_enc_weight/sum':
nmt-master/nmt/model_test.py:73:            0.058022559,
nmt-master/nmt/model_test.py:74:        'AttentionMechanismScaledLuong/att_layer_weight/shape': (10, 5),
nmt-master/nmt/model_test.py:75:        'AttentionMechanismScaledLuong/att_layer_weight/sum':
nmt-master/nmt/model_test.py:76:            -0.64981574,
nmt-master/nmt/model_test.py:77:        'AttentionMechanismScaledLuong/last_dec_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:78:        'AttentionMechanismScaledLuong/last_dec_weight/sum':
nmt-master/nmt/model_test.py:79:            0.058069646,
nmt-master/nmt/model_test.py:80:        'AttentionMechanismScaledLuong/last_enc_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:81:        'AttentionMechanismScaledLuong/last_enc_weight/sum':
nmt-master/nmt/model_test.py:82:            0.058028102,
nmt-master/nmt/model_test.py:83:        'GNMTModel_gnmt/last_dec_weight/shape': (15, 20),
nmt-master/nmt/model_test.py:84:        'GNMTModel_gnmt/last_dec_weight/sum':
nmt-master/nmt/model_test.py:85:            -0.48634407,
nmt-master/nmt/model_test.py:86:        'GNMTModel_gnmt/last_enc_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:87:        'GNMTModel_gnmt/last_enc_weight/sum':
nmt-master/nmt/model_test.py:88:            0.058025002,
nmt-master/nmt/model_test.py:89:        'GNMTModel_gnmt/mem_layer_weight/shape': (5, 5),
nmt-master/nmt/model_test.py:90:        'GNMTModel_gnmt/mem_layer_weight/sum':
nmt-master/nmt/model_test.py:91:            -0.44815454,
nmt-master/nmt/model_test.py:92:        'GNMTModel_gnmt_v2/last_dec_weight/shape': (15, 20),
nmt-master/nmt/model_test.py:93:        'GNMTModel_gnmt_v2/last_dec_weight/sum':
nmt-master/nmt/model_test.py:94:            -0.48634392,
nmt-master/nmt/model_test.py:95:        'GNMTModel_gnmt_v2/last_enc_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:96:        'GNMTModel_gnmt_v2/last_enc_weight/sum':
nmt-master/nmt/model_test.py:97:            0.058024824,
nmt-master/nmt/model_test.py:98:        'GNMTModel_gnmt_v2/mem_layer_weight/shape': (5, 5),
nmt-master/nmt/model_test.py:99:        'GNMTModel_gnmt_v2/mem_layer_weight/sum':
nmt-master/nmt/model_test.py:100:            -0.44815454,
nmt-master/nmt/model_test.py:101:        'NoAttentionNoResidualUniEncoder/last_dec_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:102:        'NoAttentionNoResidualUniEncoder/last_dec_weight/sum':
nmt-master/nmt/model_test.py:103:            0.057424068,
nmt-master/nmt/model_test.py:104:        'NoAttentionNoResidualUniEncoder/last_enc_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:105:        'NoAttentionNoResidualUniEncoder/last_enc_weight/sum':
nmt-master/nmt/model_test.py:106:            0.058453858,
nmt-master/nmt/model_test.py:107:        'NoAttentionResidualBiEncoder/last_dec_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:108:        'NoAttentionResidualBiEncoder/last_dec_weight/sum':
nmt-master/nmt/model_test.py:109:            0.058025062,
nmt-master/nmt/model_test.py:110:        'NoAttentionResidualBiEncoder/last_enc_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:111:        'NoAttentionResidualBiEncoder/last_enc_weight/sum':
nmt-master/nmt/model_test.py:112:            0.058053195,
nmt-master/nmt/model_test.py:113:        'UniEncoderBottomAttentionArchitecture/last_dec_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:114:        'UniEncoderBottomAttentionArchitecture/last_dec_weight/sum':
nmt-master/nmt/model_test.py:115:            0.058024943,
nmt-master/nmt/model_test.py:116:        'UniEncoderBottomAttentionArchitecture/last_enc_weight/shape': (10, 20),
nmt-master/nmt/model_test.py:117:        'UniEncoderBottomAttentionArchitecture/last_enc_weight/sum':
nmt-master/nmt/model_test.py:118:            0.058025122,
nmt-master/nmt/model_test.py:119:        'UniEncoderBottomAttentionArchitecture/mem_layer_weight/shape': (5, 5),
nmt-master/nmt/model_test.py:120:        'UniEncoderBottomAttentionArchitecture/mem_layer_weight/sum':
nmt-master/nmt/model_test.py:121:            -0.44815454,
nmt-master/nmt/model_test.py:122:        'UniEncoderStandardAttentionArchitecture/last_dec_weight/shape': (10,
nmt-master/nmt/model_test.py:123:                                                                          20),
nmt-master/nmt/model_test.py:124:        'UniEncoderStandardAttentionArchitecture/last_dec_weight/sum':
nmt-master/nmt/model_test.py:125:            0.058025002,
nmt-master/nmt/model_test.py:126:        'UniEncoderStandardAttentionArchitecture/last_enc_weight/shape': (10,
nmt-master/nmt/model_test.py:127:                                                                          20),
nmt-master/nmt/model_test.py:128:        'UniEncoderStandardAttentionArchitecture/last_enc_weight/sum':
nmt-master/nmt/model_test.py:129:            0.058024883,
nmt-master/nmt/model_test.py:130:        'UniEncoderStandardAttentionArchitecture/mem_layer_weight/shape': (5,
nmt-master/nmt/model_test.py:131:                                                                           5),
nmt-master/nmt/model_test.py:132:        'UniEncoderStandardAttentionArchitecture/mem_layer_weight/sum':
nmt-master/nmt/model_test.py:133:            -0.44815454,
nmt-master/nmt/model_test.py:134:    }
nmt-master/nmt/model_test.py:136:    cls.actual_train_values = {}
nmt-master/nmt/model_test.py:137:    cls.expected_train_values = {
nmt-master/nmt/model_test.py:138:        'AttentionMechanismBahdanau/loss': 8.8519039,
nmt-master/nmt/model_test.py:139:        'AttentionMechanismLuong/loss': 8.8519039,
nmt-master/nmt/model_test.py:140:        'AttentionMechanismNormedBahdanau/loss': 8.851902,
nmt-master/nmt/model_test.py:141:        'AttentionMechanismScaledLuong/loss': 8.8519039,
nmt-master/nmt/model_test.py:142:        'GNMTModel_gnmt/loss': 8.8519087,
nmt-master/nmt/model_test.py:143:        'GNMTModel_gnmt_v2/loss': 8.8519087,
nmt-master/nmt/model_test.py:144:        'NoAttentionNoResidualUniEncoder/loss': 8.8516064,
nmt-master/nmt/model_test.py:145:        'NoAttentionResidualBiEncoder/loss': 8.851984,
nmt-master/nmt/model_test.py:146:        'UniEncoderStandardAttentionArchitecture/loss': 8.8519087,
nmt-master/nmt/model_test.py:147:        'InitializerGlorotNormal/loss': 8.9779415,
nmt-master/nmt/model_test.py:148:        'InitializerGlorotUniform/loss': 8.7643699,
nmt-master/nmt/model_test.py:149:        'SampledSoftmaxLoss/loss': 5.83928,
nmt-master/nmt/model_test.py:150:    }
nmt-master/nmt/model_test.py:152:    cls.actual_eval_values = {}
nmt-master/nmt/model_test.py:153:    cls.expected_eval_values = {
nmt-master/nmt/model_test.py:154:        'AttentionMechanismBahdanau/loss': 8.8517132,
nmt-master/nmt/model_test.py:155:        'AttentionMechanismBahdanau/predict_count': 11.0,
nmt-master/nmt/model_test.py:156:        'AttentionMechanismLuong/loss': 8.8517132,
nmt-master/nmt/model_test.py:157:        'AttentionMechanismLuong/predict_count': 11.0,
nmt-master/nmt/model_test.py:158:        'AttentionMechanismNormedBahdanau/loss': 8.8517132,
nmt-master/nmt/model_test.py:159:        'AttentionMechanismNormedBahdanau/predict_count': 11.0,
nmt-master/nmt/model_test.py:160:        'AttentionMechanismScaledLuong/loss': 8.8517132,
nmt-master/nmt/model_test.py:161:        'AttentionMechanismScaledLuong/predict_count': 11.0,
nmt-master/nmt/model_test.py:162:        'GNMTModel_gnmt/loss': 8.8443403,
nmt-master/nmt/model_test.py:163:        'GNMTModel_gnmt/predict_count': 11.0,
nmt-master/nmt/model_test.py:164:        'GNMTModel_gnmt_v2/loss': 8.8443756,
nmt-master/nmt/model_test.py:165:        'GNMTModel_gnmt_v2/predict_count': 11.0,
nmt-master/nmt/model_test.py:166:        'NoAttentionNoResidualUniEncoder/loss': 8.8440113,
nmt-master/nmt/model_test.py:167:        'NoAttentionNoResidualUniEncoder/predict_count': 11.0,
nmt-master/nmt/model_test.py:168:        'NoAttentionResidualBiEncoder/loss': 8.8291245,
nmt-master/nmt/model_test.py:169:        'NoAttentionResidualBiEncoder/predict_count': 11.0,
nmt-master/nmt/model_test.py:170:        'UniEncoderBottomAttentionArchitecture/loss': 8.844492,
nmt-master/nmt/model_test.py:171:        'UniEncoderBottomAttentionArchitecture/predict_count': 11.0,
nmt-master/nmt/model_test.py:172:        'UniEncoderStandardAttentionArchitecture/loss': 8.8517151,
nmt-master/nmt/model_test.py:173:        'UniEncoderStandardAttentionArchitecture/predict_count': 11.0
nmt-master/nmt/model_test.py:174:    }
nmt-master/nmt/model_test.py:176:    cls.actual_infer_values = {}
nmt-master/nmt/model_test.py:177:    cls.expected_infer_values = {
nmt-master/nmt/model_test.py:178:        'AttentionMechanismBahdanau/logits_sum': -0.026374687,
nmt-master/nmt/model_test.py:179:        'AttentionMechanismLuong/logits_sum': -0.026374735,
nmt-master/nmt/model_test.py:180:        'AttentionMechanismNormedBahdanau/logits_sum': -0.026376063,
nmt-master/nmt/model_test.py:181:        'AttentionMechanismScaledLuong/logits_sum': -0.026374735,
nmt-master/nmt/model_test.py:182:        'GNMTModel_gnmt/logits_sum': -1.10848486,
nmt-master/nmt/model_test.py:183:        'GNMTModel_gnmt_v2/logits_sum': -1.10950875,
nmt-master/nmt/model_test.py:184:        'NoAttentionNoResidualUniEncoder/logits_sum': -1.0808625,
nmt-master/nmt/model_test.py:185:        'NoAttentionResidualBiEncoder/logits_sum': -2.8147559,
nmt-master/nmt/model_test.py:186:        'UniEncoderBottomAttentionArchitecture/logits_sum': -0.97026241,
nmt-master/nmt/model_test.py:187:        'UniEncoderStandardAttentionArchitecture/logits_sum': -0.02665353
nmt-master/nmt/model_test.py:188:    }
nmt-master/nmt/model_test.py:190:    cls.actual_beam_sentences = {}
nmt-master/nmt/model_test.py:191:    cls.expected_beam_sentences = {
nmt-master/nmt/model_test.py:192:        'BeamSearchAttentionModel: batch 0 of beam 0': '',
nmt-master/nmt/model_test.py:193:        'BeamSearchAttentionModel: batch 0 of beam 1': '%s a %s a' % (SOS, SOS),
nmt-master/nmt/model_test.py:194:        'BeamSearchAttentionModel: batch 1 of beam 0': '',
nmt-master/nmt/model_test.py:195:        'BeamSearchAttentionModel: batch 1 of beam 1': 'b',
nmt-master/nmt/model_test.py:196:        'BeamSearchBasicModel: batch 0 of beam 0': 'b b b b',
nmt-master/nmt/model_test.py:197:        'BeamSearchBasicModel: batch 0 of beam 1': 'b b b %s' % SOS,
nmt-master/nmt/model_test.py:198:        'BeamSearchBasicModel: batch 0 of beam 2': 'b b b c',
nmt-master/nmt/model_test.py:199:        'BeamSearchBasicModel: batch 1 of beam 0': 'b b b b',
nmt-master/nmt/model_test.py:200:        'BeamSearchBasicModel: batch 1 of beam 1': 'a b b b',
nmt-master/nmt/model_test.py:201:        'BeamSearchBasicModel: batch 1 of beam 2': 'b b b %s' % SOS,
nmt-master/nmt/model_test.py:202:        'BeamSearchGNMTModel: batch 0 of beam 0': '',
nmt-master/nmt/model_test.py:203:        'BeamSearchGNMTModel: batch 1 of beam 0': '',
nmt-master/nmt/model_test.py:204:    }
nmt-master/nmt/model_test.py:205:    cls.expected_beam_sentences = dict(
nmt-master/nmt/model_test.py:206:        (k, v.encode()) for k, v in cls.expected_beam_sentences.items())
nmt-master/nmt/model_test.py:208:  @classmethod
nmt-master/nmt/model_test.py:209:  def tearDownClass(cls):
nmt-master/nmt/model_test.py:210:    print('ModelTest - actual_vars_values: ')
nmt-master/nmt/model_test.py:211:    pprint.pprint(cls.actual_vars_values)
nmt-master/nmt/model_test.py:212:    sys.stdout.flush()
nmt-master/nmt/model_test.py:214:    print('ModelTest - actual_train_values: ')
nmt-master/nmt/model_test.py:215:    pprint.pprint(cls.actual_train_values)
nmt-master/nmt/model_test.py:216:    sys.stdout.flush()
nmt-master/nmt/model_test.py:218:    print('ModelTest - actual_eval_values: ')
nmt-master/nmt/model_test.py:219:    pprint.pprint(cls.actual_eval_values)
nmt-master/nmt/model_test.py:220:    sys.stdout.flush()
nmt-master/nmt/model_test.py:222:    print('ModelTest - actual_infer_values: ')
nmt-master/nmt/model_test.py:223:    pprint.pprint(cls.actual_infer_values)
nmt-master/nmt/model_test.py:224:    sys.stdout.flush()
nmt-master/nmt/model_test.py:226:    print('ModelTest - actual_beam_sentences: ')
nmt-master/nmt/model_test.py:227:    pprint.pprint(cls.actual_beam_sentences)
nmt-master/nmt/model_test.py:228:    sys.stdout.flush()
nmt-master/nmt/model_test.py:230:  def assertAllClose(self, *args, **kwargs):
nmt-master/nmt/model_test.py:231:    kwargs['atol'] = 5e-2
nmt-master/nmt/model_test.py:232:    kwargs['rtol'] = 5e-2
nmt-master/nmt/model_test.py:233:    return super(ModelTest, self).assertAllClose(*args, **kwargs)
nmt-master/nmt/model_test.py:235:  def _assertModelVariableNames(self, expected_var_names, model_var_names,
nmt-master/nmt/model_test.py:236:                                name):
nmt-master/nmt/model_test.py:238:    print('{} variable names are: '.format(name), model_var_names)
nmt-master/nmt/model_test.py:240:    self.assertEqual(len(expected_var_names), len(model_var_names))
nmt-master/nmt/model_test.py:241:    self.assertEqual(sorted(expected_var_names), sorted(model_var_names))
nmt-master/nmt/model_test.py:243:  def _assertModelVariable(self, variable, sess, name):
nmt-master/nmt/model_test.py:244:    var_shape = tuple(variable.get_shape().as_list())
nmt-master/nmt/model_test.py:245:    var_res = sess.run(variable)
nmt-master/nmt/model_test.py:246:    var_weight_sum = np.sum(var_res)
nmt-master/nmt/model_test.py:248:    print('{} weight sum is: '.format(name), var_weight_sum)
nmt-master/nmt/model_test.py:249:    expected_sum = self.expected_vars_values[name + '/sum']
nmt-master/nmt/model_test.py:250:    expected_shape = self.expected_vars_values[name + '/shape']
nmt-master/nmt/model_test.py:251:    self.actual_vars_values[name + '/sum'] = var_weight_sum
nmt-master/nmt/model_test.py:252:    self.actual_vars_values[name + '/shape'] = var_shape
nmt-master/nmt/model_test.py:254:    self.assertEqual(expected_shape, var_shape)
nmt-master/nmt/model_test.py:255:    self.assertAllClose(expected_sum, var_weight_sum)
nmt-master/nmt/model_test.py:257:  def _assertTrainStepsLoss(self, m, sess, name, num_steps=1):
nmt-master/nmt/model_test.py:258:    for _ in range(num_steps):
nmt-master/nmt/model_test.py:259:      _, output_tuple = m.train(sess)
nmt-master/nmt/model_test.py:260:    loss = output_tuple.train_loss
nmt-master/nmt/model_test.py:261:    print('{} {}-th step loss is: '.format(name, num_steps), loss)
nmt-master/nmt/model_test.py:262:    expected_loss = self.expected_train_values[name + '/loss']
nmt-master/nmt/model_test.py:263:    self.actual_train_values[name + '/loss'] = loss
nmt-master/nmt/model_test.py:265:    self.assertAllClose(expected_loss, loss)
nmt-master/nmt/model_test.py:267:  def _assertEvalLossAndPredictCount(self, m, sess, name):
nmt-master/nmt/model_test.py:268:    output_tuple = m.eval(sess)
nmt-master/nmt/model_test.py:269:    loss = output_tuple.eval_loss
nmt-master/nmt/model_test.py:270:    predict_count = output_tuple.predict_count
nmt-master/nmt/model_test.py:271:    print('{} eval loss is: '.format(name), loss)
nmt-master/nmt/model_test.py:272:    print('{} predict count is: '.format(name), predict_count)
nmt-master/nmt/model_test.py:273:    expected_loss = self.expected_eval_values[name + '/loss']
nmt-master/nmt/model_test.py:274:    expected_predict_count = self.expected_eval_values[name + '/predict_count']
nmt-master/nmt/model_test.py:275:    self.actual_eval_values[name + '/loss'] = loss
nmt-master/nmt/model_test.py:276:    self.actual_eval_values[name + '/predict_count'] = predict_count
nmt-master/nmt/model_test.py:278:    self.assertAllClose(expected_loss, loss)
nmt-master/nmt/model_test.py:279:    self.assertAllClose(expected_predict_count, predict_count)
nmt-master/nmt/model_test.py:281:  def _assertInferLogits(self, m, sess, name):
nmt-master/nmt/model_test.py:282:    output_tuple = m.infer(sess)
nmt-master/nmt/model_test.py:283:    logits_sum = np.sum(output_tuple.infer_logits)
nmt-master/nmt/model_test.py:285:    print('{} infer logits sum is: '.format(name), logits_sum)
nmt-master/nmt/model_test.py:286:    expected_logits_sum = self.expected_infer_values[name + '/logits_sum']
nmt-master/nmt/model_test.py:287:    self.actual_infer_values[name + '/logits_sum'] = logits_sum
nmt-master/nmt/model_test.py:289:    self.assertAllClose(expected_logits_sum, logits_sum)
nmt-master/nmt/model_test.py:291:  def _assertBeamSearchOutputs(self, m, sess, assert_top_k_sentence, name):
nmt-master/nmt/model_test.py:292:    nmt_outputs, _ = m.decode(sess)
nmt-master/nmt/model_test.py:294:    for i in range(assert_top_k_sentence):
nmt-master/nmt/model_test.py:295:      output_words = nmt_outputs[i]
nmt-master/nmt/model_test.py:296:      for j in range(output_words.shape[0]):
nmt-master/nmt/model_test.py:297:        sentence = nmt_utils.get_translation(
nmt-master/nmt/model_test.py:298:            output_words, j, tgt_eos=EOS, subword_option='')
nmt-master/nmt/model_test.py:299:        sentence_key = ('%s: batch %d of beam %d' % (name, j, i))
nmt-master/nmt/model_test.py:300:        self.actual_beam_sentences[sentence_key] = sentence
nmt-master/nmt/model_test.py:301:        expected_sentence = self.expected_beam_sentences[sentence_key]
nmt-master/nmt/model_test.py:302:        self.assertEqual(expected_sentence, sentence)
nmt-master/nmt/model_test.py:304:  def _createTestTrainModel(self, m_creator, hparams, sess):
nmt-master/nmt/model_test.py:305:    train_mode = tf.contrib.learn.ModeKeys.TRAIN
nmt-master/nmt/model_test.py:306:    train_iterator, src_vocab_table, tgt_vocab_table = (
nmt-master/nmt/model_test.py:307:        common_test_utils.create_test_iterator(hparams, train_mode))
nmt-master/nmt/model_test.py:308:    train_m = m_creator(
nmt-master/nmt/model_test.py:309:        hparams,
nmt-master/nmt/model_test.py:310:        train_mode,
nmt-master/nmt/model_test.py:311:        train_iterator,
nmt-master/nmt/model_test.py:312:        src_vocab_table,
nmt-master/nmt/model_test.py:313:        tgt_vocab_table,
nmt-master/nmt/model_test.py:314:        scope='dynamic_seq2seq')
nmt-master/nmt/model_test.py:315:    sess.run(tf.global_variables_initializer())
nmt-master/nmt/model_test.py:316:    sess.run(tf.tables_initializer())
nmt-master/nmt/model_test.py:317:    sess.run(train_iterator.initializer)
nmt-master/nmt/model_test.py:318:    return train_m
nmt-master/nmt/model_test.py:320:  def _createTestEvalModel(self, m_creator, hparams, sess):
nmt-master/nmt/model_test.py:321:    eval_mode = tf.contrib.learn.ModeKeys.EVAL
nmt-master/nmt/model_test.py:322:    eval_iterator, src_vocab_table, tgt_vocab_table = (
nmt-master/nmt/model_test.py:323:        common_test_utils.create_test_iterator(hparams, eval_mode))
nmt-master/nmt/model_test.py:324:    eval_m = m_creator(
nmt-master/nmt/model_test.py:325:        hparams,
nmt-master/nmt/model_test.py:326:        eval_mode,
nmt-master/nmt/model_test.py:327:        eval_iterator,
nmt-master/nmt/model_test.py:328:        src_vocab_table,
nmt-master/nmt/model_test.py:329:        tgt_vocab_table,
nmt-master/nmt/model_test.py:330:        scope='dynamic_seq2seq')
nmt-master/nmt/model_test.py:331:    sess.run(tf.tables_initializer())
nmt-master/nmt/model_test.py:332:    sess.run(eval_iterator.initializer)
nmt-master/nmt/model_test.py:333:    return eval_m
nmt-master/nmt/model_test.py:335:  def _createTestInferModel(
nmt-master/nmt/model_test.py:336:      self, m_creator, hparams, sess, init_global_vars=False):
nmt-master/nmt/model_test.py:337:    infer_mode = tf.contrib.learn.ModeKeys.INFER
nmt-master/nmt/model_test.py:338:    (infer_iterator, src_vocab_table,
nmt-master/nmt/model_test.py:339:     tgt_vocab_table, reverse_tgt_vocab_table) = (
nmt-master/nmt/model_test.py:340:         common_test_utils.create_test_iterator(hparams, infer_mode))
nmt-master/nmt/model_test.py:341:    infer_m = m_creator(
nmt-master/nmt/model_test.py:342:        hparams,
nmt-master/nmt/model_test.py:343:        infer_mode,
nmt-master/nmt/model_test.py:344:        infer_iterator,
nmt-master/nmt/model_test.py:345:        src_vocab_table,
nmt-master/nmt/model_test.py:346:        tgt_vocab_table,
nmt-master/nmt/model_test.py:347:        reverse_tgt_vocab_table,
nmt-master/nmt/model_test.py:348:        scope='dynamic_seq2seq')
nmt-master/nmt/model_test.py:349:    if init_global_vars:
nmt-master/nmt/model_test.py:350:      sess.run(tf.global_variables_initializer())
nmt-master/nmt/model_test.py:351:    sess.run(tf.tables_initializer())
nmt-master/nmt/model_test.py:352:    sess.run(infer_iterator.initializer)
nmt-master/nmt/model_test.py:353:    return infer_m
nmt-master/nmt/model_test.py:355:  def _get_session_config(self):
nmt-master/nmt/model_test.py:356:    config = tf.ConfigProto()
nmt-master/nmt/model_test.py:357:    config.allow_soft_placement = True
nmt-master/nmt/model_test.py:358:    return config
nmt-master/nmt/model_test.py:360:  ## Testing 3 encoders:
nmt-master/nmt/model_test.py:361:  # uni: no attention, no residual, 1 layers
nmt-master/nmt/model_test.py:362:  # bi: no attention, with residual, 4 layers
nmt-master/nmt/model_test.py:363:  def testNoAttentionNoResidualUniEncoder(self):
nmt-master/nmt/model_test.py:364:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:365:        encoder_type='uni',
nmt-master/nmt/model_test.py:366:        num_layers=1,
nmt-master/nmt/model_test.py:367:        attention='',
nmt-master/nmt/model_test.py:368:        attention_architecture='',
nmt-master/nmt/model_test.py:369:        use_residual=False,)
nmt-master/nmt/model_test.py:371:    workers, _ = tf.test.create_local_cluster(1, 0)
nmt-master/nmt/model_test.py:372:    worker = workers[0]
nmt-master/nmt/model_test.py:374:    # pylint: disable=line-too-long
nmt-master/nmt/model_test.py:375:    expected_var_names = [
nmt-master/nmt/model_test.py:376:        'dynamic_seq2seq/encoder/embedding_encoder:0',
nmt-master/nmt/model_test.py:377:        'dynamic_seq2seq/decoder/embedding_decoder:0',
nmt-master/nmt/model_test.py:378:        'dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:379:        'dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:380:        'dynamic_seq2seq/decoder/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:381:        'dynamic_seq2seq/decoder/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:382:        'dynamic_seq2seq/decoder/output_projection/kernel:0'
nmt-master/nmt/model_test.py:383:    ]
nmt-master/nmt/model_test.py:384:    # pylint: enable=line-too-long
nmt-master/nmt/model_test.py:386:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:387:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:388:        train_m = self._createTestTrainModel(model.Model, hparams, sess)
nmt-master/nmt/model_test.py:390:        m_vars = tf.trainable_variables()
nmt-master/nmt/model_test.py:391:        self._assertModelVariableNames(expected_var_names,
nmt-master/nmt/model_test.py:392:                                       [v.name for v in m_vars],
nmt-master/nmt/model_test.py:393:                                       'NoAttentionNoResidualUniEncoder')
nmt-master/nmt/model_test.py:395:        with tf.variable_scope('dynamic_seq2seq', reuse=True):
nmt-master/nmt/model_test.py:396:          last_enc_weight = tf.get_variable(
nmt-master/nmt/model_test.py:397:              'encoder/rnn/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:398:          last_dec_weight = tf.get_variable('decoder/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:399:        self._assertTrainStepsLoss(train_m, sess,
nmt-master/nmt/model_test.py:400:                                   'NoAttentionNoResidualUniEncoder')
nmt-master/nmt/model_test.py:401:        self._assertModelVariable(
nmt-master/nmt/model_test.py:402:            last_enc_weight, sess,
nmt-master/nmt/model_test.py:403:            'NoAttentionNoResidualUniEncoder/last_enc_weight')
nmt-master/nmt/model_test.py:404:        self._assertModelVariable(
nmt-master/nmt/model_test.py:405:            last_dec_weight, sess,
nmt-master/nmt/model_test.py:406:            'NoAttentionNoResidualUniEncoder/last_dec_weight')
nmt-master/nmt/model_test.py:408:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:409:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:410:        eval_m = self._createTestEvalModel(model.Model, hparams, sess)
nmt-master/nmt/model_test.py:411:        self._assertEvalLossAndPredictCount(eval_m, sess,
nmt-master/nmt/model_test.py:412:                                            'NoAttentionNoResidualUniEncoder')
nmt-master/nmt/model_test.py:414:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:415:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:416:        infer_m = self._createTestInferModel(model.Model, hparams, sess)
nmt-master/nmt/model_test.py:417:        self._assertInferLogits(infer_m, sess,
nmt-master/nmt/model_test.py:418:                                'NoAttentionNoResidualUniEncoder')
nmt-master/nmt/model_test.py:420:  def testNoAttentionResidualBiEncoder(self):
nmt-master/nmt/model_test.py:421:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:422:        encoder_type='bi',
nmt-master/nmt/model_test.py:423:        num_layers=4,
nmt-master/nmt/model_test.py:424:        attention='',
nmt-master/nmt/model_test.py:425:        attention_architecture='',
nmt-master/nmt/model_test.py:426:        use_residual=True,)
nmt-master/nmt/model_test.py:428:    workers, _ = tf.test.create_local_cluster(1, 0)
nmt-master/nmt/model_test.py:429:    worker = workers[0]
nmt-master/nmt/model_test.py:431:    # pylint: disable=line-too-long
nmt-master/nmt/model_test.py:432:    expected_var_names = [
nmt-master/nmt/model_test.py:433:        'dynamic_seq2seq/encoder/embedding_encoder:0',
nmt-master/nmt/model_test.py:434:        'dynamic_seq2seq/decoder/embedding_decoder:0',
nmt-master/nmt/model_test.py:435:        'dynamic_seq2seq/encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:436:        'dynamic_seq2seq/encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:437:        'dynamic_seq2seq/encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:438:        'dynamic_seq2seq/encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:439:        'dynamic_seq2seq/encoder/bidirectional_rnn/bw/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:440:        'dynamic_seq2seq/encoder/bidirectional_rnn/bw/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:441:        'dynamic_seq2seq/encoder/bidirectional_rnn/bw/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:442:        'dynamic_seq2seq/encoder/bidirectional_rnn/bw/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:443:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:444:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:445:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:446:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:447:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:448:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:449:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:450:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:451:        'dynamic_seq2seq/decoder/output_projection/kernel:0'
nmt-master/nmt/model_test.py:452:    ]
nmt-master/nmt/model_test.py:453:    # pylint: enable=line-too-long
nmt-master/nmt/model_test.py:455:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:456:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:457:        train_m = self._createTestTrainModel(model.Model, hparams, sess)
nmt-master/nmt/model_test.py:459:        m_vars = tf.trainable_variables()
nmt-master/nmt/model_test.py:460:        self._assertModelVariableNames(expected_var_names,
nmt-master/nmt/model_test.py:461:                                       [v.name for v in m_vars],
nmt-master/nmt/model_test.py:462:                                       'NoAttentionResidualBiEncoder')
nmt-master/nmt/model_test.py:463:        with tf.variable_scope('dynamic_seq2seq', reuse=True):
nmt-master/nmt/model_test.py:464:          last_enc_weight = tf.get_variable(
nmt-master/nmt/model_test.py:465:              'encoder/bidirectional_rnn/bw/multi_rnn_cell/cell_1/basic_lstm_cell/kernel'
nmt-master/nmt/model_test.py:466:          )
nmt-master/nmt/model_test.py:467:          last_dec_weight = tf.get_variable(
nmt-master/nmt/model_test.py:468:              'decoder/multi_rnn_cell/cell_3/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:469:        self._assertTrainStepsLoss(train_m, sess,
nmt-master/nmt/model_test.py:470:                                   'NoAttentionResidualBiEncoder')
nmt-master/nmt/model_test.py:471:        self._assertModelVariable(
nmt-master/nmt/model_test.py:472:            last_enc_weight, sess,
nmt-master/nmt/model_test.py:473:            'NoAttentionResidualBiEncoder/last_enc_weight')
nmt-master/nmt/model_test.py:474:        self._assertModelVariable(
nmt-master/nmt/model_test.py:475:            last_dec_weight, sess,
nmt-master/nmt/model_test.py:476:            'NoAttentionResidualBiEncoder/last_dec_weight')
nmt-master/nmt/model_test.py:478:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:479:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:480:        eval_m = self._createTestEvalModel(model.Model, hparams, sess)
nmt-master/nmt/model_test.py:481:        self._assertEvalLossAndPredictCount(eval_m, sess,
nmt-master/nmt/model_test.py:482:                                            'NoAttentionResidualBiEncoder')
nmt-master/nmt/model_test.py:484:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:485:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:486:        infer_m = self._createTestInferModel(model.Model, hparams, sess)
nmt-master/nmt/model_test.py:487:        self._assertInferLogits(infer_m, sess, 'NoAttentionResidualBiEncoder')
nmt-master/nmt/model_test.py:489:  ## Test attention mechanisms: luong, scaled_luong, bahdanau, normed_bahdanau
nmt-master/nmt/model_test.py:490:  def testAttentionMechanismLuong(self):
nmt-master/nmt/model_test.py:491:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:492:        encoder_type='uni',
nmt-master/nmt/model_test.py:493:        attention='luong',
nmt-master/nmt/model_test.py:494:        attention_architecture='standard',
nmt-master/nmt/model_test.py:495:        num_layers=2,
nmt-master/nmt/model_test.py:496:        use_residual=False,)
nmt-master/nmt/model_test.py:498:    workers, _ = tf.test.create_local_cluster(1, 0)
nmt-master/nmt/model_test.py:499:    worker = workers[0]
nmt-master/nmt/model_test.py:501:    # pylint: disable=line-too-long
nmt-master/nmt/model_test.py:502:    expected_var_names = [
nmt-master/nmt/model_test.py:503:        'dynamic_seq2seq/encoder/embedding_encoder:0',
nmt-master/nmt/model_test.py:504:        'dynamic_seq2seq/decoder/embedding_decoder:0',
nmt-master/nmt/model_test.py:505:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:506:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:507:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:508:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:509:        'dynamic_seq2seq/decoder/memory_layer/kernel:0',
nmt-master/nmt/model_test.py:510:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:511:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:512:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:513:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:514:        'dynamic_seq2seq/decoder/attention/attention_layer/kernel:0',
nmt-master/nmt/model_test.py:515:        'dynamic_seq2seq/decoder/output_projection/kernel:0'
nmt-master/nmt/model_test.py:516:    ]
nmt-master/nmt/model_test.py:517:    # pylint: enable=line-too-long
nmt-master/nmt/model_test.py:518:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:519:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:520:        train_m = self._createTestTrainModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:521:                                             hparams, sess)
nmt-master/nmt/model_test.py:523:        m_vars = tf.trainable_variables()
nmt-master/nmt/model_test.py:524:        self._assertModelVariableNames(
nmt-master/nmt/model_test.py:525:            expected_var_names, [v.name
nmt-master/nmt/model_test.py:526:                                 for v in m_vars], 'AttentionMechanismLuong')
nmt-master/nmt/model_test.py:528:        with tf.variable_scope('dynamic_seq2seq', reuse=True):
nmt-master/nmt/model_test.py:529:          # pylint: disable=line-too-long
nmt-master/nmt/model_test.py:530:          last_enc_weight = tf.get_variable(
nmt-master/nmt/model_test.py:531:              'encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:532:          last_dec_weight = tf.get_variable(
nmt-master/nmt/model_test.py:533:              'decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:534:          att_layer_weight = tf.get_variable(
nmt-master/nmt/model_test.py:535:              'decoder/attention/attention_layer/kernel')
nmt-master/nmt/model_test.py:536:          # pylint: enable=line-too-long
nmt-master/nmt/model_test.py:537:        self._assertTrainStepsLoss(train_m, sess, 'AttentionMechanismLuong')
nmt-master/nmt/model_test.py:538:        self._assertModelVariable(last_enc_weight, sess,
nmt-master/nmt/model_test.py:539:                                  'AttentionMechanismLuong/last_enc_weight')
nmt-master/nmt/model_test.py:540:        self._assertModelVariable(last_dec_weight, sess,
nmt-master/nmt/model_test.py:541:                                  'AttentionMechanismLuong/last_dec_weight')
nmt-master/nmt/model_test.py:542:        self._assertModelVariable(att_layer_weight, sess,
nmt-master/nmt/model_test.py:543:                                  'AttentionMechanismLuong/att_layer_weight')
nmt-master/nmt/model_test.py:545:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:546:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:547:        eval_m = self._createTestEvalModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:548:                                           hparams, sess)
nmt-master/nmt/model_test.py:549:        self._assertEvalLossAndPredictCount(eval_m, sess,
nmt-master/nmt/model_test.py:550:                                            'AttentionMechanismLuong')
nmt-master/nmt/model_test.py:552:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:553:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:554:        infer_m = self._createTestInferModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:555:                                             hparams, sess)
nmt-master/nmt/model_test.py:556:        self._assertInferLogits(infer_m, sess, 'AttentionMechanismLuong')
nmt-master/nmt/model_test.py:558:  def testAttentionMechanismScaledLuong(self):
nmt-master/nmt/model_test.py:559:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:560:        encoder_type='uni',
nmt-master/nmt/model_test.py:561:        attention='scaled_luong',
nmt-master/nmt/model_test.py:562:        attention_architecture='standard',
nmt-master/nmt/model_test.py:563:        num_layers=2,
nmt-master/nmt/model_test.py:564:        use_residual=False,)
nmt-master/nmt/model_test.py:566:    workers, _ = tf.test.create_local_cluster(1, 0)
nmt-master/nmt/model_test.py:567:    worker = workers[0]
nmt-master/nmt/model_test.py:569:    # pylint: disable=line-too-long
nmt-master/nmt/model_test.py:570:    expected_var_names = [
nmt-master/nmt/model_test.py:571:        'dynamic_seq2seq/encoder/embedding_encoder:0',
nmt-master/nmt/model_test.py:572:        'dynamic_seq2seq/decoder/embedding_decoder:0',
nmt-master/nmt/model_test.py:573:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:574:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:575:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:576:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:577:        'dynamic_seq2seq/decoder/memory_layer/kernel:0',
nmt-master/nmt/model_test.py:578:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:579:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:580:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:581:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:582:        'dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0',
nmt-master/nmt/model_test.py:583:        'dynamic_seq2seq/decoder/attention/attention_layer/kernel:0',
nmt-master/nmt/model_test.py:584:        'dynamic_seq2seq/decoder/output_projection/kernel:0'
nmt-master/nmt/model_test.py:585:    ]
nmt-master/nmt/model_test.py:586:    # pylint: enable=line-too-long
nmt-master/nmt/model_test.py:587:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:588:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:589:        train_m = self._createTestTrainModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:590:                                             hparams, sess)
nmt-master/nmt/model_test.py:592:        m_vars = tf.trainable_variables()
nmt-master/nmt/model_test.py:593:        self._assertModelVariableNames(expected_var_names,
nmt-master/nmt/model_test.py:594:                                       [v.name for v in m_vars],
nmt-master/nmt/model_test.py:595:                                       'AttentionMechanismScaledLuong')
nmt-master/nmt/model_test.py:597:        with tf.variable_scope('dynamic_seq2seq', reuse=True):
nmt-master/nmt/model_test.py:598:          # pylint: disable=line-too-long
nmt-master/nmt/model_test.py:599:          last_enc_weight = tf.get_variable(
nmt-master/nmt/model_test.py:600:              'encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:601:          last_dec_weight = tf.get_variable(
nmt-master/nmt/model_test.py:602:              'decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:603:          att_layer_weight = tf.get_variable(
nmt-master/nmt/model_test.py:604:              'decoder/attention/attention_layer/kernel')
nmt-master/nmt/model_test.py:605:          # pylint: enable=line-too-long
nmt-master/nmt/model_test.py:607:        self._assertTrainStepsLoss(train_m, sess,
nmt-master/nmt/model_test.py:608:                                   'AttentionMechanismScaledLuong')
nmt-master/nmt/model_test.py:609:        self._assertModelVariable(
nmt-master/nmt/model_test.py:610:            last_enc_weight, sess,
nmt-master/nmt/model_test.py:611:            'AttentionMechanismScaledLuong/last_enc_weight')
nmt-master/nmt/model_test.py:612:        self._assertModelVariable(
nmt-master/nmt/model_test.py:613:            last_dec_weight, sess,
nmt-master/nmt/model_test.py:614:            'AttentionMechanismScaledLuong/last_dec_weight')
nmt-master/nmt/model_test.py:615:        self._assertModelVariable(
nmt-master/nmt/model_test.py:616:            att_layer_weight, sess,
nmt-master/nmt/model_test.py:617:            'AttentionMechanismScaledLuong/att_layer_weight')
nmt-master/nmt/model_test.py:619:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:620:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:621:        eval_m = self._createTestEvalModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:622:                                           hparams, sess)
nmt-master/nmt/model_test.py:623:        self._assertEvalLossAndPredictCount(eval_m, sess,
nmt-master/nmt/model_test.py:624:                                            'AttentionMechanismScaledLuong')
nmt-master/nmt/model_test.py:626:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:627:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:628:        infer_m = self._createTestInferModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:629:                                             hparams, sess)
nmt-master/nmt/model_test.py:630:        self._assertInferLogits(infer_m, sess, 'AttentionMechanismScaledLuong')
nmt-master/nmt/model_test.py:632:  def testAttentionMechanismBahdanau(self):
nmt-master/nmt/model_test.py:633:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:634:        encoder_type='uni',
nmt-master/nmt/model_test.py:635:        attention='bahdanau',
nmt-master/nmt/model_test.py:636:        attention_architecture='standard',
nmt-master/nmt/model_test.py:637:        num_layers=2,
nmt-master/nmt/model_test.py:638:        use_residual=False,)
nmt-master/nmt/model_test.py:640:    workers, _ = tf.test.create_local_cluster(1, 0)
nmt-master/nmt/model_test.py:641:    worker = workers[0]
nmt-master/nmt/model_test.py:643:    # pylint: disable=line-too-long
nmt-master/nmt/model_test.py:644:    expected_var_names = [
nmt-master/nmt/model_test.py:645:        'dynamic_seq2seq/encoder/embedding_encoder:0',
nmt-master/nmt/model_test.py:646:        'dynamic_seq2seq/decoder/embedding_decoder:0',
nmt-master/nmt/model_test.py:647:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:648:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:649:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:650:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:651:        'dynamic_seq2seq/decoder/memory_layer/kernel:0',
nmt-master/nmt/model_test.py:652:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:653:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:654:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:655:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:656:        'dynamic_seq2seq/decoder/attention/bahdanau_attention/query_layer/kernel:0',
nmt-master/nmt/model_test.py:657:        'dynamic_seq2seq/decoder/attention/bahdanau_attention/attention_v:0',
nmt-master/nmt/model_test.py:658:        'dynamic_seq2seq/decoder/attention/attention_layer/kernel:0',
nmt-master/nmt/model_test.py:659:        'dynamic_seq2seq/decoder/output_projection/kernel:0'
nmt-master/nmt/model_test.py:660:    ]
nmt-master/nmt/model_test.py:661:    # pylint: enable=line-too-long
nmt-master/nmt/model_test.py:662:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:663:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:664:        train_m = self._createTestTrainModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:665:                                             hparams, sess)
nmt-master/nmt/model_test.py:667:        m_vars = tf.trainable_variables()
nmt-master/nmt/model_test.py:668:        self._assertModelVariableNames(
nmt-master/nmt/model_test.py:669:            expected_var_names, [v.name
nmt-master/nmt/model_test.py:670:                                 for v in m_vars], 'AttentionMechanismBahdanau')
nmt-master/nmt/model_test.py:672:        with tf.variable_scope('dynamic_seq2seq', reuse=True):
nmt-master/nmt/model_test.py:673:          # pylint: disable=line-too-long
nmt-master/nmt/model_test.py:674:          last_enc_weight = tf.get_variable(
nmt-master/nmt/model_test.py:675:              'encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:676:          last_dec_weight = tf.get_variable(
nmt-master/nmt/model_test.py:677:              'decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:678:          att_layer_weight = tf.get_variable(
nmt-master/nmt/model_test.py:679:              'decoder/attention/attention_layer/kernel')
nmt-master/nmt/model_test.py:680:          # pylint: enable=line-too-long
nmt-master/nmt/model_test.py:681:        self._assertTrainStepsLoss(train_m, sess, 'AttentionMechanismBahdanau')
nmt-master/nmt/model_test.py:682:        self._assertModelVariable(last_enc_weight, sess,
nmt-master/nmt/model_test.py:683:                                  'AttentionMechanismBahdanau/last_enc_weight')
nmt-master/nmt/model_test.py:684:        self._assertModelVariable(last_dec_weight, sess,
nmt-master/nmt/model_test.py:685:                                  'AttentionMechanismBahdanau/last_dec_weight')
nmt-master/nmt/model_test.py:686:        self._assertModelVariable(att_layer_weight, sess,
nmt-master/nmt/model_test.py:687:                                  'AttentionMechanismBahdanau/att_layer_weight')
nmt-master/nmt/model_test.py:689:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:690:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:691:        eval_m = self._createTestEvalModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:692:                                           hparams, sess)
nmt-master/nmt/model_test.py:693:        self._assertEvalLossAndPredictCount(eval_m, sess,
nmt-master/nmt/model_test.py:694:                                            'AttentionMechanismBahdanau')
nmt-master/nmt/model_test.py:696:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:697:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:698:        infer_m = self._createTestInferModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:699:                                             hparams, sess)
nmt-master/nmt/model_test.py:700:        self._assertInferLogits(infer_m, sess, 'AttentionMechanismBahdanau')
nmt-master/nmt/model_test.py:702:  def testAttentionMechanismNormedBahdanau(self):
nmt-master/nmt/model_test.py:703:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:704:        encoder_type='uni',
nmt-master/nmt/model_test.py:705:        attention='normed_bahdanau',
nmt-master/nmt/model_test.py:706:        attention_architecture='standard',
nmt-master/nmt/model_test.py:707:        num_layers=2,
nmt-master/nmt/model_test.py:708:        use_residual=False,)
nmt-master/nmt/model_test.py:710:    workers, _ = tf.test.create_local_cluster(1, 0)
nmt-master/nmt/model_test.py:711:    worker = workers[0]
nmt-master/nmt/model_test.py:713:    # pylint: disable=line-too-long
nmt-master/nmt/model_test.py:714:    expected_var_names = [
nmt-master/nmt/model_test.py:715:        'dynamic_seq2seq/encoder/embedding_encoder:0',
nmt-master/nmt/model_test.py:716:        'dynamic_seq2seq/decoder/embedding_decoder:0',
nmt-master/nmt/model_test.py:717:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:718:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:719:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:720:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:721:        'dynamic_seq2seq/decoder/memory_layer/kernel:0',
nmt-master/nmt/model_test.py:722:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:723:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:724:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:725:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:726:        'dynamic_seq2seq/decoder/attention/bahdanau_attention/query_layer/kernel:0',
nmt-master/nmt/model_test.py:727:        'dynamic_seq2seq/decoder/attention/bahdanau_attention/attention_v:0',
nmt-master/nmt/model_test.py:728:        'dynamic_seq2seq/decoder/attention/bahdanau_attention/attention_g:0',
nmt-master/nmt/model_test.py:729:        'dynamic_seq2seq/decoder/attention/bahdanau_attention/attention_b:0',
nmt-master/nmt/model_test.py:730:        'dynamic_seq2seq/decoder/attention/attention_layer/kernel:0',
nmt-master/nmt/model_test.py:731:        'dynamic_seq2seq/decoder/output_projection/kernel:0'
nmt-master/nmt/model_test.py:732:    ]
nmt-master/nmt/model_test.py:733:    # pylint: enable=line-too-long
nmt-master/nmt/model_test.py:735:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:736:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:737:        train_m = self._createTestTrainModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:738:                                             hparams, sess)
nmt-master/nmt/model_test.py:740:        m_vars = tf.trainable_variables()
nmt-master/nmt/model_test.py:741:        self._assertModelVariableNames(expected_var_names,
nmt-master/nmt/model_test.py:742:                                       [v.name for v in m_vars],
nmt-master/nmt/model_test.py:743:                                       'AttentionMechanismNormedBahdanau')
nmt-master/nmt/model_test.py:745:        with tf.variable_scope('dynamic_seq2seq', reuse=True):
nmt-master/nmt/model_test.py:746:          # pylint: disable=line-too-long
nmt-master/nmt/model_test.py:747:          last_enc_weight = tf.get_variable(
nmt-master/nmt/model_test.py:748:              'encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:749:          last_dec_weight = tf.get_variable(
nmt-master/nmt/model_test.py:750:              'decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:751:          att_layer_weight = tf.get_variable(
nmt-master/nmt/model_test.py:752:              'decoder/attention/attention_layer/kernel')
nmt-master/nmt/model_test.py:753:          # pylint: enable=line-too-long
nmt-master/nmt/model_test.py:754:        self._assertTrainStepsLoss(train_m, sess,
nmt-master/nmt/model_test.py:755:                                   'AttentionMechanismNormedBahdanau')
nmt-master/nmt/model_test.py:756:        self._assertModelVariable(
nmt-master/nmt/model_test.py:757:            last_enc_weight, sess,
nmt-master/nmt/model_test.py:758:            'AttentionMechanismNormedBahdanau/last_enc_weight')
nmt-master/nmt/model_test.py:759:        self._assertModelVariable(
nmt-master/nmt/model_test.py:760:            last_dec_weight, sess,
nmt-master/nmt/model_test.py:761:            'AttentionMechanismNormedBahdanau/last_dec_weight')
nmt-master/nmt/model_test.py:762:        self._assertModelVariable(
nmt-master/nmt/model_test.py:763:            att_layer_weight, sess,
nmt-master/nmt/model_test.py:764:            'AttentionMechanismNormedBahdanau/att_layer_weight')
nmt-master/nmt/model_test.py:766:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:767:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:768:        eval_m = self._createTestEvalModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:769:                                           hparams, sess)
nmt-master/nmt/model_test.py:770:        self._assertEvalLossAndPredictCount(eval_m, sess,
nmt-master/nmt/model_test.py:771:                                            'AttentionMechanismNormedBahdanau')
nmt-master/nmt/model_test.py:773:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:774:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:775:        infer_m = self._createTestInferModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:776:                                             hparams, sess)
nmt-master/nmt/model_test.py:777:        self._assertInferLogits(infer_m, sess,
nmt-master/nmt/model_test.py:778:                                'AttentionMechanismNormedBahdanau')
nmt-master/nmt/model_test.py:780:  ## Test encoder vs. attention (all use residual):
nmt-master/nmt/model_test.py:781:  # uni encoder, standard attention
nmt-master/nmt/model_test.py:782:  def testUniEncoderStandardAttentionArchitecture(self):
nmt-master/nmt/model_test.py:783:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:784:        encoder_type='uni',
nmt-master/nmt/model_test.py:785:        num_layers=4,
nmt-master/nmt/model_test.py:786:        attention='scaled_luong',
nmt-master/nmt/model_test.py:787:        attention_architecture='standard',)
nmt-master/nmt/model_test.py:789:    workers, _ = tf.test.create_local_cluster(1, 0)
nmt-master/nmt/model_test.py:790:    worker = workers[0]
nmt-master/nmt/model_test.py:792:    # pylint: disable=line-too-long
nmt-master/nmt/model_test.py:793:    expected_var_names = [
nmt-master/nmt/model_test.py:794:        'dynamic_seq2seq/encoder/embedding_encoder:0',
nmt-master/nmt/model_test.py:795:        'dynamic_seq2seq/decoder/embedding_decoder:0',
nmt-master/nmt/model_test.py:796:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:797:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:798:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:799:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:800:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:801:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:802:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:803:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:804:        'dynamic_seq2seq/decoder/memory_layer/kernel:0',
nmt-master/nmt/model_test.py:805:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:806:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:807:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:808:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:809:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:810:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:811:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:812:        'dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:813:        'dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0',
nmt-master/nmt/model_test.py:814:        'dynamic_seq2seq/decoder/attention/attention_layer/kernel:0',
nmt-master/nmt/model_test.py:815:        'dynamic_seq2seq/decoder/output_projection/kernel:0'
nmt-master/nmt/model_test.py:816:    ]
nmt-master/nmt/model_test.py:817:    # pylint: enable=line-too-long
nmt-master/nmt/model_test.py:819:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:820:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:821:        train_m = self._createTestTrainModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:822:                                             hparams, sess)
nmt-master/nmt/model_test.py:824:        m_vars = tf.trainable_variables()
nmt-master/nmt/model_test.py:825:        self._assertModelVariableNames(expected_var_names, [
nmt-master/nmt/model_test.py:826:            v.name for v in m_vars
nmt-master/nmt/model_test.py:827:        ], 'UniEncoderStandardAttentionArchitecture')
nmt-master/nmt/model_test.py:828:        with tf.variable_scope('dynamic_seq2seq', reuse=True):
nmt-master/nmt/model_test.py:829:          last_enc_weight = tf.get_variable(
nmt-master/nmt/model_test.py:830:              'encoder/rnn/multi_rnn_cell/cell_3/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:831:          last_dec_weight = tf.get_variable(
nmt-master/nmt/model_test.py:832:              'decoder/attention/multi_rnn_cell/cell_3/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:833:          mem_layer_weight = tf.get_variable('decoder/memory_layer/kernel')
nmt-master/nmt/model_test.py:834:        self._assertTrainStepsLoss(train_m, sess,
nmt-master/nmt/model_test.py:835:                                   'UniEncoderStandardAttentionArchitecture')
nmt-master/nmt/model_test.py:836:        self._assertModelVariable(
nmt-master/nmt/model_test.py:837:            last_enc_weight, sess,
nmt-master/nmt/model_test.py:838:            'UniEncoderStandardAttentionArchitecture/last_enc_weight')
nmt-master/nmt/model_test.py:839:        self._assertModelVariable(
nmt-master/nmt/model_test.py:840:            last_dec_weight, sess,
nmt-master/nmt/model_test.py:841:            'UniEncoderStandardAttentionArchitecture/last_dec_weight')
nmt-master/nmt/model_test.py:842:        self._assertModelVariable(
nmt-master/nmt/model_test.py:843:            mem_layer_weight, sess,
nmt-master/nmt/model_test.py:844:            'UniEncoderStandardAttentionArchitecture/mem_layer_weight')
nmt-master/nmt/model_test.py:846:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:847:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:848:        eval_m = self._createTestEvalModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:849:                                           hparams, sess)
nmt-master/nmt/model_test.py:850:        self._assertEvalLossAndPredictCount(
nmt-master/nmt/model_test.py:851:            eval_m, sess, 'UniEncoderStandardAttentionArchitecture')
nmt-master/nmt/model_test.py:853:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:854:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:855:        infer_m = self._createTestInferModel(attention_model.AttentionModel,
nmt-master/nmt/model_test.py:856:                                             hparams, sess)
nmt-master/nmt/model_test.py:857:        self._assertInferLogits(infer_m, sess,
nmt-master/nmt/model_test.py:858:                                'UniEncoderStandardAttentionArchitecture')
nmt-master/nmt/model_test.py:860:  # Test gnmt model.
nmt-master/nmt/model_test.py:861:  def _testGNMTModel(self, architecture):
nmt-master/nmt/model_test.py:862:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:863:        encoder_type='gnmt',
nmt-master/nmt/model_test.py:864:        num_layers=4,
nmt-master/nmt/model_test.py:865:        attention='scaled_luong',
nmt-master/nmt/model_test.py:866:        attention_architecture=architecture)
nmt-master/nmt/model_test.py:868:    workers, _ = tf.test.create_local_cluster(1, 0)
nmt-master/nmt/model_test.py:869:    worker = workers[0]
nmt-master/nmt/model_test.py:871:    # pylint: disable=line-too-long
nmt-master/nmt/model_test.py:872:    expected_var_names = [
nmt-master/nmt/model_test.py:873:        'dynamic_seq2seq/encoder/embedding_encoder:0',
nmt-master/nmt/model_test.py:874:        'dynamic_seq2seq/decoder/embedding_decoder:0',
nmt-master/nmt/model_test.py:875:        'dynamic_seq2seq/encoder/bidirectional_rnn/fw/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:876:        'dynamic_seq2seq/encoder/bidirectional_rnn/fw/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:877:        'dynamic_seq2seq/encoder/bidirectional_rnn/bw/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:878:        'dynamic_seq2seq/encoder/bidirectional_rnn/bw/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:879:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:880:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:881:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:882:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:883:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:884:        'dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:885:        'dynamic_seq2seq/decoder/memory_layer/kernel:0',
nmt-master/nmt/model_test.py:886:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_0_attention/attention/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:887:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_0_attention/attention/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:888:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_0_attention/attention/luong_attention/attention_g:0',
nmt-master/nmt/model_test.py:889:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:890:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:891:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:892:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:893:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_3/basic_lstm_cell/kernel:0',
nmt-master/nmt/model_test.py:894:        'dynamic_seq2seq/decoder/multi_rnn_cell/cell_3/basic_lstm_cell/bias:0',
nmt-master/nmt/model_test.py:895:        'dynamic_seq2seq/decoder/output_projection/kernel:0'
nmt-master/nmt/model_test.py:896:    ]
nmt-master/nmt/model_test.py:897:    # pylint: enable=line-too-long
nmt-master/nmt/model_test.py:899:    test_prefix = 'GNMTModel_%s' % architecture
nmt-master/nmt/model_test.py:900:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:901:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:902:        train_m = self._createTestTrainModel(gnmt_model.GNMTModel, hparams,
nmt-master/nmt/model_test.py:903:                                             sess)
nmt-master/nmt/model_test.py:905:        m_vars = tf.trainable_variables()
nmt-master/nmt/model_test.py:906:        self._assertModelVariableNames(expected_var_names,
nmt-master/nmt/model_test.py:907:                                       [v.name for v in m_vars], test_prefix)
nmt-master/nmt/model_test.py:908:        with tf.variable_scope('dynamic_seq2seq', reuse=True):
nmt-master/nmt/model_test.py:909:          last_enc_weight = tf.get_variable(
nmt-master/nmt/model_test.py:910:              'encoder/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:911:          last_dec_weight = tf.get_variable(
nmt-master/nmt/model_test.py:912:              'decoder/multi_rnn_cell/cell_3/basic_lstm_cell/kernel')
nmt-master/nmt/model_test.py:913:          mem_layer_weight = tf.get_variable('decoder/memory_layer/kernel')
nmt-master/nmt/model_test.py:914:        self._assertTrainStepsLoss(train_m, sess, test_prefix)
nmt-master/nmt/model_test.py:916:        self._assertModelVariable(last_enc_weight, sess,
nmt-master/nmt/model_test.py:917:                                  '%s/last_enc_weight' % test_prefix)
nmt-master/nmt/model_test.py:918:        self._assertModelVariable(last_dec_weight, sess,
nmt-master/nmt/model_test.py:919:                                  '%s/last_dec_weight' % test_prefix)
nmt-master/nmt/model_test.py:920:        self._assertModelVariable(mem_layer_weight, sess,
nmt-master/nmt/model_test.py:921:                                  '%s/mem_layer_weight' % test_prefix)
nmt-master/nmt/model_test.py:923:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:924:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:925:        eval_m = self._createTestEvalModel(gnmt_model.GNMTModel, hparams, sess)
nmt-master/nmt/model_test.py:926:        self._assertEvalLossAndPredictCount(eval_m, sess, test_prefix)
nmt-master/nmt/model_test.py:928:    with tf.Graph().as_default():
nmt-master/nmt/model_test.py:929:      with tf.Session(worker.target, config=self._get_session_config()) as sess:
nmt-master/nmt/model_test.py:930:        infer_m = self._createTestInferModel(gnmt_model.GNMTModel, hparams,
nmt-master/nmt/model_test.py:931:                                             sess)
nmt-master/nmt/model_test.py:932:        self._assertInferLogits(infer_m, sess, test_prefix)
nmt-master/nmt/model_test.py:934:  def testGNMTModel(self):
nmt-master/nmt/model_test.py:935:    self._testGNMTModel('gnmt')
nmt-master/nmt/model_test.py:937:  def testGNMTModelV2(self):
nmt-master/nmt/model_test.py:938:    self._testGNMTModel('gnmt_v2')
nmt-master/nmt/model_test.py:940:  # Test beam search.
nmt-master/nmt/model_test.py:941:  def testBeamSearchBasicModel(self):
nmt-master/nmt/model_test.py:942:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:943:        encoder_type='uni',
nmt-master/nmt/model_test.py:944:        num_layers=1,
nmt-master/nmt/model_test.py:945:        attention='',
nmt-master/nmt/model_test.py:946:        attention_architecture='',
nmt-master/nmt/model_test.py:947:        use_residual=False,)
nmt-master/nmt/model_test.py:948:    hparams.beam_width = 3
nmt-master/nmt/model_test.py:949:    hparams.infer_mode = "beam_search"
nmt-master/nmt/model_test.py:950:    hparams.tgt_max_len_infer = 4
nmt-master/nmt/model_test.py:951:    assert_top_k_sentence = 3
nmt-master/nmt/model_test.py:953:    with self.test_session() as sess:
nmt-master/nmt/model_test.py:954:      infer_m = self._createTestInferModel(
nmt-master/nmt/model_test.py:955:          model.Model, hparams, sess, True)
nmt-master/nmt/model_test.py:956:      self._assertBeamSearchOutputs(
nmt-master/nmt/model_test.py:957:          infer_m, sess, assert_top_k_sentence, 'BeamSearchBasicModel')
nmt-master/nmt/model_test.py:959:  def testBeamSearchAttentionModel(self):
nmt-master/nmt/model_test.py:960:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:961:        encoder_type='uni',
nmt-master/nmt/model_test.py:962:        attention='scaled_luong',
nmt-master/nmt/model_test.py:963:        attention_architecture='standard',
nmt-master/nmt/model_test.py:964:        num_layers=2,
nmt-master/nmt/model_test.py:965:        use_residual=False,)
nmt-master/nmt/model_test.py:966:    hparams.beam_width = 3
nmt-master/nmt/model_test.py:967:    hparams.infer_mode = "beam_search"
nmt-master/nmt/model_test.py:968:    hparams.tgt_max_len_infer = 4
nmt-master/nmt/model_test.py:969:    assert_top_k_sentence = 2
nmt-master/nmt/model_test.py:971:    with self.test_session() as sess:
nmt-master/nmt/model_test.py:972:      infer_m = self._createTestInferModel(
nmt-master/nmt/model_test.py:973:          attention_model.AttentionModel, hparams, sess, True)
nmt-master/nmt/model_test.py:974:      self._assertBeamSearchOutputs(
nmt-master/nmt/model_test.py:975:          infer_m, sess, assert_top_k_sentence, 'BeamSearchAttentionModel')
nmt-master/nmt/model_test.py:977:  def testBeamSearchGNMTModel(self):
nmt-master/nmt/model_test.py:978:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:979:        encoder_type='gnmt',
nmt-master/nmt/model_test.py:980:        num_layers=4,
nmt-master/nmt/model_test.py:981:        attention='scaled_luong',
nmt-master/nmt/model_test.py:982:        attention_architecture='gnmt')
nmt-master/nmt/model_test.py:983:    hparams.beam_width = 3
nmt-master/nmt/model_test.py:984:    hparams.infer_mode = "beam_search"
nmt-master/nmt/model_test.py:985:    hparams.tgt_max_len_infer = 4
nmt-master/nmt/model_test.py:986:    assert_top_k_sentence = 1
nmt-master/nmt/model_test.py:988:    with self.test_session() as sess:
nmt-master/nmt/model_test.py:989:      infer_m = self._createTestInferModel(
nmt-master/nmt/model_test.py:990:          gnmt_model.GNMTModel, hparams, sess, True)
nmt-master/nmt/model_test.py:991:      self._assertBeamSearchOutputs(
nmt-master/nmt/model_test.py:992:          infer_m, sess, assert_top_k_sentence, 'BeamSearchGNMTModel')
nmt-master/nmt/model_test.py:994:  def testInitializerGlorotNormal(self):
nmt-master/nmt/model_test.py:995:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:996:        encoder_type='uni',
nmt-master/nmt/model_test.py:997:        num_layers=1,
nmt-master/nmt/model_test.py:998:        attention='',
nmt-master/nmt/model_test.py:999:        attention_architecture='',
nmt-master/nmt/model_test.py:1000:        use_residual=False,
nmt-master/nmt/model_test.py:1001:        init_op='glorot_normal')
nmt-master/nmt/model_test.py:1003:    with self.test_session() as sess:
nmt-master/nmt/model_test.py:1004:      train_m = self._createTestTrainModel(model.Model, hparams, sess)
nmt-master/nmt/model_test.py:1005:      self._assertTrainStepsLoss(train_m, sess,
nmt-master/nmt/model_test.py:1006:                                 'InitializerGlorotNormal')
nmt-master/nmt/model_test.py:1008:  def testInitializerGlorotUniform(self):
nmt-master/nmt/model_test.py:1009:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:1010:        encoder_type='uni',
nmt-master/nmt/model_test.py:1011:        num_layers=1,
nmt-master/nmt/model_test.py:1012:        attention='',
nmt-master/nmt/model_test.py:1013:        attention_architecture='',
nmt-master/nmt/model_test.py:1014:        use_residual=False,
nmt-master/nmt/model_test.py:1015:        init_op='glorot_uniform')
nmt-master/nmt/model_test.py:1017:    with self.test_session() as sess:
nmt-master/nmt/model_test.py:1018:      train_m = self._createTestTrainModel(model.Model, hparams, sess)
nmt-master/nmt/model_test.py:1019:      self._assertTrainStepsLoss(train_m, sess,
nmt-master/nmt/model_test.py:1020:                                 'InitializerGlorotUniform')
nmt-master/nmt/model_test.py:1022:  def testSampledSoftmaxLoss(self):
nmt-master/nmt/model_test.py:1023:    hparams = common_test_utils.create_test_hparams(
nmt-master/nmt/model_test.py:1024:        encoder_type='gnmt',
nmt-master/nmt/model_test.py:1025:        num_layers=4,
nmt-master/nmt/model_test.py:1026:        attention='scaled_luong',
nmt-master/nmt/model_test.py:1027:        attention_architecture='gnmt')
nmt-master/nmt/model_test.py:1028:    hparams.num_sampled_softmax = 3
nmt-master/nmt/model_test.py:1030:    with self.test_session() as sess:
nmt-master/nmt/model_test.py:1031:      train_m = self._createTestTrainModel(gnmt_model.GNMTModel, hparams, sess)
nmt-master/nmt/model_test.py:1032:      self._assertTrainStepsLoss(train_m, sess,
nmt-master/nmt/model_test.py:1033:                                 'SampledSoftmaxLoss')
nmt-master/nmt/model_test.py:1035:if __name__ == '__main__':
nmt-master/nmt/model_test.py:1036:  tf.test.main()
nmt-master/nmt/nmt.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/nmt.py:2:#
nmt-master/nmt/nmt.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/nmt.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/nmt.py:5:# You may obtain a copy of the License at
nmt-master/nmt/nmt.py:6:#
nmt-master/nmt/nmt.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/nmt.py:8:#
nmt-master/nmt/nmt.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/nmt.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/nmt.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/nmt.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/nmt.py:13:# limitations under the License.
nmt-master/nmt/nmt.py:14:# ==============================================================================
nmt-master/nmt/nmt.py:16:"""TensorFlow NMT model implementation."""
nmt-master/nmt/nmt.py:17:from __future__ import print_function
nmt-master/nmt/nmt.py:19:import argparse
nmt-master/nmt/nmt.py:20:import os
nmt-master/nmt/nmt.py:21:import random
nmt-master/nmt/nmt.py:22:import sys
nmt-master/nmt/nmt.py:24:# import matplotlib.image as mpimg
nmt-master/nmt/nmt.py:25:import numpy as np
nmt-master/nmt/nmt.py:26:import tensorflow as tf
nmt-master/nmt/nmt.py:28:from . import inference
nmt-master/nmt/nmt.py:29:from . import train
nmt-master/nmt/nmt.py:30:from .utils import evaluation_utils
nmt-master/nmt/nmt.py:31:from .utils import misc_utils as utils
nmt-master/nmt/nmt.py:32:from .utils import vocab_utils
nmt-master/nmt/nmt.py:34:utils.check_tensorflow_version()
nmt-master/nmt/nmt.py:36:FLAGS = None
nmt-master/nmt/nmt.py:38:INFERENCE_KEYS = ["src_max_len_infer", "tgt_max_len_infer", "subword_option",
nmt-master/nmt/nmt.py:39:                  "infer_batch_size", "beam_width",
nmt-master/nmt/nmt.py:40:                  "length_penalty_weight", "coverage_penalty_weight",
nmt-master/nmt/nmt.py:41:                  "sampling_temperature", "num_translations_per_input",
nmt-master/nmt/nmt.py:42:                  "infer_mode"]
nmt-master/nmt/nmt.py:45:def add_arguments(parser):
nmt-master/nmt/nmt.py:46:  """Build ArgumentParser."""
nmt-master/nmt/nmt.py:47:  parser.register("type", "bool", lambda v: v.lower() == "true")
nmt-master/nmt/nmt.py:49:  # network
nmt-master/nmt/nmt.py:50:  parser.add_argument("--num_units", type=int, default=32, help="Network size.")
nmt-master/nmt/nmt.py:51:  parser.add_argument("--num_layers", type=int, default=2,
nmt-master/nmt/nmt.py:52:                      help="Network depth.")
nmt-master/nmt/nmt.py:53:  parser.add_argument("--num_encoder_layers", type=int, default=None,
nmt-master/nmt/nmt.py:54:                      help="Encoder depth, equal to num_layers if None.")
nmt-master/nmt/nmt.py:55:  parser.add_argument("--num_decoder_layers", type=int, default=None,
nmt-master/nmt/nmt.py:56:                      help="Decoder depth, equal to num_layers if None.")
nmt-master/nmt/nmt.py:57:  parser.add_argument("--encoder_type", type=str, default="uni", help="""\
nmt-master/nmt/nmt.py:58:      uni | bi | gnmt.
nmt-master/nmt/nmt.py:59:      For bi, we build num_encoder_layers/2 bi-directional layers.
nmt-master/nmt/nmt.py:60:      For gnmt, we build 1 bi-directional layer, and (num_encoder_layers - 1)
nmt-master/nmt/nmt.py:61:        uni-directional layers.\
nmt-master/nmt/nmt.py:62:      """)
nmt-master/nmt/nmt.py:63:  parser.add_argument("--residual", type="bool", nargs="?", const=True,
nmt-master/nmt/nmt.py:64:                      default=False,
nmt-master/nmt/nmt.py:65:                      help="Whether to add residual connections.")
nmt-master/nmt/nmt.py:66:  parser.add_argument("--time_major", type="bool", nargs="?", const=True,
nmt-master/nmt/nmt.py:67:                      default=True,
nmt-master/nmt/nmt.py:68:                      help="Whether to use time-major mode for dynamic RNN.")
nmt-master/nmt/nmt.py:69:  parser.add_argument("--num_embeddings_partitions", type=int, default=0,
nmt-master/nmt/nmt.py:70:                      help="Number of partitions for embedding vars.")
nmt-master/nmt/nmt.py:72:  # attention mechanisms
nmt-master/nmt/nmt.py:73:  parser.add_argument("--attention", type=str, default="", help="""\
nmt-master/nmt/nmt.py:74:      luong | scaled_luong | bahdanau | normed_bahdanau or set to "" for no
nmt-master/nmt/nmt.py:75:      attention\
nmt-master/nmt/nmt.py:76:      """)
nmt-master/nmt/nmt.py:77:  parser.add_argument(
nmt-master/nmt/nmt.py:78:      "--attention_architecture",
nmt-master/nmt/nmt.py:79:      type=str,
nmt-master/nmt/nmt.py:80:      default="standard",
nmt-master/nmt/nmt.py:81:      help="""\
nmt-master/nmt/nmt.py:82:      standard | gnmt | gnmt_v2.
nmt-master/nmt/nmt.py:83:      standard: use top layer to compute attention.
nmt-master/nmt/nmt.py:84:      gnmt: GNMT style of computing attention, use previous bottom layer to
nmt-master/nmt/nmt.py:85:          compute attention.
nmt-master/nmt/nmt.py:86:      gnmt_v2: similar to gnmt, but use current bottom layer to compute
nmt-master/nmt/nmt.py:87:          attention.\
nmt-master/nmt/nmt.py:88:      """)
nmt-master/nmt/nmt.py:89:  parser.add_argument(
nmt-master/nmt/nmt.py:90:      "--output_attention", type="bool", nargs="?", const=True,
nmt-master/nmt/nmt.py:91:      default=True,
nmt-master/nmt/nmt.py:92:      help="""\
nmt-master/nmt/nmt.py:93:      Only used in standard attention_architecture. Whether use attention as
nmt-master/nmt/nmt.py:94:      the cell output at each timestep.
nmt-master/nmt/nmt.py:95:      .\
nmt-master/nmt/nmt.py:96:      """)
nmt-master/nmt/nmt.py:97:  parser.add_argument(
nmt-master/nmt/nmt.py:98:      "--pass_hidden_state", type="bool", nargs="?", const=True,
nmt-master/nmt/nmt.py:99:      default=True,
nmt-master/nmt/nmt.py:100:      help="""\
nmt-master/nmt/nmt.py:101:      Whether to pass encoder's hidden state to decoder when using an attention
nmt-master/nmt/nmt.py:102:      based model.\
nmt-master/nmt/nmt.py:103:      """)
nmt-master/nmt/nmt.py:105:  # optimizer
nmt-master/nmt/nmt.py:106:  parser.add_argument("--optimizer", type=str, default="sgd", help="sgd | adam")
nmt-master/nmt/nmt.py:107:  parser.add_argument("--learning_rate", type=float, default=1.0,
nmt-master/nmt/nmt.py:108:                      help="Learning rate. Adam: 0.001 | 0.0001")
nmt-master/nmt/nmt.py:109:  parser.add_argument("--warmup_steps", type=int, default=0,
nmt-master/nmt/nmt.py:110:                      help="How many steps we inverse-decay learning.")
nmt-master/nmt/nmt.py:111:  parser.add_argument("--warmup_scheme", type=str, default="t2t", help="""\
nmt-master/nmt/nmt.py:112:      How to warmup learning rates. Options include:
nmt-master/nmt/nmt.py:113:        t2t: Tensor2Tensor's way, start with lr 100 times smaller, then
nmt-master/nmt/nmt.py:114:             exponentiate until the specified lr.\
nmt-master/nmt/nmt.py:115:      """)
nmt-master/nmt/nmt.py:116:  parser.add_argument(
nmt-master/nmt/nmt.py:117:      "--decay_scheme", type=str, default="", help="""\
nmt-master/nmt/nmt.py:118:      How we decay learning rate. Options include:
nmt-master/nmt/nmt.py:119:        luong234: after 2/3 num train steps, we start halving the learning rate
nmt-master/nmt/nmt.py:120:          for 4 times before finishing.
nmt-master/nmt/nmt.py:121:        luong5: after 1/2 num train steps, we start halving the learning rate
nmt-master/nmt/nmt.py:122:          for 5 times before finishing.\
nmt-master/nmt/nmt.py:123:        luong10: after 1/2 num train steps, we start halving the learning rate
nmt-master/nmt/nmt.py:124:          for 10 times before finishing.\
nmt-master/nmt/nmt.py:125:      """)
nmt-master/nmt/nmt.py:127:  parser.add_argument(
nmt-master/nmt/nmt.py:128:      "--num_train_steps", type=int, default=12000, help="Num steps to train.")
nmt-master/nmt/nmt.py:129:  parser.add_argument("--colocate_gradients_with_ops", type="bool", nargs="?",
nmt-master/nmt/nmt.py:130:                      const=True,
nmt-master/nmt/nmt.py:131:                      default=True,
nmt-master/nmt/nmt.py:132:                      help=("Whether try colocating gradients with "
nmt-master/nmt/nmt.py:133:                            "corresponding op"))
nmt-master/nmt/nmt.py:135:  # initializer
nmt-master/nmt/nmt.py:136:  parser.add_argument("--init_op", type=str, default="uniform",
nmt-master/nmt/nmt.py:137:                      help="uniform | glorot_normal | glorot_uniform")
nmt-master/nmt/nmt.py:138:  parser.add_argument("--init_weight", type=float, default=0.1,
nmt-master/nmt/nmt.py:139:                      help=("for uniform init_op, initialize weights "
nmt-master/nmt/nmt.py:140:                            "between [-this, this]."))
nmt-master/nmt/nmt.py:142:  # data
nmt-master/nmt/nmt.py:143:  parser.add_argument("--src", type=str, default=None,
nmt-master/nmt/nmt.py:144:                      help="Source suffix, e.g., en.")
nmt-master/nmt/nmt.py:145:  parser.add_argument("--tgt", type=str, default=None,
nmt-master/nmt/nmt.py:146:                      help="Target suffix, e.g., de.")
nmt-master/nmt/nmt.py:147:  parser.add_argument("--train_prefix", type=str, default=None,
nmt-master/nmt/nmt.py:148:                      help="Train prefix, expect files with src/tgt suffixes.")
nmt-master/nmt/nmt.py:149:  parser.add_argument("--dev_prefix", type=str, default=None,
nmt-master/nmt/nmt.py:150:                      help="Dev prefix, expect files with src/tgt suffixes.")
nmt-master/nmt/nmt.py:151:  parser.add_argument("--test_prefix", type=str, default=None,
nmt-master/nmt/nmt.py:152:                      help="Test prefix, expect files with src/tgt suffixes.")
nmt-master/nmt/nmt.py:153:  parser.add_argument("--out_dir", type=str, default=None,
nmt-master/nmt/nmt.py:154:                      help="Store log/model files.")
nmt-master/nmt/nmt.py:156:  # Vocab
nmt-master/nmt/nmt.py:157:  parser.add_argument("--vocab_prefix", type=str, default=None, help="""\
nmt-master/nmt/nmt.py:158:      Vocab prefix, expect files with src/tgt suffixes.\
nmt-master/nmt/nmt.py:159:      """)
nmt-master/nmt/nmt.py:160:  parser.add_argument("--embed_prefix", type=str, default=None, help="""\
nmt-master/nmt/nmt.py:161:      Pretrained embedding prefix, expect files with src/tgt suffixes.
nmt-master/nmt/nmt.py:162:      The embedding files should be Glove formated txt files.\
nmt-master/nmt/nmt.py:163:      """)
nmt-master/nmt/nmt.py:164:  parser.add_argument("--sos", type=str, default="<s>",
nmt-master/nmt/nmt.py:165:                      help="Start-of-sentence symbol.")
nmt-master/nmt/nmt.py:166:  parser.add_argument("--eos", type=str, default="</s>",
nmt-master/nmt/nmt.py:167:                      help="End-of-sentence symbol.")
nmt-master/nmt/nmt.py:168:  parser.add_argument("--share_vocab", type="bool", nargs="?", const=True,
nmt-master/nmt/nmt.py:169:                      default=False,
nmt-master/nmt/nmt.py:170:                      help="""\
nmt-master/nmt/nmt.py:171:      Whether to use the source vocab and embeddings for both source and
nmt-master/nmt/nmt.py:172:      target.\
nmt-master/nmt/nmt.py:173:      """)
nmt-master/nmt/nmt.py:174:  parser.add_argument("--check_special_token", type="bool", default=True,
nmt-master/nmt/nmt.py:175:                      help="""\
nmt-master/nmt/nmt.py:176:                      Whether check special sos, eos, unk tokens exist in the
nmt-master/nmt/nmt.py:177:                      vocab files.\
nmt-master/nmt/nmt.py:178:                      """)
nmt-master/nmt/nmt.py:180:  # Sequence lengths
nmt-master/nmt/nmt.py:181:  parser.add_argument("--src_max_len", type=int, default=50,
nmt-master/nmt/nmt.py:182:                      help="Max length of src sequences during training.")
nmt-master/nmt/nmt.py:183:  parser.add_argument("--tgt_max_len", type=int, default=50,
nmt-master/nmt/nmt.py:184:                      help="Max length of tgt sequences during training.")
nmt-master/nmt/nmt.py:185:  parser.add_argument("--src_max_len_infer", type=int, default=None,
nmt-master/nmt/nmt.py:186:                      help="Max length of src sequences during inference.")
nmt-master/nmt/nmt.py:187:  parser.add_argument("--tgt_max_len_infer", type=int, default=None,
nmt-master/nmt/nmt.py:188:                      help="""\
nmt-master/nmt/nmt.py:189:      Max length of tgt sequences during inference.  Also use to restrict the
nmt-master/nmt/nmt.py:190:      maximum decoding length.\
nmt-master/nmt/nmt.py:191:      """)
nmt-master/nmt/nmt.py:193:  # Default settings works well (rarely need to change)
nmt-master/nmt/nmt.py:194:  parser.add_argument("--unit_type", type=str, default="lstm",
nmt-master/nmt/nmt.py:195:                      help="lstm | gru | layer_norm_lstm | nas")
nmt-master/nmt/nmt.py:196:  parser.add_argument("--forget_bias", type=float, default=1.0,
nmt-master/nmt/nmt.py:197:                      help="Forget bias for BasicLSTMCell.")
nmt-master/nmt/nmt.py:198:  parser.add_argument("--dropout", type=float, default=0.2,
nmt-master/nmt/nmt.py:199:                      help="Dropout rate (not keep_prob)")
nmt-master/nmt/nmt.py:200:  parser.add_argument("--max_gradient_norm", type=float, default=5.0,
nmt-master/nmt/nmt.py:201:                      help="Clip gradients to this norm.")
nmt-master/nmt/nmt.py:202:  parser.add_argument("--batch_size", type=int, default=128, help="Batch size.")
nmt-master/nmt/nmt.py:204:  parser.add_argument("--steps_per_stats", type=int, default=100,
nmt-master/nmt/nmt.py:205:                      help=("How many training steps to do per stats logging."
nmt-master/nmt/nmt.py:206:                            "Save checkpoint every 10x steps_per_stats"))
nmt-master/nmt/nmt.py:207:  parser.add_argument("--max_train", type=int, default=0,
nmt-master/nmt/nmt.py:208:                      help="Limit on the size of training data (0: no limit).")
nmt-master/nmt/nmt.py:209:  parser.add_argument("--num_buckets", type=int, default=5,
nmt-master/nmt/nmt.py:210:                      help="Put data into similar-length buckets.")
nmt-master/nmt/nmt.py:211:  parser.add_argument("--num_sampled_softmax", type=int, default=0,
nmt-master/nmt/nmt.py:212:                      help=("Use sampled_softmax_loss if > 0."
nmt-master/nmt/nmt.py:213:                            "Otherwise, use full softmax loss."))
nmt-master/nmt/nmt.py:215:  # SPM
nmt-master/nmt/nmt.py:216:  parser.add_argument("--subword_option", type=str, default="",
nmt-master/nmt/nmt.py:217:                      choices=["", "bpe", "spm"],
nmt-master/nmt/nmt.py:218:                      help="""\
nmt-master/nmt/nmt.py:219:                      Set to bpe or spm to activate subword desegmentation.\
nmt-master/nmt/nmt.py:220:                      """)
nmt-master/nmt/nmt.py:222:  # Experimental encoding feature.
nmt-master/nmt/nmt.py:223:  parser.add_argument("--use_char_encode", type="bool", default=False,
nmt-master/nmt/nmt.py:224:                      help="""\
nmt-master/nmt/nmt.py:225:                      Whether to split each word or bpe into character, and then
nmt-master/nmt/nmt.py:226:                      generate the word-level representation from the character
nmt-master/nmt/nmt.py:227:                      reprentation.
nmt-master/nmt/nmt.py:228:                      """)
nmt-master/nmt/nmt.py:230:  # Misc
nmt-master/nmt/nmt.py:231:  parser.add_argument("--num_gpus", type=int, default=1,
nmt-master/nmt/nmt.py:232:                      help="Number of gpus in each worker.")
nmt-master/nmt/nmt.py:233:  parser.add_argument("--log_device_placement", type="bool", nargs="?",
nmt-master/nmt/nmt.py:234:                      const=True, default=False, help="Debug GPU allocation.")
nmt-master/nmt/nmt.py:235:  parser.add_argument("--metrics", type=str, default="bleu",
nmt-master/nmt/nmt.py:236:                      help=("Comma-separated list of evaluations "
nmt-master/nmt/nmt.py:237:                            "metrics (bleu,rouge,accuracy)"))
nmt-master/nmt/nmt.py:238:  parser.add_argument("--steps_per_external_eval", type=int, default=None,
nmt-master/nmt/nmt.py:239:                      help="""\
nmt-master/nmt/nmt.py:240:      How many training steps to do per external evaluation.  Automatically set
nmt-master/nmt/nmt.py:241:      based on data if None.\
nmt-master/nmt/nmt.py:242:      """)
nmt-master/nmt/nmt.py:243:  parser.add_argument("--scope", type=str, default=None,
nmt-master/nmt/nmt.py:244:                      help="scope to put variables under")
nmt-master/nmt/nmt.py:245:  parser.add_argument("--hparams_path", type=str, default=None,
nmt-master/nmt/nmt.py:246:                      help=("Path to standard hparams json file that overrides"
nmt-master/nmt/nmt.py:247:                            "hparams values from FLAGS."))
nmt-master/nmt/nmt.py:248:  parser.add_argument("--random_seed", type=int, default=None,
nmt-master/nmt/nmt.py:249:                      help="Random seed (>0, set a specific seed).")
nmt-master/nmt/nmt.py:250:  parser.add_argument("--override_loaded_hparams", type="bool", nargs="?",
nmt-master/nmt/nmt.py:251:                      const=True, default=False,
nmt-master/nmt/nmt.py:252:                      help="Override loaded hparams with values specified")
nmt-master/nmt/nmt.py:253:  parser.add_argument("--num_keep_ckpts", type=int, default=5,
nmt-master/nmt/nmt.py:254:                      help="Max number of checkpoints to keep.")
nmt-master/nmt/nmt.py:255:  parser.add_argument("--avg_ckpts", type="bool", nargs="?",
nmt-master/nmt/nmt.py:256:                      const=True, default=False, help=("""\
nmt-master/nmt/nmt.py:257:                      Average the last N checkpoints for external evaluation.
nmt-master/nmt/nmt.py:258:                      N can be controlled by setting --num_keep_ckpts.\
nmt-master/nmt/nmt.py:259:                      """))
nmt-master/nmt/nmt.py:260:  parser.add_argument("--language_model", type="bool", nargs="?",
nmt-master/nmt/nmt.py:261:                      const=True, default=False,
nmt-master/nmt/nmt.py:262:                      help="True to train a language model, ignoring encoder")
nmt-master/nmt/nmt.py:264:  # Inference
nmt-master/nmt/nmt.py:265:  parser.add_argument("--ckpt", type=str, default="",
nmt-master/nmt/nmt.py:266:                      help="Checkpoint file to load a model for inference.")
nmt-master/nmt/nmt.py:267:  parser.add_argument("--inference_input_file", type=str, default=None,
nmt-master/nmt/nmt.py:268:                      help="Set to the text to decode.")
nmt-master/nmt/nmt.py:269:  parser.add_argument("--inference_list", type=str, default=None,
nmt-master/nmt/nmt.py:270:                      help=("A comma-separated list of sentence indices "
nmt-master/nmt/nmt.py:271:                            "(0-based) to decode."))
nmt-master/nmt/nmt.py:272:  parser.add_argument("--infer_batch_size", type=int, default=32,
nmt-master/nmt/nmt.py:273:                      help="Batch size for inference mode.")
nmt-master/nmt/nmt.py:274:  parser.add_argument("--inference_output_file", type=str, default=None,
nmt-master/nmt/nmt.py:275:                      help="Output file to store decoding results.")
nmt-master/nmt/nmt.py:276:  parser.add_argument("--inference_ref_file", type=str, default=None,
nmt-master/nmt/nmt.py:277:                      help=("""\
nmt-master/nmt/nmt.py:278:      Reference file to compute evaluation scores (if provided).\
nmt-master/nmt/nmt.py:279:      """))
nmt-master/nmt/nmt.py:281:  # Advanced inference arguments
nmt-master/nmt/nmt.py:282:  parser.add_argument("--infer_mode", type=str, default="greedy",
nmt-master/nmt/nmt.py:283:                      choices=["greedy", "sample", "beam_search"],
nmt-master/nmt/nmt.py:284:                      help="Which type of decoder to use during inference.")
nmt-master/nmt/nmt.py:285:  parser.add_argument("--beam_width", type=int, default=0,
nmt-master/nmt/nmt.py:286:                      help=("""\
nmt-master/nmt/nmt.py:287:      beam width when using beam search decoder. If 0 (default), use standard
nmt-master/nmt/nmt.py:288:      decoder with greedy helper.\
nmt-master/nmt/nmt.py:289:      """))
nmt-master/nmt/nmt.py:290:  parser.add_argument("--length_penalty_weight", type=float, default=0.0,
nmt-master/nmt/nmt.py:291:                      help="Length penalty for beam search.")
nmt-master/nmt/nmt.py:292:  parser.add_argument("--coverage_penalty_weight", type=float, default=0.0,
nmt-master/nmt/nmt.py:293:                      help="Coverage penalty for beam search.")
nmt-master/nmt/nmt.py:294:  parser.add_argument("--sampling_temperature", type=float,
nmt-master/nmt/nmt.py:295:                      default=0.0,
nmt-master/nmt/nmt.py:296:                      help=("""\
nmt-master/nmt/nmt.py:297:      Softmax sampling temperature for inference decoding, 0.0 means greedy
nmt-master/nmt/nmt.py:298:      decoding. This option is ignored when using beam search.\
nmt-master/nmt/nmt.py:299:      """))
nmt-master/nmt/nmt.py:300:  parser.add_argument("--num_translations_per_input", type=int, default=1,
nmt-master/nmt/nmt.py:301:                      help=("""\
nmt-master/nmt/nmt.py:302:      Number of translations generated for each sentence. This is only used for
nmt-master/nmt/nmt.py:303:      inference.\
nmt-master/nmt/nmt.py:304:      """))
nmt-master/nmt/nmt.py:306:  # Job info
nmt-master/nmt/nmt.py:307:  parser.add_argument("--jobid", type=int, default=0,
nmt-master/nmt/nmt.py:308:                      help="Task id of the worker.")
nmt-master/nmt/nmt.py:309:  parser.add_argument("--num_workers", type=int, default=1,
nmt-master/nmt/nmt.py:310:                      help="Number of workers (inference only).")
nmt-master/nmt/nmt.py:311:  parser.add_argument("--num_inter_threads", type=int, default=0,
nmt-master/nmt/nmt.py:312:                      help="number of inter_op_parallelism_threads")
nmt-master/nmt/nmt.py:313:  parser.add_argument("--num_intra_threads", type=int, default=0,
nmt-master/nmt/nmt.py:314:                      help="number of intra_op_parallelism_threads")
nmt-master/nmt/nmt.py:317:def create_hparams(flags):
nmt-master/nmt/nmt.py:318:  """Create training hparams."""
nmt-master/nmt/nmt.py:319:  return tf.contrib.training.HParams(
nmt-master/nmt/nmt.py:320:      # Data
nmt-master/nmt/nmt.py:321:      src=flags.src,
nmt-master/nmt/nmt.py:322:      tgt=flags.tgt,
nmt-master/nmt/nmt.py:323:      train_prefix=flags.train_prefix,
nmt-master/nmt/nmt.py:324:      dev_prefix=flags.dev_prefix,
nmt-master/nmt/nmt.py:325:      test_prefix=flags.test_prefix,
nmt-master/nmt/nmt.py:326:      vocab_prefix=flags.vocab_prefix,
nmt-master/nmt/nmt.py:327:      embed_prefix=flags.embed_prefix,
nmt-master/nmt/nmt.py:328:      out_dir=flags.out_dir,
nmt-master/nmt/nmt.py:330:      # Networks
nmt-master/nmt/nmt.py:331:      num_units=flags.num_units,
nmt-master/nmt/nmt.py:332:      num_encoder_layers=(flags.num_encoder_layers or flags.num_layers),
nmt-master/nmt/nmt.py:333:      num_decoder_layers=(flags.num_decoder_layers or flags.num_layers),
nmt-master/nmt/nmt.py:334:      dropout=flags.dropout,
nmt-master/nmt/nmt.py:335:      unit_type=flags.unit_type,
nmt-master/nmt/nmt.py:336:      encoder_type=flags.encoder_type,
nmt-master/nmt/nmt.py:337:      residual=flags.residual,
nmt-master/nmt/nmt.py:338:      time_major=flags.time_major,
nmt-master/nmt/nmt.py:339:      num_embeddings_partitions=flags.num_embeddings_partitions,
nmt-master/nmt/nmt.py:341:      # Attention mechanisms
nmt-master/nmt/nmt.py:342:      attention=flags.attention,
nmt-master/nmt/nmt.py:343:      attention_architecture=flags.attention_architecture,
nmt-master/nmt/nmt.py:344:      output_attention=flags.output_attention,
nmt-master/nmt/nmt.py:345:      pass_hidden_state=flags.pass_hidden_state,
nmt-master/nmt/nmt.py:347:      # Train
nmt-master/nmt/nmt.py:348:      optimizer=flags.optimizer,
nmt-master/nmt/nmt.py:349:      num_train_steps=flags.num_train_steps,
nmt-master/nmt/nmt.py:350:      batch_size=flags.batch_size,
nmt-master/nmt/nmt.py:351:      init_op=flags.init_op,
nmt-master/nmt/nmt.py:352:      init_weight=flags.init_weight,
nmt-master/nmt/nmt.py:353:      max_gradient_norm=flags.max_gradient_norm,
nmt-master/nmt/nmt.py:354:      learning_rate=flags.learning_rate,
nmt-master/nmt/nmt.py:355:      warmup_steps=flags.warmup_steps,
nmt-master/nmt/nmt.py:356:      warmup_scheme=flags.warmup_scheme,
nmt-master/nmt/nmt.py:357:      decay_scheme=flags.decay_scheme,
nmt-master/nmt/nmt.py:358:      colocate_gradients_with_ops=flags.colocate_gradients_with_ops,
nmt-master/nmt/nmt.py:359:      num_sampled_softmax=flags.num_sampled_softmax,
nmt-master/nmt/nmt.py:361:      # Data constraints
nmt-master/nmt/nmt.py:362:      num_buckets=flags.num_buckets,
nmt-master/nmt/nmt.py:363:      max_train=flags.max_train,
nmt-master/nmt/nmt.py:364:      src_max_len=flags.src_max_len,
nmt-master/nmt/nmt.py:365:      tgt_max_len=flags.tgt_max_len,
nmt-master/nmt/nmt.py:367:      # Inference
nmt-master/nmt/nmt.py:368:      src_max_len_infer=flags.src_max_len_infer,
nmt-master/nmt/nmt.py:369:      tgt_max_len_infer=flags.tgt_max_len_infer,
nmt-master/nmt/nmt.py:370:      infer_batch_size=flags.infer_batch_size,
nmt-master/nmt/nmt.py:372:      # Advanced inference arguments
nmt-master/nmt/nmt.py:373:      infer_mode=flags.infer_mode,
nmt-master/nmt/nmt.py:374:      beam_width=flags.beam_width,
nmt-master/nmt/nmt.py:375:      length_penalty_weight=flags.length_penalty_weight,
nmt-master/nmt/nmt.py:376:      coverage_penalty_weight=flags.coverage_penalty_weight,
nmt-master/nmt/nmt.py:377:      sampling_temperature=flags.sampling_temperature,
nmt-master/nmt/nmt.py:378:      num_translations_per_input=flags.num_translations_per_input,
nmt-master/nmt/nmt.py:380:      # Vocab
nmt-master/nmt/nmt.py:381:      sos=flags.sos if flags.sos else vocab_utils.SOS,
nmt-master/nmt/nmt.py:382:      eos=flags.eos if flags.eos else vocab_utils.EOS,
nmt-master/nmt/nmt.py:383:      subword_option=flags.subword_option,
nmt-master/nmt/nmt.py:384:      check_special_token=flags.check_special_token,
nmt-master/nmt/nmt.py:385:      use_char_encode=flags.use_char_encode,
nmt-master/nmt/nmt.py:387:      # Misc
nmt-master/nmt/nmt.py:388:      forget_bias=flags.forget_bias,
nmt-master/nmt/nmt.py:389:      num_gpus=flags.num_gpus,
nmt-master/nmt/nmt.py:390:      epoch_step=0,  # record where we were within an epoch.
nmt-master/nmt/nmt.py:391:      steps_per_stats=flags.steps_per_stats,
nmt-master/nmt/nmt.py:392:      steps_per_external_eval=flags.steps_per_external_eval,
nmt-master/nmt/nmt.py:393:      share_vocab=flags.share_vocab,
nmt-master/nmt/nmt.py:394:      metrics=flags.metrics.split(","),
nmt-master/nmt/nmt.py:395:      log_device_placement=flags.log_device_placement,
nmt-master/nmt/nmt.py:396:      random_seed=flags.random_seed,
nmt-master/nmt/nmt.py:397:      override_loaded_hparams=flags.override_loaded_hparams,
nmt-master/nmt/nmt.py:398:      num_keep_ckpts=flags.num_keep_ckpts,
nmt-master/nmt/nmt.py:399:      avg_ckpts=flags.avg_ckpts,
nmt-master/nmt/nmt.py:400:      language_model=flags.language_model,
nmt-master/nmt/nmt.py:401:      num_intra_threads=flags.num_intra_threads,
nmt-master/nmt/nmt.py:402:      num_inter_threads=flags.num_inter_threads,
nmt-master/nmt/nmt.py:403:  )
nmt-master/nmt/nmt.py:406:def _add_argument(hparams, key, value, update=True):
nmt-master/nmt/nmt.py:407:  """Add an argument to hparams; if exists, change the value if update==True."""
nmt-master/nmt/nmt.py:408:  if hasattr(hparams, key):
nmt-master/nmt/nmt.py:409:    if update:
nmt-master/nmt/nmt.py:410:      setattr(hparams, key, value)
nmt-master/nmt/nmt.py:411:  else:
nmt-master/nmt/nmt.py:412:    hparams.add_hparam(key, value)
nmt-master/nmt/nmt.py:415:def extend_hparams(hparams):
nmt-master/nmt/nmt.py:416:  """Add new arguments to hparams."""
nmt-master/nmt/nmt.py:417:  # Sanity checks
nmt-master/nmt/nmt.py:418:  if hparams.encoder_type == "bi" and hparams.num_encoder_layers % 2 != 0:
nmt-master/nmt/nmt.py:419:    raise ValueError("For bi, num_encoder_layers %d should be even" %
nmt-master/nmt/nmt.py:420:                     hparams.num_encoder_layers)
nmt-master/nmt/nmt.py:421:  if (hparams.attention_architecture in ["gnmt"] and
nmt-master/nmt/nmt.py:422:      hparams.num_encoder_layers < 2):
nmt-master/nmt/nmt.py:423:    raise ValueError("For gnmt attention architecture, "
nmt-master/nmt/nmt.py:424:                     "num_encoder_layers %d should be >= 2" %
nmt-master/nmt/nmt.py:425:                     hparams.num_encoder_layers)
nmt-master/nmt/nmt.py:426:  if hparams.subword_option and hparams.subword_option not in ["spm", "bpe"]:
nmt-master/nmt/nmt.py:427:    raise ValueError("subword option must be either spm, or bpe")
nmt-master/nmt/nmt.py:428:  if hparams.infer_mode == "beam_search" and hparams.beam_width <= 0:
nmt-master/nmt/nmt.py:429:    raise ValueError("beam_width must greater than 0 when using beam_search"
nmt-master/nmt/nmt.py:430:                     "decoder.")
nmt-master/nmt/nmt.py:431:  if hparams.infer_mode == "sample" and hparams.sampling_temperature <= 0.0:
nmt-master/nmt/nmt.py:432:    raise ValueError("sampling_temperature must greater than 0.0 when using"
nmt-master/nmt/nmt.py:433:                     "sample decoder.")
nmt-master/nmt/nmt.py:435:  # Different number of encoder / decoder layers
nmt-master/nmt/nmt.py:436:  assert hparams.num_encoder_layers and hparams.num_decoder_layers
nmt-master/nmt/nmt.py:437:  if hparams.num_encoder_layers != hparams.num_decoder_layers:
nmt-master/nmt/nmt.py:438:    hparams.pass_hidden_state = False
nmt-master/nmt/nmt.py:439:    utils.print_out("Num encoder layer %d is different from num decoder layer"
nmt-master/nmt/nmt.py:440:                    " %d, so set pass_hidden_state to False" % (
nmt-master/nmt/nmt.py:441:                        hparams.num_encoder_layers,
nmt-master/nmt/nmt.py:442:                        hparams.num_decoder_layers))
nmt-master/nmt/nmt.py:444:  # Set residual layers
nmt-master/nmt/nmt.py:445:  num_encoder_residual_layers = 0
nmt-master/nmt/nmt.py:446:  num_decoder_residual_layers = 0
nmt-master/nmt/nmt.py:447:  if hparams.residual:
nmt-master/nmt/nmt.py:448:    if hparams.num_encoder_layers > 1:
nmt-master/nmt/nmt.py:449:      num_encoder_residual_layers = hparams.num_encoder_layers - 1
nmt-master/nmt/nmt.py:450:    if hparams.num_decoder_layers > 1:
nmt-master/nmt/nmt.py:451:      num_decoder_residual_layers = hparams.num_decoder_layers - 1
nmt-master/nmt/nmt.py:453:    if hparams.encoder_type == "gnmt":
nmt-master/nmt/nmt.py:454:      # The first unidirectional layer (after the bi-directional layer) in
nmt-master/nmt/nmt.py:455:      # the GNMT encoder can't have residual connection due to the input is
nmt-master/nmt/nmt.py:456:      # the concatenation of fw_cell and bw_cell's outputs.
nmt-master/nmt/nmt.py:457:      num_encoder_residual_layers = hparams.num_encoder_layers - 2
nmt-master/nmt/nmt.py:459:      # Compatible for GNMT models
nmt-master/nmt/nmt.py:460:      if hparams.num_encoder_layers == hparams.num_decoder_layers:
nmt-master/nmt/nmt.py:461:        num_decoder_residual_layers = num_encoder_residual_layers
nmt-master/nmt/nmt.py:462:  _add_argument(hparams, "num_encoder_residual_layers",
nmt-master/nmt/nmt.py:463:                num_encoder_residual_layers)
nmt-master/nmt/nmt.py:464:  _add_argument(hparams, "num_decoder_residual_layers",
nmt-master/nmt/nmt.py:465:                num_decoder_residual_layers)
nmt-master/nmt/nmt.py:467:  # Language modeling
nmt-master/nmt/nmt.py:468:  if getattr(hparams, "language_model", None):
nmt-master/nmt/nmt.py:469:    hparams.attention = ""
nmt-master/nmt/nmt.py:470:    hparams.attention_architecture = ""
nmt-master/nmt/nmt.py:471:    hparams.pass_hidden_state = False
nmt-master/nmt/nmt.py:472:    hparams.share_vocab = True
nmt-master/nmt/nmt.py:473:    hparams.src = hparams.tgt
nmt-master/nmt/nmt.py:474:    utils.print_out("For language modeling, we turn off attention and "
nmt-master/nmt/nmt.py:475:                    "pass_hidden_state; turn on share_vocab; set src to tgt.")
nmt-master/nmt/nmt.py:477:  ## Vocab
nmt-master/nmt/nmt.py:478:  # Get vocab file names first
nmt-master/nmt/nmt.py:479:  if hparams.vocab_prefix:
nmt-master/nmt/nmt.py:480:    src_vocab_file = hparams.vocab_prefix + "." + hparams.src
nmt-master/nmt/nmt.py:481:    tgt_vocab_file = hparams.vocab_prefix + "." + hparams.tgt
nmt-master/nmt/nmt.py:482:  else:
nmt-master/nmt/nmt.py:483:    raise ValueError("hparams.vocab_prefix must be provided.")
nmt-master/nmt/nmt.py:485:  # Source vocab
nmt-master/nmt/nmt.py:486:  check_special_token = getattr(hparams, "check_special_token", True)
nmt-master/nmt/nmt.py:487:  src_vocab_size, src_vocab_file = vocab_utils.check_vocab(
nmt-master/nmt/nmt.py:488:      src_vocab_file,
nmt-master/nmt/nmt.py:489:      hparams.out_dir,
nmt-master/nmt/nmt.py:490:      check_special_token=check_special_token,
nmt-master/nmt/nmt.py:491:      sos=hparams.sos,
nmt-master/nmt/nmt.py:492:      eos=hparams.eos,
nmt-master/nmt/nmt.py:493:      unk=vocab_utils.UNK)
nmt-master/nmt/nmt.py:495:  # Target vocab
nmt-master/nmt/nmt.py:496:  if hparams.share_vocab:
nmt-master/nmt/nmt.py:497:    utils.print_out("  using source vocab for target")
nmt-master/nmt/nmt.py:498:    tgt_vocab_file = src_vocab_file
nmt-master/nmt/nmt.py:499:    tgt_vocab_size = src_vocab_size
nmt-master/nmt/nmt.py:500:  else:
nmt-master/nmt/nmt.py:501:    tgt_vocab_size, tgt_vocab_file = vocab_utils.check_vocab(
nmt-master/nmt/nmt.py:502:        tgt_vocab_file,
nmt-master/nmt/nmt.py:503:        hparams.out_dir,
nmt-master/nmt/nmt.py:504:        check_special_token=check_special_token,
nmt-master/nmt/nmt.py:505:        sos=hparams.sos,
nmt-master/nmt/nmt.py:506:        eos=hparams.eos,
nmt-master/nmt/nmt.py:507:        unk=vocab_utils.UNK)
nmt-master/nmt/nmt.py:508:  _add_argument(hparams, "src_vocab_size", src_vocab_size)
nmt-master/nmt/nmt.py:509:  _add_argument(hparams, "tgt_vocab_size", tgt_vocab_size)
nmt-master/nmt/nmt.py:510:  _add_argument(hparams, "src_vocab_file", src_vocab_file)
nmt-master/nmt/nmt.py:511:  _add_argument(hparams, "tgt_vocab_file", tgt_vocab_file)
nmt-master/nmt/nmt.py:513:  # Num embedding partitions
nmt-master/nmt/nmt.py:514:  num_embeddings_partitions = getattr(hparams, "num_embeddings_partitions", 0)
nmt-master/nmt/nmt.py:515:  _add_argument(hparams, "num_enc_emb_partitions", num_embeddings_partitions)
nmt-master/nmt/nmt.py:516:  _add_argument(hparams, "num_dec_emb_partitions", num_embeddings_partitions)
nmt-master/nmt/nmt.py:518:  # Pretrained Embeddings
nmt-master/nmt/nmt.py:519:  _add_argument(hparams, "src_embed_file", "")
nmt-master/nmt/nmt.py:520:  _add_argument(hparams, "tgt_embed_file", "")
nmt-master/nmt/nmt.py:521:  if getattr(hparams, "embed_prefix", None):
nmt-master/nmt/nmt.py:522:    src_embed_file = hparams.embed_prefix + "." + hparams.src
nmt-master/nmt/nmt.py:523:    tgt_embed_file = hparams.embed_prefix + "." + hparams.tgt
nmt-master/nmt/nmt.py:525:    if tf.gfile.Exists(src_embed_file):
nmt-master/nmt/nmt.py:526:      utils.print_out("  src_embed_file %s exist" % src_embed_file)
nmt-master/nmt/nmt.py:527:      hparams.src_embed_file = src_embed_file
nmt-master/nmt/nmt.py:529:      utils.print_out(
nmt-master/nmt/nmt.py:530:          "For pretrained embeddings, set num_enc_emb_partitions to 1")
nmt-master/nmt/nmt.py:531:      hparams.num_enc_emb_partitions = 1
nmt-master/nmt/nmt.py:532:    else:
nmt-master/nmt/nmt.py:533:      utils.print_out("  src_embed_file %s doesn't exist" % src_embed_file)
nmt-master/nmt/nmt.py:535:    if tf.gfile.Exists(tgt_embed_file):
nmt-master/nmt/nmt.py:536:      utils.print_out("  tgt_embed_file %s exist" % tgt_embed_file)
nmt-master/nmt/nmt.py:537:      hparams.tgt_embed_file = tgt_embed_file
nmt-master/nmt/nmt.py:539:      utils.print_out(
nmt-master/nmt/nmt.py:540:          "For pretrained embeddings, set num_dec_emb_partitions to 1")
nmt-master/nmt/nmt.py:541:      hparams.num_dec_emb_partitions = 1
nmt-master/nmt/nmt.py:542:    else:
nmt-master/nmt/nmt.py:543:      utils.print_out("  tgt_embed_file %s doesn't exist" % tgt_embed_file)
nmt-master/nmt/nmt.py:545:  # Evaluation
nmt-master/nmt/nmt.py:546:  for metric in hparams.metrics:
nmt-master/nmt/nmt.py:547:    best_metric_dir = os.path.join(hparams.out_dir, "best_" + metric)
nmt-master/nmt/nmt.py:548:    tf.gfile.MakeDirs(best_metric_dir)
nmt-master/nmt/nmt.py:549:    _add_argument(hparams, "best_" + metric, 0, update=False)
nmt-master/nmt/nmt.py:550:    _add_argument(hparams, "best_" + metric + "_dir", best_metric_dir)
nmt-master/nmt/nmt.py:552:    if getattr(hparams, "avg_ckpts", None):
nmt-master/nmt/nmt.py:553:      best_metric_dir = os.path.join(hparams.out_dir, "avg_best_" + metric)
nmt-master/nmt/nmt.py:554:      tf.gfile.MakeDirs(best_metric_dir)
nmt-master/nmt/nmt.py:555:      _add_argument(hparams, "avg_best_" + metric, 0, update=False)
nmt-master/nmt/nmt.py:556:      _add_argument(hparams, "avg_best_" + metric + "_dir", best_metric_dir)
nmt-master/nmt/nmt.py:558:  return hparams
nmt-master/nmt/nmt.py:561:def ensure_compatible_hparams(hparams, default_hparams, hparams_path=""):
nmt-master/nmt/nmt.py:562:  """Make sure the loaded hparams is compatible with new changes."""
nmt-master/nmt/nmt.py:563:  default_hparams = utils.maybe_parse_standard_hparams(
nmt-master/nmt/nmt.py:564:      default_hparams, hparams_path)
nmt-master/nmt/nmt.py:566:  # Set num encoder/decoder layers (for old checkpoints)
nmt-master/nmt/nmt.py:567:  if hasattr(hparams, "num_layers"):
nmt-master/nmt/nmt.py:568:    if not hasattr(hparams, "num_encoder_layers"):
nmt-master/nmt/nmt.py:569:      hparams.add_hparam("num_encoder_layers", hparams.num_layers)
nmt-master/nmt/nmt.py:570:    if not hasattr(hparams, "num_decoder_layers"):
nmt-master/nmt/nmt.py:571:      hparams.add_hparam("num_decoder_layers", hparams.num_layers)
nmt-master/nmt/nmt.py:573:  # For compatible reason, if there are new fields in default_hparams,
nmt-master/nmt/nmt.py:574:  #   we add them to the current hparams
nmt-master/nmt/nmt.py:575:  default_config = default_hparams.values()
nmt-master/nmt/nmt.py:576:  config = hparams.values()
nmt-master/nmt/nmt.py:577:  for key in default_config:
nmt-master/nmt/nmt.py:578:    if key not in config:
nmt-master/nmt/nmt.py:579:      hparams.add_hparam(key, default_config[key])
nmt-master/nmt/nmt.py:581:  # Update all hparams' keys if override_loaded_hparams=True
nmt-master/nmt/nmt.py:582:  if getattr(default_hparams, "override_loaded_hparams", None):
nmt-master/nmt/nmt.py:583:    overwritten_keys = default_config.keys()
nmt-master/nmt/nmt.py:584:  else:
nmt-master/nmt/nmt.py:585:    # For inference
nmt-master/nmt/nmt.py:586:    overwritten_keys = INFERENCE_KEYS
nmt-master/nmt/nmt.py:588:  for key in overwritten_keys:
nmt-master/nmt/nmt.py:589:    if getattr(hparams, key) != default_config[key]:
nmt-master/nmt/nmt.py:590:      utils.print_out("# Updating hparams.%s: %s -> %s" %
nmt-master/nmt/nmt.py:591:                      (key, str(getattr(hparams, key)),
nmt-master/nmt/nmt.py:592:                       str(default_config[key])))
nmt-master/nmt/nmt.py:593:      setattr(hparams, key, default_config[key])
nmt-master/nmt/nmt.py:594:  return hparams
nmt-master/nmt/nmt.py:597:def create_or_load_hparams(
nmt-master/nmt/nmt.py:598:    out_dir, default_hparams, hparams_path, save_hparams=True):
nmt-master/nmt/nmt.py:599:  """Create hparams or load hparams from out_dir."""
nmt-master/nmt/nmt.py:600:  hparams = utils.load_hparams(out_dir)
nmt-master/nmt/nmt.py:601:  if not hparams:
nmt-master/nmt/nmt.py:602:    hparams = default_hparams
nmt-master/nmt/nmt.py:603:    hparams = utils.maybe_parse_standard_hparams(
nmt-master/nmt/nmt.py:604:        hparams, hparams_path)
nmt-master/nmt/nmt.py:605:  else:
nmt-master/nmt/nmt.py:606:    hparams = ensure_compatible_hparams(hparams, default_hparams, hparams_path)
nmt-master/nmt/nmt.py:607:  hparams = extend_hparams(hparams)
nmt-master/nmt/nmt.py:609:  # Save HParams
nmt-master/nmt/nmt.py:610:  if save_hparams:
nmt-master/nmt/nmt.py:611:    utils.save_hparams(out_dir, hparams)
nmt-master/nmt/nmt.py:612:    for metric in hparams.metrics:
nmt-master/nmt/nmt.py:613:      utils.save_hparams(getattr(hparams, "best_" + metric + "_dir"), hparams)
nmt-master/nmt/nmt.py:615:  # Print HParams
nmt-master/nmt/nmt.py:616:  utils.print_hparams(hparams)
nmt-master/nmt/nmt.py:617:  return hparams
nmt-master/nmt/nmt.py:620:def run_main(flags, default_hparams, train_fn, inference_fn, target_session=""):
nmt-master/nmt/nmt.py:621:  """Run main."""
nmt-master/nmt/nmt.py:622:  # Job
nmt-master/nmt/nmt.py:623:  jobid = flags.jobid
nmt-master/nmt/nmt.py:624:  num_workers = flags.num_workers
nmt-master/nmt/nmt.py:625:  utils.print_out("# Job id %d" % jobid)
nmt-master/nmt/nmt.py:627:  # GPU device
nmt-master/nmt/nmt.py:628:  utils.print_out(
nmt-master/nmt/nmt.py:629:      "# Devices visible to TensorFlow: %s" % repr(tf.Session().list_devices()))
nmt-master/nmt/nmt.py:631:  # Random
nmt-master/nmt/nmt.py:632:  random_seed = flags.random_seed
nmt-master/nmt/nmt.py:633:  if random_seed is not None and random_seed > 0:
nmt-master/nmt/nmt.py:634:    utils.print_out("# Set random seed to %d" % random_seed)
nmt-master/nmt/nmt.py:635:    random.seed(random_seed + jobid)
nmt-master/nmt/nmt.py:636:    np.random.seed(random_seed + jobid)
nmt-master/nmt/nmt.py:638:  # Model output directory
nmt-master/nmt/nmt.py:639:  out_dir = flags.out_dir
nmt-master/nmt/nmt.py:640:  if out_dir and not tf.gfile.Exists(out_dir):
nmt-master/nmt/nmt.py:641:    utils.print_out("# Creating output directory %s ..." % out_dir)
nmt-master/nmt/nmt.py:642:    tf.gfile.MakeDirs(out_dir)
nmt-master/nmt/nmt.py:644:  # Load hparams.
nmt-master/nmt/nmt.py:645:  loaded_hparams = False
nmt-master/nmt/nmt.py:646:  if flags.ckpt:  # Try to load hparams from the same directory as ckpt
nmt-master/nmt/nmt.py:647:    ckpt_dir = os.path.dirname(flags.ckpt)
nmt-master/nmt/nmt.py:648:    ckpt_hparams_file = os.path.join(ckpt_dir, "hparams")
nmt-master/nmt/nmt.py:649:    if tf.gfile.Exists(ckpt_hparams_file) or flags.hparams_path:
nmt-master/nmt/nmt.py:650:      hparams = create_or_load_hparams(
nmt-master/nmt/nmt.py:651:          ckpt_dir, default_hparams, flags.hparams_path,
nmt-master/nmt/nmt.py:652:          save_hparams=False)
nmt-master/nmt/nmt.py:653:      loaded_hparams = True
nmt-master/nmt/nmt.py:654:  if not loaded_hparams:  # Try to load from out_dir
nmt-master/nmt/nmt.py:655:    assert out_dir
nmt-master/nmt/nmt.py:656:    hparams = create_or_load_hparams(
nmt-master/nmt/nmt.py:657:        out_dir, default_hparams, flags.hparams_path,
nmt-master/nmt/nmt.py:658:        save_hparams=(jobid == 0))
nmt-master/nmt/nmt.py:660:  ## Train / Decode
nmt-master/nmt/nmt.py:661:  if flags.inference_input_file:
nmt-master/nmt/nmt.py:662:    # Inference output directory
nmt-master/nmt/nmt.py:663:    trans_file = flags.inference_output_file
nmt-master/nmt/nmt.py:664:    assert trans_file
nmt-master/nmt/nmt.py:665:    trans_dir = os.path.dirname(trans_file)
nmt-master/nmt/nmt.py:666:    if not tf.gfile.Exists(trans_dir): tf.gfile.MakeDirs(trans_dir)
nmt-master/nmt/nmt.py:668:    # Inference indices
nmt-master/nmt/nmt.py:669:    hparams.inference_indices = None
nmt-master/nmt/nmt.py:670:    if flags.inference_list:
nmt-master/nmt/nmt.py:671:      (hparams.inference_indices) = (
nmt-master/nmt/nmt.py:672:          [int(token)  for token in flags.inference_list.split(",")])
nmt-master/nmt/nmt.py:674:    # Inference
nmt-master/nmt/nmt.py:675:    ckpt = flags.ckpt
nmt-master/nmt/nmt.py:676:    if not ckpt:
nmt-master/nmt/nmt.py:677:      ckpt = tf.train.latest_checkpoint(out_dir)
nmt-master/nmt/nmt.py:678:    inference_fn(ckpt, flags.inference_input_file,
nmt-master/nmt/nmt.py:679:                 trans_file, hparams, num_workers, jobid)
nmt-master/nmt/nmt.py:681:    # Evaluation
nmt-master/nmt/nmt.py:682:    ref_file = flags.inference_ref_file
nmt-master/nmt/nmt.py:683:    if ref_file and tf.gfile.Exists(trans_file):
nmt-master/nmt/nmt.py:684:      for metric in hparams.metrics:
nmt-master/nmt/nmt.py:685:        score = evaluation_utils.evaluate(
nmt-master/nmt/nmt.py:686:            ref_file,
nmt-master/nmt/nmt.py:687:            trans_file,
nmt-master/nmt/nmt.py:688:            metric,
nmt-master/nmt/nmt.py:689:            hparams.subword_option)
nmt-master/nmt/nmt.py:690:        utils.print_out("  %s: %.1f" % (metric, score))
nmt-master/nmt/nmt.py:691:  else:
nmt-master/nmt/nmt.py:692:    # Train
nmt-master/nmt/nmt.py:693:    train_fn(hparams, target_session=target_session)
nmt-master/nmt/nmt.py:696:def main(unused_argv):
nmt-master/nmt/nmt.py:697:  default_hparams = create_hparams(FLAGS)
nmt-master/nmt/nmt.py:698:  train_fn = train.train
nmt-master/nmt/nmt.py:699:  inference_fn = inference.inference
nmt-master/nmt/nmt.py:700:  run_main(FLAGS, default_hparams, train_fn, inference_fn)
nmt-master/nmt/nmt.py:703:if __name__ == "__main__":
nmt-master/nmt/nmt.py:704:  nmt_parser = argparse.ArgumentParser()
nmt-master/nmt/nmt.py:705:  add_arguments(nmt_parser)
nmt-master/nmt/nmt.py:706:  FLAGS, unparsed = nmt_parser.parse_known_args()
nmt-master/nmt/nmt.py:707:  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
nmt-master/nmt/nmt_test.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/nmt_test.py:2:#
nmt-master/nmt/nmt_test.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/nmt_test.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/nmt_test.py:5:# You may obtain a copy of the License at
nmt-master/nmt/nmt_test.py:6:#
nmt-master/nmt/nmt_test.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/nmt_test.py:8:#
nmt-master/nmt/nmt_test.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/nmt_test.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/nmt_test.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/nmt_test.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/nmt_test.py:13:# limitations under the License.
nmt-master/nmt/nmt_test.py:14:# ==============================================================================
nmt-master/nmt/nmt_test.py:15:"""Tests for nmt.py, train.py and inference.py."""
nmt-master/nmt/nmt_test.py:17:from __future__ import absolute_import
nmt-master/nmt/nmt_test.py:18:from __future__ import division
nmt-master/nmt/nmt_test.py:19:from __future__ import print_function
nmt-master/nmt/nmt_test.py:21:import argparse
nmt-master/nmt/nmt_test.py:22:import os
nmt-master/nmt/nmt_test.py:24:import tensorflow as tf
nmt-master/nmt/nmt_test.py:26:from . import inference
nmt-master/nmt/nmt_test.py:27:from . import nmt
nmt-master/nmt/nmt_test.py:28:from . import train
nmt-master/nmt/nmt_test.py:31:def _update_flags(flags, test_name):
nmt-master/nmt/nmt_test.py:32:  """Update flags for basic training."""
nmt-master/nmt/nmt_test.py:33:  flags.num_train_steps = 100
nmt-master/nmt/nmt_test.py:34:  flags.steps_per_stats = 5
nmt-master/nmt/nmt_test.py:35:  flags.src = "en"
nmt-master/nmt/nmt_test.py:36:  flags.tgt = "vi"
nmt-master/nmt/nmt_test.py:37:  flags.train_prefix = ("nmt/testdata/"
nmt-master/nmt/nmt_test.py:38:                        "iwslt15.tst2013.100")
nmt-master/nmt/nmt_test.py:39:  flags.vocab_prefix = ("nmt/testdata/"
nmt-master/nmt/nmt_test.py:40:                        "iwslt15.vocab.100")
nmt-master/nmt/nmt_test.py:41:  flags.dev_prefix = ("nmt/testdata/"
nmt-master/nmt/nmt_test.py:42:                      "iwslt15.tst2013.100")
nmt-master/nmt/nmt_test.py:43:  flags.test_prefix = ("nmt/testdata/"
nmt-master/nmt/nmt_test.py:44:                       "iwslt15.tst2013.100")
nmt-master/nmt/nmt_test.py:45:  flags.out_dir = os.path.join(tf.test.get_temp_dir(), test_name)
nmt-master/nmt/nmt_test.py:48:class NMTTest(tf.test.TestCase):
nmt-master/nmt/nmt_test.py:50:  def testTrain(self):
nmt-master/nmt/nmt_test.py:51:    """Test the training loop is functional with basic hparams."""
nmt-master/nmt/nmt_test.py:52:    nmt_parser = argparse.ArgumentParser()
nmt-master/nmt/nmt_test.py:53:    nmt.add_arguments(nmt_parser)
nmt-master/nmt/nmt_test.py:54:    FLAGS, unparsed = nmt_parser.parse_known_args()
nmt-master/nmt/nmt_test.py:56:    _update_flags(FLAGS, "nmt_train_test")
nmt-master/nmt/nmt_test.py:58:    default_hparams = nmt.create_hparams(FLAGS)
nmt-master/nmt/nmt_test.py:60:    train_fn = train.train
nmt-master/nmt/nmt_test.py:61:    nmt.run_main(FLAGS, default_hparams, train_fn, None)
nmt-master/nmt/nmt_test.py:64:  def testTrainWithAvgCkpts(self):
nmt-master/nmt/nmt_test.py:65:    """Test the training loop is functional with basic hparams."""
nmt-master/nmt/nmt_test.py:66:    nmt_parser = argparse.ArgumentParser()
nmt-master/nmt/nmt_test.py:67:    nmt.add_arguments(nmt_parser)
nmt-master/nmt/nmt_test.py:68:    FLAGS, unparsed = nmt_parser.parse_known_args()
nmt-master/nmt/nmt_test.py:70:    _update_flags(FLAGS, "nmt_train_test_avg_ckpts")
nmt-master/nmt/nmt_test.py:71:    FLAGS.avg_ckpts = True
nmt-master/nmt/nmt_test.py:73:    default_hparams = nmt.create_hparams(FLAGS)
nmt-master/nmt/nmt_test.py:75:    train_fn = train.train
nmt-master/nmt/nmt_test.py:76:    nmt.run_main(FLAGS, default_hparams, train_fn, None)
nmt-master/nmt/nmt_test.py:79:  def testInference(self):
nmt-master/nmt/nmt_test.py:80:    """Test inference is function with basic hparams."""
nmt-master/nmt/nmt_test.py:81:    nmt_parser = argparse.ArgumentParser()
nmt-master/nmt/nmt_test.py:82:    nmt.add_arguments(nmt_parser)
nmt-master/nmt/nmt_test.py:83:    FLAGS, unparsed = nmt_parser.parse_known_args()
nmt-master/nmt/nmt_test.py:85:    _update_flags(FLAGS, "nmt_train_infer")
nmt-master/nmt/nmt_test.py:87:    # Train one step so we have a checkpoint.
nmt-master/nmt/nmt_test.py:88:    FLAGS.num_train_steps = 1
nmt-master/nmt/nmt_test.py:89:    default_hparams = nmt.create_hparams(FLAGS)
nmt-master/nmt/nmt_test.py:90:    train_fn = train.train
nmt-master/nmt/nmt_test.py:91:    nmt.run_main(FLAGS, default_hparams, train_fn, None)
nmt-master/nmt/nmt_test.py:93:    # Update FLAGS for inference.
nmt-master/nmt/nmt_test.py:94:    FLAGS.inference_input_file = ("nmt/testdata/"
nmt-master/nmt/nmt_test.py:95:                                  "iwslt15.tst2013.100.en")
nmt-master/nmt/nmt_test.py:96:    FLAGS.inference_output_file = os.path.join(FLAGS.out_dir, "output")
nmt-master/nmt/nmt_test.py:97:    FLAGS.inference_ref_file = ("nmt/testdata/"
nmt-master/nmt/nmt_test.py:98:                                "iwslt15.tst2013.100.vi")
nmt-master/nmt/nmt_test.py:100:    default_hparams = nmt.create_hparams(FLAGS)
nmt-master/nmt/nmt_test.py:102:    inference_fn = inference.inference
nmt-master/nmt/nmt_test.py:103:    nmt.run_main(FLAGS, default_hparams, None, inference_fn)
nmt-master/nmt/nmt_test.py:106:if __name__ == "__main__":
nmt-master/nmt/nmt_test.py:107:  tf.test.main()
nmt-master/nmt/scripts/bleu.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/scripts/bleu.py:2:#
nmt-master/nmt/scripts/bleu.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/scripts/bleu.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/scripts/bleu.py:5:# You may obtain a copy of the License at
nmt-master/nmt/scripts/bleu.py:6:#
nmt-master/nmt/scripts/bleu.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/scripts/bleu.py:8:#
nmt-master/nmt/scripts/bleu.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/scripts/bleu.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/scripts/bleu.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/scripts/bleu.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/scripts/bleu.py:13:# limitations under the License.
nmt-master/nmt/scripts/bleu.py:14:# ==============================================================================
nmt-master/nmt/scripts/bleu.py:16:"""Python implementation of BLEU and smooth-BLEU.
nmt-master/nmt/scripts/bleu.py:18:This module provides a Python implementation of BLEU and smooth-BLEU.
nmt-master/nmt/scripts/bleu.py:19:Smooth BLEU is computed following the method outlined in the paper:
nmt-master/nmt/scripts/bleu.py:20:Chin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic
nmt-master/nmt/scripts/bleu.py:21:evaluation metrics for machine translation. COLING 2004.
nmt-master/nmt/scripts/bleu.py:22:"""
nmt-master/nmt/scripts/bleu.py:24:import collections
nmt-master/nmt/scripts/bleu.py:25:import math
nmt-master/nmt/scripts/bleu.py:28:def _get_ngrams(segment, max_order):
nmt-master/nmt/scripts/bleu.py:29:  """Extracts all n-grams upto a given maximum order from an input segment.
nmt-master/nmt/scripts/bleu.py:31:  Args:
nmt-master/nmt/scripts/bleu.py:32:    segment: text segment from which n-grams will be extracted.
nmt-master/nmt/scripts/bleu.py:33:    max_order: maximum length in tokens of the n-grams returned by this
nmt-master/nmt/scripts/bleu.py:34:        methods.
nmt-master/nmt/scripts/bleu.py:36:  Returns:
nmt-master/nmt/scripts/bleu.py:37:    The Counter containing all n-grams upto max_order in segment
nmt-master/nmt/scripts/bleu.py:38:    with a count of how many times each n-gram occurred.
nmt-master/nmt/scripts/bleu.py:39:  """
nmt-master/nmt/scripts/bleu.py:40:  ngram_counts = collections.Counter()
nmt-master/nmt/scripts/bleu.py:41:  for order in range(1, max_order + 1):
nmt-master/nmt/scripts/bleu.py:42:    for i in range(0, len(segment) - order + 1):
nmt-master/nmt/scripts/bleu.py:43:      ngram = tuple(segment[i:i+order])
nmt-master/nmt/scripts/bleu.py:44:      ngram_counts[ngram] += 1
nmt-master/nmt/scripts/bleu.py:45:  return ngram_counts
nmt-master/nmt/scripts/bleu.py:48:def compute_bleu(reference_corpus, translation_corpus, max_order=4,
nmt-master/nmt/scripts/bleu.py:49:                 smooth=False):
nmt-master/nmt/scripts/bleu.py:50:  """Computes BLEU score of translated segments against one or more references.
nmt-master/nmt/scripts/bleu.py:52:  Args:
nmt-master/nmt/scripts/bleu.py:53:    reference_corpus: list of lists of references for each translation. Each
nmt-master/nmt/scripts/bleu.py:54:        reference should be tokenized into a list of tokens.
nmt-master/nmt/scripts/bleu.py:55:    translation_corpus: list of translations to score. Each translation
nmt-master/nmt/scripts/bleu.py:56:        should be tokenized into a list of tokens.
nmt-master/nmt/scripts/bleu.py:57:    max_order: Maximum n-gram order to use when computing BLEU score.
nmt-master/nmt/scripts/bleu.py:58:    smooth: Whether or not to apply Lin et al. 2004 smoothing.
nmt-master/nmt/scripts/bleu.py:60:  Returns:
nmt-master/nmt/scripts/bleu.py:61:    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram
nmt-master/nmt/scripts/bleu.py:62:    precisions and brevity penalty.
nmt-master/nmt/scripts/bleu.py:63:  """
nmt-master/nmt/scripts/bleu.py:64:  matches_by_order = [0] * max_order
nmt-master/nmt/scripts/bleu.py:65:  possible_matches_by_order = [0] * max_order
nmt-master/nmt/scripts/bleu.py:66:  reference_length = 0
nmt-master/nmt/scripts/bleu.py:67:  translation_length = 0
nmt-master/nmt/scripts/bleu.py:68:  for (references, translation) in zip(reference_corpus,
nmt-master/nmt/scripts/bleu.py:69:                                       translation_corpus):
nmt-master/nmt/scripts/bleu.py:70:    reference_length += min(len(r) for r in references)
nmt-master/nmt/scripts/bleu.py:71:    translation_length += len(translation)
nmt-master/nmt/scripts/bleu.py:73:    merged_ref_ngram_counts = collections.Counter()
nmt-master/nmt/scripts/bleu.py:74:    for reference in references:
nmt-master/nmt/scripts/bleu.py:75:      merged_ref_ngram_counts |= _get_ngrams(reference, max_order)
nmt-master/nmt/scripts/bleu.py:76:    translation_ngram_counts = _get_ngrams(translation, max_order)
nmt-master/nmt/scripts/bleu.py:77:    overlap = translation_ngram_counts & merged_ref_ngram_counts
nmt-master/nmt/scripts/bleu.py:78:    for ngram in overlap:
nmt-master/nmt/scripts/bleu.py:79:      matches_by_order[len(ngram)-1] += overlap[ngram]
nmt-master/nmt/scripts/bleu.py:80:    for order in range(1, max_order+1):
nmt-master/nmt/scripts/bleu.py:81:      possible_matches = len(translation) - order + 1
nmt-master/nmt/scripts/bleu.py:82:      if possible_matches > 0:
nmt-master/nmt/scripts/bleu.py:83:        possible_matches_by_order[order-1] += possible_matches
nmt-master/nmt/scripts/bleu.py:85:  precisions = [0] * max_order
nmt-master/nmt/scripts/bleu.py:86:  for i in range(0, max_order):
nmt-master/nmt/scripts/bleu.py:87:    if smooth:
nmt-master/nmt/scripts/bleu.py:88:      precisions[i] = ((matches_by_order[i] + 1.) /
nmt-master/nmt/scripts/bleu.py:89:                       (possible_matches_by_order[i] + 1.))
nmt-master/nmt/scripts/bleu.py:90:    else:
nmt-master/nmt/scripts/bleu.py:91:      if possible_matches_by_order[i] > 0:
nmt-master/nmt/scripts/bleu.py:92:        precisions[i] = (float(matches_by_order[i]) /
nmt-master/nmt/scripts/bleu.py:93:                         possible_matches_by_order[i])
nmt-master/nmt/scripts/bleu.py:94:      else:
nmt-master/nmt/scripts/bleu.py:95:        precisions[i] = 0.0
nmt-master/nmt/scripts/bleu.py:97:  if min(precisions) > 0:
nmt-master/nmt/scripts/bleu.py:98:    p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)
nmt-master/nmt/scripts/bleu.py:99:    geo_mean = math.exp(p_log_sum)
nmt-master/nmt/scripts/bleu.py:100:  else:
nmt-master/nmt/scripts/bleu.py:101:    geo_mean = 0
nmt-master/nmt/scripts/bleu.py:103:  ratio = float(translation_length) / reference_length
nmt-master/nmt/scripts/bleu.py:105:  if ratio > 1.0:
nmt-master/nmt/scripts/bleu.py:106:    bp = 1.
nmt-master/nmt/scripts/bleu.py:107:  else:
nmt-master/nmt/scripts/bleu.py:108:    bp = math.exp(1 - 1. / ratio)
nmt-master/nmt/scripts/bleu.py:110:  bleu = geo_mean * bp
nmt-master/nmt/scripts/bleu.py:112:  return (bleu, precisions, bp, ratio, translation_length, reference_length)
nmt-master/nmt/scripts/rouge.py:1:"""ROUGE metric implementation.
nmt-master/nmt/scripts/rouge.py:3:Copy from tf_seq2seq/seq2seq/metrics/rouge.py.
nmt-master/nmt/scripts/rouge.py:4:This is a modified and slightly extended verison of
nmt-master/nmt/scripts/rouge.py:5:https://github.com/miso-belica/sumy/blob/dev/sumy/evaluation/rouge.py.
nmt-master/nmt/scripts/rouge.py:6:"""
nmt-master/nmt/scripts/rouge.py:8:from __future__ import absolute_import
nmt-master/nmt/scripts/rouge.py:9:from __future__ import division
nmt-master/nmt/scripts/rouge.py:10:from __future__ import print_function
nmt-master/nmt/scripts/rouge.py:11:from __future__ import unicode_literals
nmt-master/nmt/scripts/rouge.py:13:import itertools
nmt-master/nmt/scripts/rouge.py:14:import numpy as np
nmt-master/nmt/scripts/rouge.py:16:#pylint: disable=C0103
nmt-master/nmt/scripts/rouge.py:19:def _get_ngrams(n, text):
nmt-master/nmt/scripts/rouge.py:20:  """Calcualtes n-grams.
nmt-master/nmt/scripts/rouge.py:22:  Args:
nmt-master/nmt/scripts/rouge.py:23:    n: which n-grams to calculate
nmt-master/nmt/scripts/rouge.py:24:    text: An array of tokens
nmt-master/nmt/scripts/rouge.py:26:  Returns:
nmt-master/nmt/scripts/rouge.py:27:    A set of n-grams
nmt-master/nmt/scripts/rouge.py:28:  """
nmt-master/nmt/scripts/rouge.py:29:  ngram_set = set()
nmt-master/nmt/scripts/rouge.py:30:  text_length = len(text)
nmt-master/nmt/scripts/rouge.py:31:  max_index_ngram_start = text_length - n
nmt-master/nmt/scripts/rouge.py:32:  for i in range(max_index_ngram_start + 1):
nmt-master/nmt/scripts/rouge.py:33:    ngram_set.add(tuple(text[i:i + n]))
nmt-master/nmt/scripts/rouge.py:34:  return ngram_set
nmt-master/nmt/scripts/rouge.py:37:def _split_into_words(sentences):
nmt-master/nmt/scripts/rouge.py:38:  """Splits multiple sentences into words and flattens the result"""
nmt-master/nmt/scripts/rouge.py:39:  return list(itertools.chain(*[_.split(" ") for _ in sentences]))
nmt-master/nmt/scripts/rouge.py:42:def _get_word_ngrams(n, sentences):
nmt-master/nmt/scripts/rouge.py:43:  """Calculates word n-grams for multiple sentences.
nmt-master/nmt/scripts/rouge.py:44:  """
nmt-master/nmt/scripts/rouge.py:45:  assert len(sentences) > 0
nmt-master/nmt/scripts/rouge.py:46:  assert n > 0
nmt-master/nmt/scripts/rouge.py:48:  words = _split_into_words(sentences)
nmt-master/nmt/scripts/rouge.py:49:  return _get_ngrams(n, words)
nmt-master/nmt/scripts/rouge.py:52:def _len_lcs(x, y):
nmt-master/nmt/scripts/rouge.py:53:  """
nmt-master/nmt/scripts/rouge.py:54:  Returns the length of the Longest Common Subsequence between sequences x
nmt-master/nmt/scripts/rouge.py:55:  and y.
nmt-master/nmt/scripts/rouge.py:56:  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence
nmt-master/nmt/scripts/rouge.py:58:  Args:
nmt-master/nmt/scripts/rouge.py:59:    x: sequence of words
nmt-master/nmt/scripts/rouge.py:60:    y: sequence of words
nmt-master/nmt/scripts/rouge.py:62:  Returns
nmt-master/nmt/scripts/rouge.py:63:    integer: Length of LCS between x and y
nmt-master/nmt/scripts/rouge.py:64:  """
nmt-master/nmt/scripts/rouge.py:65:  table = _lcs(x, y)
nmt-master/nmt/scripts/rouge.py:66:  n, m = len(x), len(y)
nmt-master/nmt/scripts/rouge.py:67:  return table[n, m]
nmt-master/nmt/scripts/rouge.py:70:def _lcs(x, y):
nmt-master/nmt/scripts/rouge.py:71:  """
nmt-master/nmt/scripts/rouge.py:72:  Computes the length of the longest common subsequence (lcs) between two
nmt-master/nmt/scripts/rouge.py:73:  strings. The implementation below uses a DP programming algorithm and runs
nmt-master/nmt/scripts/rouge.py:74:  in O(nm) time where n = len(x) and m = len(y).
nmt-master/nmt/scripts/rouge.py:75:  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence
nmt-master/nmt/scripts/rouge.py:77:  Args:
nmt-master/nmt/scripts/rouge.py:78:    x: collection of words
nmt-master/nmt/scripts/rouge.py:79:    y: collection of words
nmt-master/nmt/scripts/rouge.py:81:  Returns:
nmt-master/nmt/scripts/rouge.py:82:    Table of dictionary of coord and len lcs
nmt-master/nmt/scripts/rouge.py:83:  """
nmt-master/nmt/scripts/rouge.py:84:  n, m = len(x), len(y)
nmt-master/nmt/scripts/rouge.py:85:  table = dict()
nmt-master/nmt/scripts/rouge.py:86:  for i in range(n + 1):
nmt-master/nmt/scripts/rouge.py:87:    for j in range(m + 1):
nmt-master/nmt/scripts/rouge.py:88:      if i == 0 or j == 0:
nmt-master/nmt/scripts/rouge.py:89:        table[i, j] = 0
nmt-master/nmt/scripts/rouge.py:90:      elif x[i - 1] == y[j - 1]:
nmt-master/nmt/scripts/rouge.py:91:        table[i, j] = table[i - 1, j - 1] + 1
nmt-master/nmt/scripts/rouge.py:92:      else:
nmt-master/nmt/scripts/rouge.py:93:        table[i, j] = max(table[i - 1, j], table[i, j - 1])
nmt-master/nmt/scripts/rouge.py:94:  return table
nmt-master/nmt/scripts/rouge.py:97:def _recon_lcs(x, y):
nmt-master/nmt/scripts/rouge.py:98:  """
nmt-master/nmt/scripts/rouge.py:99:  Returns the Longest Subsequence between x and y.
nmt-master/nmt/scripts/rouge.py:100:  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence
nmt-master/nmt/scripts/rouge.py:102:  Args:
nmt-master/nmt/scripts/rouge.py:103:    x: sequence of words
nmt-master/nmt/scripts/rouge.py:104:    y: sequence of words
nmt-master/nmt/scripts/rouge.py:106:  Returns:
nmt-master/nmt/scripts/rouge.py:107:    sequence: LCS of x and y
nmt-master/nmt/scripts/rouge.py:108:  """
nmt-master/nmt/scripts/rouge.py:109:  i, j = len(x), len(y)
nmt-master/nmt/scripts/rouge.py:110:  table = _lcs(x, y)
nmt-master/nmt/scripts/rouge.py:112:  def _recon(i, j):
nmt-master/nmt/scripts/rouge.py:113:    """private recon calculation"""
nmt-master/nmt/scripts/rouge.py:114:    if i == 0 or j == 0:
nmt-master/nmt/scripts/rouge.py:115:      return []
nmt-master/nmt/scripts/rouge.py:116:    elif x[i - 1] == y[j - 1]:
nmt-master/nmt/scripts/rouge.py:117:      return _recon(i - 1, j - 1) + [(x[i - 1], i)]
nmt-master/nmt/scripts/rouge.py:118:    elif table[i - 1, j] > table[i, j - 1]:
nmt-master/nmt/scripts/rouge.py:119:      return _recon(i - 1, j)
nmt-master/nmt/scripts/rouge.py:120:    else:
nmt-master/nmt/scripts/rouge.py:121:      return _recon(i, j - 1)
nmt-master/nmt/scripts/rouge.py:123:  recon_tuple = tuple(map(lambda x: x[0], _recon(i, j)))
nmt-master/nmt/scripts/rouge.py:124:  return recon_tuple
nmt-master/nmt/scripts/rouge.py:127:def rouge_n(evaluated_sentences, reference_sentences, n=2):
nmt-master/nmt/scripts/rouge.py:128:  """
nmt-master/nmt/scripts/rouge.py:129:  Computes ROUGE-N of two text collections of sentences.
nmt-master/nmt/scripts/rouge.py:130:  Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/
nmt-master/nmt/scripts/rouge.py:131:  papers/rouge-working-note-v1.3.1.pdf
nmt-master/nmt/scripts/rouge.py:133:  Args:
nmt-master/nmt/scripts/rouge.py:134:    evaluated_sentences: The sentences that have been picked by the summarizer
nmt-master/nmt/scripts/rouge.py:135:    reference_sentences: The sentences from the referene set
nmt-master/nmt/scripts/rouge.py:136:    n: Size of ngram.  Defaults to 2.
nmt-master/nmt/scripts/rouge.py:138:  Returns:
nmt-master/nmt/scripts/rouge.py:139:    A tuple (f1, precision, recall) for ROUGE-N
nmt-master/nmt/scripts/rouge.py:141:  Raises:
nmt-master/nmt/scripts/rouge.py:142:    ValueError: raises exception if a param has len <= 0
nmt-master/nmt/scripts/rouge.py:143:  """
nmt-master/nmt/scripts/rouge.py:144:  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:
nmt-master/nmt/scripts/rouge.py:145:    raise ValueError("Collections must contain at least 1 sentence.")
nmt-master/nmt/scripts/rouge.py:147:  evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)
nmt-master/nmt/scripts/rouge.py:148:  reference_ngrams = _get_word_ngrams(n, reference_sentences)
nmt-master/nmt/scripts/rouge.py:149:  reference_count = len(reference_ngrams)
nmt-master/nmt/scripts/rouge.py:150:  evaluated_count = len(evaluated_ngrams)
nmt-master/nmt/scripts/rouge.py:152:  # Gets the overlapping ngrams between evaluated and reference
nmt-master/nmt/scripts/rouge.py:153:  overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)
nmt-master/nmt/scripts/rouge.py:154:  overlapping_count = len(overlapping_ngrams)
nmt-master/nmt/scripts/rouge.py:156:  # Handle edge case. This isn't mathematically correct, but it's good enough
nmt-master/nmt/scripts/rouge.py:157:  if evaluated_count == 0:
nmt-master/nmt/scripts/rouge.py:158:    precision = 0.0
nmt-master/nmt/scripts/rouge.py:159:  else:
nmt-master/nmt/scripts/rouge.py:160:    precision = overlapping_count / evaluated_count
nmt-master/nmt/scripts/rouge.py:162:  if reference_count == 0:
nmt-master/nmt/scripts/rouge.py:163:    recall = 0.0
nmt-master/nmt/scripts/rouge.py:164:  else:
nmt-master/nmt/scripts/rouge.py:165:    recall = overlapping_count / reference_count
nmt-master/nmt/scripts/rouge.py:167:  f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))
nmt-master/nmt/scripts/rouge.py:169:  # return overlapping_count / reference_count
nmt-master/nmt/scripts/rouge.py:170:  return f1_score, precision, recall
nmt-master/nmt/scripts/rouge.py:173:def _f_p_r_lcs(llcs, m, n):
nmt-master/nmt/scripts/rouge.py:174:  """
nmt-master/nmt/scripts/rouge.py:175:  Computes the LCS-based F-measure score
nmt-master/nmt/scripts/rouge.py:176:  Source: http://research.microsoft.com/en-us/um/people/cyl/download/papers/
nmt-master/nmt/scripts/rouge.py:177:  rouge-working-note-v1.3.1.pdf
nmt-master/nmt/scripts/rouge.py:179:  Args:
nmt-master/nmt/scripts/rouge.py:180:    llcs: Length of LCS
nmt-master/nmt/scripts/rouge.py:181:    m: number of words in reference summary
nmt-master/nmt/scripts/rouge.py:182:    n: number of words in candidate summary
nmt-master/nmt/scripts/rouge.py:184:  Returns:
nmt-master/nmt/scripts/rouge.py:185:    Float. LCS-based F-measure score
nmt-master/nmt/scripts/rouge.py:186:  """
nmt-master/nmt/scripts/rouge.py:187:  r_lcs = llcs / m
nmt-master/nmt/scripts/rouge.py:188:  p_lcs = llcs / n
nmt-master/nmt/scripts/rouge.py:189:  beta = p_lcs / (r_lcs + 1e-12)
nmt-master/nmt/scripts/rouge.py:190:  num = (1 + (beta**2)) * r_lcs * p_lcs
nmt-master/nmt/scripts/rouge.py:191:  denom = r_lcs + ((beta**2) * p_lcs)
nmt-master/nmt/scripts/rouge.py:192:  f_lcs = num / (denom + 1e-12)
nmt-master/nmt/scripts/rouge.py:193:  return f_lcs, p_lcs, r_lcs
nmt-master/nmt/scripts/rouge.py:196:def rouge_l_sentence_level(evaluated_sentences, reference_sentences):
nmt-master/nmt/scripts/rouge.py:197:  """
nmt-master/nmt/scripts/rouge.py:198:  Computes ROUGE-L (sentence level) of two text collections of sentences.
nmt-master/nmt/scripts/rouge.py:199:  http://research.microsoft.com/en-us/um/people/cyl/download/papers/
nmt-master/nmt/scripts/rouge.py:200:  rouge-working-note-v1.3.1.pdf
nmt-master/nmt/scripts/rouge.py:202:  Calculated according to:
nmt-master/nmt/scripts/rouge.py:203:  R_lcs = LCS(X,Y)/m
nmt-master/nmt/scripts/rouge.py:204:  P_lcs = LCS(X,Y)/n
nmt-master/nmt/scripts/rouge.py:205:  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)
nmt-master/nmt/scripts/rouge.py:207:  where:
nmt-master/nmt/scripts/rouge.py:208:  X = reference summary
nmt-master/nmt/scripts/rouge.py:209:  Y = Candidate summary
nmt-master/nmt/scripts/rouge.py:210:  m = length of reference summary
nmt-master/nmt/scripts/rouge.py:211:  n = length of candidate summary
nmt-master/nmt/scripts/rouge.py:213:  Args:
nmt-master/nmt/scripts/rouge.py:214:    evaluated_sentences: The sentences that have been picked by the summarizer
nmt-master/nmt/scripts/rouge.py:215:    reference_sentences: The sentences from the referene set
nmt-master/nmt/scripts/rouge.py:217:  Returns:
nmt-master/nmt/scripts/rouge.py:218:    A float: F_lcs
nmt-master/nmt/scripts/rouge.py:220:  Raises:
nmt-master/nmt/scripts/rouge.py:221:    ValueError: raises exception if a param has len <= 0
nmt-master/nmt/scripts/rouge.py:222:  """
nmt-master/nmt/scripts/rouge.py:223:  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:
nmt-master/nmt/scripts/rouge.py:224:    raise ValueError("Collections must contain at least 1 sentence.")
nmt-master/nmt/scripts/rouge.py:225:  reference_words = _split_into_words(reference_sentences)
nmt-master/nmt/scripts/rouge.py:226:  evaluated_words = _split_into_words(evaluated_sentences)
nmt-master/nmt/scripts/rouge.py:227:  m = len(reference_words)
nmt-master/nmt/scripts/rouge.py:228:  n = len(evaluated_words)
nmt-master/nmt/scripts/rouge.py:229:  lcs = _len_lcs(evaluated_words, reference_words)
nmt-master/nmt/scripts/rouge.py:230:  return _f_p_r_lcs(lcs, m, n)
nmt-master/nmt/scripts/rouge.py:233:def _union_lcs(evaluated_sentences, reference_sentence):
nmt-master/nmt/scripts/rouge.py:234:  """
nmt-master/nmt/scripts/rouge.py:235:  Returns LCS_u(r_i, C) which is the LCS score of the union longest common
nmt-master/nmt/scripts/rouge.py:236:  subsequence between reference sentence ri and candidate summary C. For example
nmt-master/nmt/scripts/rouge.py:237:  if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and
nmt-master/nmt/scripts/rouge.py:238:  c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1 is
nmt-master/nmt/scripts/rouge.py:239:  "w1 w2" and the longest common subsequence of r_i and c2 is "w1 w3 w5". The
nmt-master/nmt/scripts/rouge.py:240:  union longest common subsequence of r_i, c1, and c2 is "w1 w2 w3 w5" and
nmt-master/nmt/scripts/rouge.py:241:  LCS_u(r_i, C) = 4/5.
nmt-master/nmt/scripts/rouge.py:243:  Args:
nmt-master/nmt/scripts/rouge.py:244:    evaluated_sentences: The sentences that have been picked by the summarizer
nmt-master/nmt/scripts/rouge.py:245:    reference_sentence: One of the sentences in the reference summaries
nmt-master/nmt/scripts/rouge.py:247:  Returns:
nmt-master/nmt/scripts/rouge.py:248:    float: LCS_u(r_i, C)
nmt-master/nmt/scripts/rouge.py:250:  ValueError:
nmt-master/nmt/scripts/rouge.py:251:    Raises exception if a param has len <= 0
nmt-master/nmt/scripts/rouge.py:252:  """
nmt-master/nmt/scripts/rouge.py:253:  if len(evaluated_sentences) <= 0:
nmt-master/nmt/scripts/rouge.py:254:    raise ValueError("Collections must contain at least 1 sentence.")
nmt-master/nmt/scripts/rouge.py:256:  lcs_union = set()
nmt-master/nmt/scripts/rouge.py:257:  reference_words = _split_into_words([reference_sentence])
nmt-master/nmt/scripts/rouge.py:258:  combined_lcs_length = 0
nmt-master/nmt/scripts/rouge.py:259:  for eval_s in evaluated_sentences:
nmt-master/nmt/scripts/rouge.py:260:    evaluated_words = _split_into_words([eval_s])
nmt-master/nmt/scripts/rouge.py:261:    lcs = set(_recon_lcs(reference_words, evaluated_words))
nmt-master/nmt/scripts/rouge.py:262:    combined_lcs_length += len(lcs)
nmt-master/nmt/scripts/rouge.py:263:    lcs_union = lcs_union.union(lcs)
nmt-master/nmt/scripts/rouge.py:265:  union_lcs_count = len(lcs_union)
nmt-master/nmt/scripts/rouge.py:266:  union_lcs_value = union_lcs_count / combined_lcs_length
nmt-master/nmt/scripts/rouge.py:267:  return union_lcs_value
nmt-master/nmt/scripts/rouge.py:270:def rouge_l_summary_level(evaluated_sentences, reference_sentences):
nmt-master/nmt/scripts/rouge.py:271:  """
nmt-master/nmt/scripts/rouge.py:272:  Computes ROUGE-L (summary level) of two text collections of sentences.
nmt-master/nmt/scripts/rouge.py:273:  http://research.microsoft.com/en-us/um/people/cyl/download/papers/
nmt-master/nmt/scripts/rouge.py:274:  rouge-working-note-v1.3.1.pdf
nmt-master/nmt/scripts/rouge.py:276:  Calculated according to:
nmt-master/nmt/scripts/rouge.py:277:  R_lcs = SUM(1, u)[LCS<union>(r_i,C)]/m
nmt-master/nmt/scripts/rouge.py:278:  P_lcs = SUM(1, u)[LCS<union>(r_i,C)]/n
nmt-master/nmt/scripts/rouge.py:279:  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)
nmt-master/nmt/scripts/rouge.py:281:  where:
nmt-master/nmt/scripts/rouge.py:282:  SUM(i,u) = SUM from i through u
nmt-master/nmt/scripts/rouge.py:283:  u = number of sentences in reference summary
nmt-master/nmt/scripts/rouge.py:284:  C = Candidate summary made up of v sentences
nmt-master/nmt/scripts/rouge.py:285:  m = number of words in reference summary
nmt-master/nmt/scripts/rouge.py:286:  n = number of words in candidate summary
nmt-master/nmt/scripts/rouge.py:288:  Args:
nmt-master/nmt/scripts/rouge.py:289:    evaluated_sentences: The sentences that have been picked by the summarizer
nmt-master/nmt/scripts/rouge.py:290:    reference_sentence: One of the sentences in the reference summaries
nmt-master/nmt/scripts/rouge.py:292:  Returns:
nmt-master/nmt/scripts/rouge.py:293:    A float: F_lcs
nmt-master/nmt/scripts/rouge.py:295:  Raises:
nmt-master/nmt/scripts/rouge.py:296:    ValueError: raises exception if a param has len <= 0
nmt-master/nmt/scripts/rouge.py:297:  """
nmt-master/nmt/scripts/rouge.py:298:  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:
nmt-master/nmt/scripts/rouge.py:299:    raise ValueError("Collections must contain at least 1 sentence.")
nmt-master/nmt/scripts/rouge.py:301:  # total number of words in reference sentences
nmt-master/nmt/scripts/rouge.py:302:  m = len(_split_into_words(reference_sentences))
nmt-master/nmt/scripts/rouge.py:304:  # total number of words in evaluated sentences
nmt-master/nmt/scripts/rouge.py:305:  n = len(_split_into_words(evaluated_sentences))
nmt-master/nmt/scripts/rouge.py:307:  union_lcs_sum_across_all_references = 0
nmt-master/nmt/scripts/rouge.py:308:  for ref_s in reference_sentences:
nmt-master/nmt/scripts/rouge.py:309:    union_lcs_sum_across_all_references += _union_lcs(evaluated_sentences,
nmt-master/nmt/scripts/rouge.py:310:                                                      ref_s)
nmt-master/nmt/scripts/rouge.py:311:  return _f_p_r_lcs(union_lcs_sum_across_all_references, m, n)
nmt-master/nmt/scripts/rouge.py:314:def rouge(hypotheses, references):
nmt-master/nmt/scripts/rouge.py:315:  """Calculates average rouge scores for a list of hypotheses and
nmt-master/nmt/scripts/rouge.py:316:  references"""
nmt-master/nmt/scripts/rouge.py:318:  # Filter out hyps that are of 0 length
nmt-master/nmt/scripts/rouge.py:319:  # hyps_and_refs = zip(hypotheses, references)
nmt-master/nmt/scripts/rouge.py:320:  # hyps_and_refs = [_ for _ in hyps_and_refs if len(_[0]) > 0]
nmt-master/nmt/scripts/rouge.py:321:  # hypotheses, references = zip(*hyps_and_refs)
nmt-master/nmt/scripts/rouge.py:323:  # Calculate ROUGE-1 F1, precision, recall scores
nmt-master/nmt/scripts/rouge.py:324:  rouge_1 = [
nmt-master/nmt/scripts/rouge.py:325:      rouge_n([hyp], [ref], 1) for hyp, ref in zip(hypotheses, references)
nmt-master/nmt/scripts/rouge.py:326:  ]
nmt-master/nmt/scripts/rouge.py:327:  rouge_1_f, rouge_1_p, rouge_1_r = map(np.mean, zip(*rouge_1))
nmt-master/nmt/scripts/rouge.py:329:  # Calculate ROUGE-2 F1, precision, recall scores
nmt-master/nmt/scripts/rouge.py:330:  rouge_2 = [
nmt-master/nmt/scripts/rouge.py:331:      rouge_n([hyp], [ref], 2) for hyp, ref in zip(hypotheses, references)
nmt-master/nmt/scripts/rouge.py:332:  ]
nmt-master/nmt/scripts/rouge.py:333:  rouge_2_f, rouge_2_p, rouge_2_r = map(np.mean, zip(*rouge_2))
nmt-master/nmt/scripts/rouge.py:335:  # Calculate ROUGE-L F1, precision, recall scores
nmt-master/nmt/scripts/rouge.py:336:  rouge_l = [
nmt-master/nmt/scripts/rouge.py:337:      rouge_l_sentence_level([hyp], [ref])
nmt-master/nmt/scripts/rouge.py:338:      for hyp, ref in zip(hypotheses, references)
nmt-master/nmt/scripts/rouge.py:339:  ]
nmt-master/nmt/scripts/rouge.py:340:  rouge_l_f, rouge_l_p, rouge_l_r = map(np.mean, zip(*rouge_l))
nmt-master/nmt/scripts/rouge.py:342:  return {
nmt-master/nmt/scripts/rouge.py:343:      "rouge_1/f_score": rouge_1_f,
nmt-master/nmt/scripts/rouge.py:344:      "rouge_1/r_score": rouge_1_r,
nmt-master/nmt/scripts/rouge.py:345:      "rouge_1/p_score": rouge_1_p,
nmt-master/nmt/scripts/rouge.py:346:      "rouge_2/f_score": rouge_2_f,
nmt-master/nmt/scripts/rouge.py:347:      "rouge_2/r_score": rouge_2_r,
nmt-master/nmt/scripts/rouge.py:348:      "rouge_2/p_score": rouge_2_p,
nmt-master/nmt/scripts/rouge.py:349:      "rouge_l/f_score": rouge_l_f,
nmt-master/nmt/scripts/rouge.py:350:      "rouge_l/r_score": rouge_l_r,
nmt-master/nmt/scripts/rouge.py:351:      "rouge_l/p_score": rouge_l_p,
nmt-master/nmt/scripts/rouge.py:352:  }
nmt-master/nmt/train.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/train.py:2:#
nmt-master/nmt/train.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/train.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/train.py:5:# You may obtain a copy of the License at
nmt-master/nmt/train.py:6:#
nmt-master/nmt/train.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/train.py:8:#
nmt-master/nmt/train.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/train.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/train.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/train.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/train.py:13:# limitations under the License.
nmt-master/nmt/train.py:14:# ==============================================================================
nmt-master/nmt/train.py:15:"""For training NMT models."""
nmt-master/nmt/train.py:16:from __future__ import print_function
nmt-master/nmt/train.py:18:import math
nmt-master/nmt/train.py:19:import os
nmt-master/nmt/train.py:20:import random
nmt-master/nmt/train.py:21:import time
nmt-master/nmt/train.py:23:import tensorflow as tf
nmt-master/nmt/train.py:25:from . import attention_model
nmt-master/nmt/train.py:26:from . import gnmt_model
nmt-master/nmt/train.py:27:from . import inference
nmt-master/nmt/train.py:28:from . import model as nmt_model
nmt-master/nmt/train.py:29:from . import model_helper
nmt-master/nmt/train.py:30:from .utils import misc_utils as utils
nmt-master/nmt/train.py:31:from .utils import nmt_utils
nmt-master/nmt/train.py:33:utils.check_tensorflow_version()
nmt-master/nmt/train.py:35:__all__ = [
nmt-master/nmt/train.py:36:    "run_sample_decode", "run_internal_eval", "run_external_eval",
nmt-master/nmt/train.py:37:    "run_avg_external_eval", "run_full_eval", "init_stats", "update_stats",
nmt-master/nmt/train.py:38:    "print_step_info", "process_stats", "train", "get_model_creator",
nmt-master/nmt/train.py:39:    "add_info_summaries", "get_best_results"
nmt-master/nmt/train.py:40:]
nmt-master/nmt/train.py:43:def run_sample_decode(infer_model, infer_sess, model_dir, hparams,
nmt-master/nmt/train.py:44:                      summary_writer, src_data, tgt_data):
nmt-master/nmt/train.py:45:  """Sample decode a random sentence from src_data."""
nmt-master/nmt/train.py:46:  with infer_model.graph.as_default():
nmt-master/nmt/train.py:47:    loaded_infer_model, global_step = model_helper.create_or_load_model(
nmt-master/nmt/train.py:48:        infer_model.model, model_dir, infer_sess, "infer")
nmt-master/nmt/train.py:50:  _sample_decode(loaded_infer_model, global_step, infer_sess, hparams,
nmt-master/nmt/train.py:51:                 infer_model.iterator, src_data, tgt_data,
nmt-master/nmt/train.py:52:                 infer_model.src_placeholder,
nmt-master/nmt/train.py:53:                 infer_model.batch_size_placeholder, summary_writer)
nmt-master/nmt/train.py:56:def run_internal_eval(eval_model,
nmt-master/nmt/train.py:57:                      eval_sess,
nmt-master/nmt/train.py:58:                      model_dir,
nmt-master/nmt/train.py:59:                      hparams,
nmt-master/nmt/train.py:60:                      summary_writer,
nmt-master/nmt/train.py:61:                      use_test_set=True,
nmt-master/nmt/train.py:62:                      dev_eval_iterator_feed_dict=None,
nmt-master/nmt/train.py:63:                      test_eval_iterator_feed_dict=None):
nmt-master/nmt/train.py:64:  """Compute internal evaluation (perplexity) for both dev / test.
nmt-master/nmt/train.py:66:  Computes development and testing perplexities for given model.
nmt-master/nmt/train.py:68:  Args:
nmt-master/nmt/train.py:69:    eval_model: Evaluation model for which to compute perplexities.
nmt-master/nmt/train.py:70:    eval_sess: Evaluation TensorFlow session.
nmt-master/nmt/train.py:71:    model_dir: Directory from which to load evaluation model from.
nmt-master/nmt/train.py:72:    hparams: Model hyper-parameters.
nmt-master/nmt/train.py:73:    summary_writer: Summary writer for logging metrics to TensorBoard.
nmt-master/nmt/train.py:74:    use_test_set: Computes testing perplexity if true; does not otherwise.
nmt-master/nmt/train.py:75:      Note that the development perplexity is always computed regardless of
nmt-master/nmt/train.py:76:      value of this parameter.
nmt-master/nmt/train.py:77:    dev_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
nmt-master/nmt/train.py:78:      Can be used to pass in additional inputs necessary for running the
nmt-master/nmt/train.py:79:      development evaluation.
nmt-master/nmt/train.py:80:    test_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
nmt-master/nmt/train.py:81:      Can be used to pass in additional inputs necessary for running the
nmt-master/nmt/train.py:82:      testing evaluation.
nmt-master/nmt/train.py:83:  Returns:
nmt-master/nmt/train.py:84:    Pair containing development perplexity and testing perplexity, in this
nmt-master/nmt/train.py:85:    order.
nmt-master/nmt/train.py:86:  """
nmt-master/nmt/train.py:87:  if dev_eval_iterator_feed_dict is None:
nmt-master/nmt/train.py:88:    dev_eval_iterator_feed_dict = {}
nmt-master/nmt/train.py:89:  if test_eval_iterator_feed_dict is None:
nmt-master/nmt/train.py:90:    test_eval_iterator_feed_dict = {}
nmt-master/nmt/train.py:91:  with eval_model.graph.as_default():
nmt-master/nmt/train.py:92:    loaded_eval_model, global_step = model_helper.create_or_load_model(
nmt-master/nmt/train.py:93:        eval_model.model, model_dir, eval_sess, "eval")
nmt-master/nmt/train.py:95:  dev_src_file = "%s.%s" % (hparams.dev_prefix, hparams.src)
nmt-master/nmt/train.py:96:  dev_tgt_file = "%s.%s" % (hparams.dev_prefix, hparams.tgt)
nmt-master/nmt/train.py:97:  dev_eval_iterator_feed_dict[eval_model.src_file_placeholder] = dev_src_file
nmt-master/nmt/train.py:98:  dev_eval_iterator_feed_dict[eval_model.tgt_file_placeholder] = dev_tgt_file
nmt-master/nmt/train.py:100:  dev_ppl = _internal_eval(loaded_eval_model, global_step, eval_sess,
nmt-master/nmt/train.py:101:                           eval_model.iterator, dev_eval_iterator_feed_dict,
nmt-master/nmt/train.py:102:                           summary_writer, "dev")
nmt-master/nmt/train.py:103:  test_ppl = None
nmt-master/nmt/train.py:104:  if use_test_set and hparams.test_prefix:
nmt-master/nmt/train.py:105:    test_src_file = "%s.%s" % (hparams.test_prefix, hparams.src)
nmt-master/nmt/train.py:106:    test_tgt_file = "%s.%s" % (hparams.test_prefix, hparams.tgt)
nmt-master/nmt/train.py:107:    test_eval_iterator_feed_dict[
nmt-master/nmt/train.py:108:        eval_model.src_file_placeholder] = test_src_file
nmt-master/nmt/train.py:109:    test_eval_iterator_feed_dict[
nmt-master/nmt/train.py:110:        eval_model.tgt_file_placeholder] = test_tgt_file
nmt-master/nmt/train.py:111:    test_ppl = _internal_eval(loaded_eval_model, global_step, eval_sess,
nmt-master/nmt/train.py:112:                              eval_model.iterator, test_eval_iterator_feed_dict,
nmt-master/nmt/train.py:113:                              summary_writer, "test")
nmt-master/nmt/train.py:114:  return dev_ppl, test_ppl
nmt-master/nmt/train.py:117:def run_external_eval(infer_model,
nmt-master/nmt/train.py:118:                      infer_sess,
nmt-master/nmt/train.py:119:                      model_dir,
nmt-master/nmt/train.py:120:                      hparams,
nmt-master/nmt/train.py:121:                      summary_writer,
nmt-master/nmt/train.py:122:                      save_best_dev=True,
nmt-master/nmt/train.py:123:                      use_test_set=True,
nmt-master/nmt/train.py:124:                      avg_ckpts=False,
nmt-master/nmt/train.py:125:                      dev_infer_iterator_feed_dict=None,
nmt-master/nmt/train.py:126:                      test_infer_iterator_feed_dict=None):
nmt-master/nmt/train.py:127:  """Compute external evaluation for both dev / test.
nmt-master/nmt/train.py:129:  Computes development and testing external evaluation (e.g. bleu, rouge) for
nmt-master/nmt/train.py:130:  given model.
nmt-master/nmt/train.py:132:  Args:
nmt-master/nmt/train.py:133:    infer_model: Inference model for which to compute perplexities.
nmt-master/nmt/train.py:134:    infer_sess: Inference TensorFlow session.
nmt-master/nmt/train.py:135:    model_dir: Directory from which to load inference model from.
nmt-master/nmt/train.py:136:    hparams: Model hyper-parameters.
nmt-master/nmt/train.py:137:    summary_writer: Summary writer for logging metrics to TensorBoard.
nmt-master/nmt/train.py:138:    use_test_set: Computes testing external evaluation if true; does not
nmt-master/nmt/train.py:139:      otherwise. Note that the development external evaluation is always
nmt-master/nmt/train.py:140:      computed regardless of value of this parameter.
nmt-master/nmt/train.py:141:    dev_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
nmt-master/nmt/train.py:142:      Can be used to pass in additional inputs necessary for running the
nmt-master/nmt/train.py:143:      development external evaluation.
nmt-master/nmt/train.py:144:    test_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
nmt-master/nmt/train.py:145:      Can be used to pass in additional inputs necessary for running the
nmt-master/nmt/train.py:146:      testing external evaluation.
nmt-master/nmt/train.py:147:  Returns:
nmt-master/nmt/train.py:148:    Triple containing development scores, testing scores and the TensorFlow
nmt-master/nmt/train.py:149:    Variable for the global step number, in this order.
nmt-master/nmt/train.py:150:  """
nmt-master/nmt/train.py:151:  if dev_infer_iterator_feed_dict is None:
nmt-master/nmt/train.py:152:    dev_infer_iterator_feed_dict = {}
nmt-master/nmt/train.py:153:  if test_infer_iterator_feed_dict is None:
nmt-master/nmt/train.py:154:    test_infer_iterator_feed_dict = {}
nmt-master/nmt/train.py:155:  with infer_model.graph.as_default():
nmt-master/nmt/train.py:156:    loaded_infer_model, global_step = model_helper.create_or_load_model(
nmt-master/nmt/train.py:157:        infer_model.model, model_dir, infer_sess, "infer")
nmt-master/nmt/train.py:159:  dev_src_file = "%s.%s" % (hparams.dev_prefix, hparams.src)
nmt-master/nmt/train.py:160:  dev_tgt_file = "%s.%s" % (hparams.dev_prefix, hparams.tgt)
nmt-master/nmt/train.py:161:  dev_infer_iterator_feed_dict[
nmt-master/nmt/train.py:162:      infer_model.src_placeholder] = inference.load_data(dev_src_file)
nmt-master/nmt/train.py:163:  dev_infer_iterator_feed_dict[
nmt-master/nmt/train.py:164:      infer_model.batch_size_placeholder] = hparams.infer_batch_size
nmt-master/nmt/train.py:165:  dev_scores = _external_eval(
nmt-master/nmt/train.py:166:      loaded_infer_model,
nmt-master/nmt/train.py:167:      global_step,
nmt-master/nmt/train.py:168:      infer_sess,
nmt-master/nmt/train.py:169:      hparams,
nmt-master/nmt/train.py:170:      infer_model.iterator,
nmt-master/nmt/train.py:171:      dev_infer_iterator_feed_dict,
nmt-master/nmt/train.py:172:      dev_tgt_file,
nmt-master/nmt/train.py:173:      "dev",
nmt-master/nmt/train.py:174:      summary_writer,
nmt-master/nmt/train.py:175:      save_on_best=save_best_dev,
nmt-master/nmt/train.py:176:      avg_ckpts=avg_ckpts)
nmt-master/nmt/train.py:178:  test_scores = None
nmt-master/nmt/train.py:179:  if use_test_set and hparams.test_prefix:
nmt-master/nmt/train.py:180:    test_src_file = "%s.%s" % (hparams.test_prefix, hparams.src)
nmt-master/nmt/train.py:181:    test_tgt_file = "%s.%s" % (hparams.test_prefix, hparams.tgt)
nmt-master/nmt/train.py:182:    test_infer_iterator_feed_dict[
nmt-master/nmt/train.py:183:        infer_model.src_placeholder] = inference.load_data(test_src_file)
nmt-master/nmt/train.py:184:    test_infer_iterator_feed_dict[
nmt-master/nmt/train.py:185:        infer_model.batch_size_placeholder] = hparams.infer_batch_size
nmt-master/nmt/train.py:186:    test_scores = _external_eval(
nmt-master/nmt/train.py:187:        loaded_infer_model,
nmt-master/nmt/train.py:188:        global_step,
nmt-master/nmt/train.py:189:        infer_sess,
nmt-master/nmt/train.py:190:        hparams,
nmt-master/nmt/train.py:191:        infer_model.iterator,
nmt-master/nmt/train.py:192:        test_infer_iterator_feed_dict,
nmt-master/nmt/train.py:193:        test_tgt_file,
nmt-master/nmt/train.py:194:        "test",
nmt-master/nmt/train.py:195:        summary_writer,
nmt-master/nmt/train.py:196:        save_on_best=False,
nmt-master/nmt/train.py:197:        avg_ckpts=avg_ckpts)
nmt-master/nmt/train.py:198:  return dev_scores, test_scores, global_step
nmt-master/nmt/train.py:201:def run_avg_external_eval(infer_model, infer_sess, model_dir, hparams,
nmt-master/nmt/train.py:202:                          summary_writer, global_step):
nmt-master/nmt/train.py:203:  """Creates an averaged checkpoint and run external eval with it."""
nmt-master/nmt/train.py:204:  avg_dev_scores, avg_test_scores = None, None
nmt-master/nmt/train.py:205:  if hparams.avg_ckpts:
nmt-master/nmt/train.py:206:    # Convert VariableName:0 to VariableName.
nmt-master/nmt/train.py:207:    global_step_name = infer_model.model.global_step.name.split(":")[0]
nmt-master/nmt/train.py:208:    avg_model_dir = model_helper.avg_checkpoints(
nmt-master/nmt/train.py:209:        model_dir, hparams.num_keep_ckpts, global_step, global_step_name)
nmt-master/nmt/train.py:211:    if avg_model_dir:
nmt-master/nmt/train.py:212:      avg_dev_scores, avg_test_scores, _ = run_external_eval(
nmt-master/nmt/train.py:213:          infer_model,
nmt-master/nmt/train.py:214:          infer_sess,
nmt-master/nmt/train.py:215:          avg_model_dir,
nmt-master/nmt/train.py:216:          hparams,
nmt-master/nmt/train.py:217:          summary_writer,
nmt-master/nmt/train.py:218:          avg_ckpts=True)
nmt-master/nmt/train.py:220:  return avg_dev_scores, avg_test_scores
nmt-master/nmt/train.py:223:def run_internal_and_external_eval(model_dir,
nmt-master/nmt/train.py:224:                                   infer_model,
nmt-master/nmt/train.py:225:                                   infer_sess,
nmt-master/nmt/train.py:226:                                   eval_model,
nmt-master/nmt/train.py:227:                                   eval_sess,
nmt-master/nmt/train.py:228:                                   hparams,
nmt-master/nmt/train.py:229:                                   summary_writer,
nmt-master/nmt/train.py:230:                                   avg_ckpts=False,
nmt-master/nmt/train.py:231:                                   dev_eval_iterator_feed_dict=None,
nmt-master/nmt/train.py:232:                                   test_eval_iterator_feed_dict=None,
nmt-master/nmt/train.py:233:                                   dev_infer_iterator_feed_dict=None,
nmt-master/nmt/train.py:234:                                   test_infer_iterator_feed_dict=None):
nmt-master/nmt/train.py:235:  """Compute internal evaluation (perplexity) for both dev / test.
nmt-master/nmt/train.py:237:  Computes development and testing perplexities for given model.
nmt-master/nmt/train.py:239:  Args:
nmt-master/nmt/train.py:240:    model_dir: Directory from which to load models from.
nmt-master/nmt/train.py:241:    infer_model: Inference model for which to compute perplexities.
nmt-master/nmt/train.py:242:    infer_sess: Inference TensorFlow session.
nmt-master/nmt/train.py:243:    eval_model: Evaluation model for which to compute perplexities.
nmt-master/nmt/train.py:244:    eval_sess: Evaluation TensorFlow session.
nmt-master/nmt/train.py:245:    hparams: Model hyper-parameters.
nmt-master/nmt/train.py:246:    summary_writer: Summary writer for logging metrics to TensorBoard.
nmt-master/nmt/train.py:247:    avg_ckpts: Whether to compute average external evaluation scores.
nmt-master/nmt/train.py:248:    dev_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
nmt-master/nmt/train.py:249:      Can be used to pass in additional inputs necessary for running the
nmt-master/nmt/train.py:250:      internal development evaluation.
nmt-master/nmt/train.py:251:    test_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
nmt-master/nmt/train.py:252:      Can be used to pass in additional inputs necessary for running the
nmt-master/nmt/train.py:253:      internal testing evaluation.
nmt-master/nmt/train.py:254:    dev_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
nmt-master/nmt/train.py:255:      Can be used to pass in additional inputs necessary for running the
nmt-master/nmt/train.py:256:      external development evaluation.
nmt-master/nmt/train.py:257:    test_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
nmt-master/nmt/train.py:258:      Can be used to pass in additional inputs necessary for running the
nmt-master/nmt/train.py:259:      external testing evaluation.
nmt-master/nmt/train.py:260:  Returns:
nmt-master/nmt/train.py:261:    Triple containing results summary, global step Tensorflow Variable and
nmt-master/nmt/train.py:262:    metrics in this order.
nmt-master/nmt/train.py:263:  """
nmt-master/nmt/train.py:264:  dev_ppl, test_ppl = run_internal_eval(
nmt-master/nmt/train.py:265:      eval_model,
nmt-master/nmt/train.py:266:      eval_sess,
nmt-master/nmt/train.py:267:      model_dir,
nmt-master/nmt/train.py:268:      hparams,
nmt-master/nmt/train.py:269:      summary_writer,
nmt-master/nmt/train.py:270:      dev_eval_iterator_feed_dict=dev_eval_iterator_feed_dict,
nmt-master/nmt/train.py:271:      test_eval_iterator_feed_dict=test_eval_iterator_feed_dict)
nmt-master/nmt/train.py:272:  dev_scores, test_scores, global_step = run_external_eval(
nmt-master/nmt/train.py:273:      infer_model,
nmt-master/nmt/train.py:274:      infer_sess,
nmt-master/nmt/train.py:275:      model_dir,
nmt-master/nmt/train.py:276:      hparams,
nmt-master/nmt/train.py:277:      summary_writer,
nmt-master/nmt/train.py:278:      dev_infer_iterator_feed_dict=dev_infer_iterator_feed_dict,
nmt-master/nmt/train.py:279:      test_infer_iterator_feed_dict=test_infer_iterator_feed_dict)
nmt-master/nmt/train.py:281:  metrics = {
nmt-master/nmt/train.py:282:      "dev_ppl": dev_ppl,
nmt-master/nmt/train.py:283:      "test_ppl": test_ppl,
nmt-master/nmt/train.py:284:      "dev_scores": dev_scores,
nmt-master/nmt/train.py:285:      "test_scores": test_scores,
nmt-master/nmt/train.py:286:  }
nmt-master/nmt/train.py:288:  avg_dev_scores, avg_test_scores = None, None
nmt-master/nmt/train.py:289:  if avg_ckpts:
nmt-master/nmt/train.py:290:    avg_dev_scores, avg_test_scores = run_avg_external_eval(
nmt-master/nmt/train.py:291:        infer_model, infer_sess, model_dir, hparams, summary_writer,
nmt-master/nmt/train.py:292:        global_step)
nmt-master/nmt/train.py:293:    metrics["avg_dev_scores"] = avg_dev_scores
nmt-master/nmt/train.py:294:    metrics["avg_test_scores"] = avg_test_scores
nmt-master/nmt/train.py:296:  result_summary = _format_results("dev", dev_ppl, dev_scores, hparams.metrics)
nmt-master/nmt/train.py:297:  if avg_dev_scores:
nmt-master/nmt/train.py:298:    result_summary += ", " + _format_results("avg_dev", None, avg_dev_scores,
nmt-master/nmt/train.py:299:                                             hparams.metrics)
nmt-master/nmt/train.py:300:  if hparams.test_prefix:
nmt-master/nmt/train.py:301:    result_summary += ", " + _format_results("test", test_ppl, test_scores,
nmt-master/nmt/train.py:302:                                             hparams.metrics)
nmt-master/nmt/train.py:303:    if avg_test_scores:
nmt-master/nmt/train.py:304:      result_summary += ", " + _format_results("avg_test", None,
nmt-master/nmt/train.py:305:                                               avg_test_scores, hparams.metrics)
nmt-master/nmt/train.py:307:  return result_summary, global_step, metrics
nmt-master/nmt/train.py:310:def run_full_eval(model_dir,
nmt-master/nmt/train.py:311:                  infer_model,
nmt-master/nmt/train.py:312:                  infer_sess,
nmt-master/nmt/train.py:313:                  eval_model,
nmt-master/nmt/train.py:314:                  eval_sess,
nmt-master/nmt/train.py:315:                  hparams,
nmt-master/nmt/train.py:316:                  summary_writer,
nmt-master/nmt/train.py:317:                  sample_src_data,
nmt-master/nmt/train.py:318:                  sample_tgt_data,
nmt-master/nmt/train.py:319:                  avg_ckpts=False):
nmt-master/nmt/train.py:320:  """Wrapper for running sample_decode, internal_eval and external_eval.
nmt-master/nmt/train.py:322:  Args:
nmt-master/nmt/train.py:323:    model_dir: Directory from which to load models from.
nmt-master/nmt/train.py:324:    infer_model: Inference model for which to compute perplexities.
nmt-master/nmt/train.py:325:    infer_sess: Inference TensorFlow session.
nmt-master/nmt/train.py:326:    eval_model: Evaluation model for which to compute perplexities.
nmt-master/nmt/train.py:327:    eval_sess: Evaluation TensorFlow session.
nmt-master/nmt/train.py:328:    hparams: Model hyper-parameters.
nmt-master/nmt/train.py:329:    summary_writer: Summary writer for logging metrics to TensorBoard.
nmt-master/nmt/train.py:330:    sample_src_data: sample of source data for sample decoding.
nmt-master/nmt/train.py:331:    sample_tgt_data: sample of target data for sample decoding.
nmt-master/nmt/train.py:332:    avg_ckpts: Whether to compute average external evaluation scores.
nmt-master/nmt/train.py:333:  Returns:
nmt-master/nmt/train.py:334:    Triple containing results summary, global step Tensorflow Variable and
nmt-master/nmt/train.py:335:    metrics in this order.
nmt-master/nmt/train.py:336:  """
nmt-master/nmt/train.py:337:  run_sample_decode(infer_model, infer_sess, model_dir, hparams, summary_writer,
nmt-master/nmt/train.py:338:                    sample_src_data, sample_tgt_data)
nmt-master/nmt/train.py:339:  return run_internal_and_external_eval(model_dir, infer_model, infer_sess,
nmt-master/nmt/train.py:340:                                        eval_model, eval_sess, hparams,
nmt-master/nmt/train.py:341:                                        summary_writer, avg_ckpts)
nmt-master/nmt/train.py:344:def init_stats():
nmt-master/nmt/train.py:345:  """Initialize statistics that we want to accumulate."""
nmt-master/nmt/train.py:346:  return {"step_time": 0.0, "train_loss": 0.0,
nmt-master/nmt/train.py:347:          "predict_count": 0.0,  # word count on the target side
nmt-master/nmt/train.py:348:          "word_count": 0.0,  # word counts for both source and target
nmt-master/nmt/train.py:349:          "sequence_count": 0.0,  # number of training examples processed
nmt-master/nmt/train.py:350:          "grad_norm": 0.0}
nmt-master/nmt/train.py:353:def update_stats(stats, start_time, step_result):
nmt-master/nmt/train.py:354:  """Update stats: write summary and accumulate statistics."""
nmt-master/nmt/train.py:355:  _, output_tuple = step_result
nmt-master/nmt/train.py:357:  # Update statistics
nmt-master/nmt/train.py:358:  batch_size = output_tuple.batch_size
nmt-master/nmt/train.py:359:  stats["step_time"] += time.time() - start_time
nmt-master/nmt/train.py:360:  stats["train_loss"] += output_tuple.train_loss * batch_size
nmt-master/nmt/train.py:361:  stats["grad_norm"] += output_tuple.grad_norm
nmt-master/nmt/train.py:362:  stats["predict_count"] += output_tuple.predict_count
nmt-master/nmt/train.py:363:  stats["word_count"] += output_tuple.word_count
nmt-master/nmt/train.py:364:  stats["sequence_count"] += batch_size
nmt-master/nmt/train.py:366:  return (output_tuple.global_step, output_tuple.learning_rate,
nmt-master/nmt/train.py:367:          output_tuple.train_summary)
nmt-master/nmt/train.py:370:def print_step_info(prefix, global_step, info, result_summary, log_f):
nmt-master/nmt/train.py:371:  """Print all info at the current global step."""
nmt-master/nmt/train.py:372:  utils.print_out(
nmt-master/nmt/train.py:373:      "%sstep %d lr %g step-time %.2fs wps %.2fK ppl %.2f gN %.2f %s, %s" %
nmt-master/nmt/train.py:374:      (prefix, global_step, info["learning_rate"], info["avg_step_time"],
nmt-master/nmt/train.py:375:       info["speed"], info["train_ppl"], info["avg_grad_norm"], result_summary,
nmt-master/nmt/train.py:376:       time.ctime()),
nmt-master/nmt/train.py:377:      log_f)
nmt-master/nmt/train.py:380:def add_info_summaries(summary_writer, global_step, info):
nmt-master/nmt/train.py:381:  """Add stuffs in info to summaries."""
nmt-master/nmt/train.py:382:  excluded_list = ["learning_rate"]
nmt-master/nmt/train.py:383:  for key in info:
nmt-master/nmt/train.py:384:    if key not in excluded_list:
nmt-master/nmt/train.py:385:      utils.add_summary(summary_writer, global_step, key, info[key])
nmt-master/nmt/train.py:388:def process_stats(stats, info, global_step, steps_per_stats, log_f):
nmt-master/nmt/train.py:389:  """Update info and check for overflow."""
nmt-master/nmt/train.py:390:  # Per-step info
nmt-master/nmt/train.py:391:  info["avg_step_time"] = stats["step_time"] / steps_per_stats
nmt-master/nmt/train.py:392:  info["avg_grad_norm"] = stats["grad_norm"] / steps_per_stats
nmt-master/nmt/train.py:393:  info["avg_sequence_count"] = stats["sequence_count"] / steps_per_stats
nmt-master/nmt/train.py:394:  info["speed"] = stats["word_count"] / (1000 * stats["step_time"])
nmt-master/nmt/train.py:396:  # Per-predict info
nmt-master/nmt/train.py:397:  info["train_ppl"] = (
nmt-master/nmt/train.py:398:      utils.safe_exp(stats["train_loss"] / stats["predict_count"]))
nmt-master/nmt/train.py:400:  # Check for overflow
nmt-master/nmt/train.py:401:  is_overflow = False
nmt-master/nmt/train.py:402:  train_ppl = info["train_ppl"]
nmt-master/nmt/train.py:403:  if math.isnan(train_ppl) or math.isinf(train_ppl) or train_ppl > 1e20:
nmt-master/nmt/train.py:404:    utils.print_out("  step %d overflow, stop early" % global_step,
nmt-master/nmt/train.py:405:                    log_f)
nmt-master/nmt/train.py:406:    is_overflow = True
nmt-master/nmt/train.py:408:  return is_overflow
nmt-master/nmt/train.py:411:def before_train(loaded_train_model, train_model, train_sess, global_step,
nmt-master/nmt/train.py:412:                 hparams, log_f):
nmt-master/nmt/train.py:413:  """Misc tasks to do before training."""
nmt-master/nmt/train.py:414:  stats = init_stats()
nmt-master/nmt/train.py:415:  info = {"train_ppl": 0.0, "speed": 0.0,
nmt-master/nmt/train.py:416:          "avg_step_time": 0.0,
nmt-master/nmt/train.py:417:          "avg_grad_norm": 0.0,
nmt-master/nmt/train.py:418:          "avg_sequence_count": 0.0,
nmt-master/nmt/train.py:419:          "learning_rate": loaded_train_model.learning_rate.eval(
nmt-master/nmt/train.py:420:              session=train_sess)}
nmt-master/nmt/train.py:421:  start_train_time = time.time()
nmt-master/nmt/train.py:422:  utils.print_out("# Start step %d, lr %g, %s" %
nmt-master/nmt/train.py:423:                  (global_step, info["learning_rate"], time.ctime()), log_f)
nmt-master/nmt/train.py:425:  # Initialize all of the iterators
nmt-master/nmt/train.py:426:  skip_count = hparams.batch_size * hparams.epoch_step
nmt-master/nmt/train.py:427:  utils.print_out("# Init train iterator, skipping %d elements" % skip_count)
nmt-master/nmt/train.py:428:  train_sess.run(
nmt-master/nmt/train.py:429:      train_model.iterator.initializer,
nmt-master/nmt/train.py:430:      feed_dict={train_model.skip_count_placeholder: skip_count})
nmt-master/nmt/train.py:432:  return stats, info, start_train_time
nmt-master/nmt/train.py:435:def get_model_creator(hparams):
nmt-master/nmt/train.py:436:  """Get the right model class depending on configuration."""
nmt-master/nmt/train.py:437:  if (hparams.encoder_type == "gnmt" or
nmt-master/nmt/train.py:438:      hparams.attention_architecture in ["gnmt", "gnmt_v2"]):
nmt-master/nmt/train.py:439:    model_creator = gnmt_model.GNMTModel
nmt-master/nmt/train.py:440:  elif hparams.attention and hparams.attention_architecture == "standard":
nmt-master/nmt/train.py:441:    model_creator = attention_model.AttentionModel
nmt-master/nmt/train.py:442:  elif not hparams.attention:
nmt-master/nmt/train.py:443:    model_creator = nmt_model.Model
nmt-master/nmt/train.py:444:  else:
nmt-master/nmt/train.py:445:    raise ValueError("Unknown attention architecture %s" %
nmt-master/nmt/train.py:446:                     hparams.attention_architecture)
nmt-master/nmt/train.py:447:  return model_creator
nmt-master/nmt/train.py:450:def train(hparams, scope=None, target_session=""):
nmt-master/nmt/train.py:451:  """Train a translation model."""
nmt-master/nmt/train.py:452:  log_device_placement = hparams.log_device_placement
nmt-master/nmt/train.py:453:  out_dir = hparams.out_dir
nmt-master/nmt/train.py:454:  num_train_steps = hparams.num_train_steps
nmt-master/nmt/train.py:455:  steps_per_stats = hparams.steps_per_stats
nmt-master/nmt/train.py:456:  steps_per_external_eval = hparams.steps_per_external_eval
nmt-master/nmt/train.py:457:  steps_per_eval = 10 * steps_per_stats
nmt-master/nmt/train.py:458:  avg_ckpts = hparams.avg_ckpts
nmt-master/nmt/train.py:460:  if not steps_per_external_eval:
nmt-master/nmt/train.py:461:    steps_per_external_eval = 5 * steps_per_eval
nmt-master/nmt/train.py:463:  # Create model
nmt-master/nmt/train.py:464:  model_creator = get_model_creator(hparams)
nmt-master/nmt/train.py:465:  train_model = model_helper.create_train_model(model_creator, hparams, scope)
nmt-master/nmt/train.py:466:  eval_model = model_helper.create_eval_model(model_creator, hparams, scope)
nmt-master/nmt/train.py:467:  infer_model = model_helper.create_infer_model(model_creator, hparams, scope)
nmt-master/nmt/train.py:469:  # Preload data for sample decoding.
nmt-master/nmt/train.py:470:  dev_src_file = "%s.%s" % (hparams.dev_prefix, hparams.src)
nmt-master/nmt/train.py:471:  dev_tgt_file = "%s.%s" % (hparams.dev_prefix, hparams.tgt)
nmt-master/nmt/train.py:472:  sample_src_data = inference.load_data(dev_src_file)
nmt-master/nmt/train.py:473:  sample_tgt_data = inference.load_data(dev_tgt_file)
nmt-master/nmt/train.py:475:  summary_name = "train_log"
nmt-master/nmt/train.py:476:  model_dir = hparams.out_dir
nmt-master/nmt/train.py:478:  # Log and output files
nmt-master/nmt/train.py:479:  log_file = os.path.join(out_dir, "log_%d" % time.time())
nmt-master/nmt/train.py:480:  log_f = tf.gfile.GFile(log_file, mode="a")
nmt-master/nmt/train.py:481:  utils.print_out("# log_file=%s" % log_file, log_f)
nmt-master/nmt/train.py:483:  # TensorFlow model
nmt-master/nmt/train.py:484:  config_proto = utils.get_config_proto(
nmt-master/nmt/train.py:485:      log_device_placement=log_device_placement,
nmt-master/nmt/train.py:486:      num_intra_threads=hparams.num_intra_threads,
nmt-master/nmt/train.py:487:      num_inter_threads=hparams.num_inter_threads)
nmt-master/nmt/train.py:488:  train_sess = tf.Session(
nmt-master/nmt/train.py:489:      target=target_session, config=config_proto, graph=train_model.graph)
nmt-master/nmt/train.py:490:  eval_sess = tf.Session(
nmt-master/nmt/train.py:491:      target=target_session, config=config_proto, graph=eval_model.graph)
nmt-master/nmt/train.py:492:  infer_sess = tf.Session(
nmt-master/nmt/train.py:493:      target=target_session, config=config_proto, graph=infer_model.graph)
nmt-master/nmt/train.py:495:  with train_model.graph.as_default():
nmt-master/nmt/train.py:496:    loaded_train_model, global_step = model_helper.create_or_load_model(
nmt-master/nmt/train.py:497:        train_model.model, model_dir, train_sess, "train")
nmt-master/nmt/train.py:499:  # Summary writer
nmt-master/nmt/train.py:500:  summary_writer = tf.summary.FileWriter(
nmt-master/nmt/train.py:501:      os.path.join(out_dir, summary_name), train_model.graph)
nmt-master/nmt/train.py:503:  # First evaluation
nmt-master/nmt/train.py:504:  run_full_eval(
nmt-master/nmt/train.py:505:      model_dir, infer_model, infer_sess,
nmt-master/nmt/train.py:506:      eval_model, eval_sess, hparams,
nmt-master/nmt/train.py:507:      summary_writer, sample_src_data,
nmt-master/nmt/train.py:508:      sample_tgt_data, avg_ckpts)
nmt-master/nmt/train.py:510:  last_stats_step = global_step
nmt-master/nmt/train.py:511:  last_eval_step = global_step
nmt-master/nmt/train.py:512:  last_external_eval_step = global_step
nmt-master/nmt/train.py:514:  # This is the training loop.
nmt-master/nmt/train.py:515:  stats, info, start_train_time = before_train(
nmt-master/nmt/train.py:516:      loaded_train_model, train_model, train_sess, global_step, hparams, log_f)
nmt-master/nmt/train.py:517:  while global_step < num_train_steps:
nmt-master/nmt/train.py:518:    ### Run a step ###
nmt-master/nmt/train.py:519:    start_time = time.time()
nmt-master/nmt/train.py:520:    try:
nmt-master/nmt/train.py:521:      step_result = loaded_train_model.train(train_sess)
nmt-master/nmt/train.py:522:      hparams.epoch_step += 1
nmt-master/nmt/train.py:523:    except tf.errors.OutOfRangeError:
nmt-master/nmt/train.py:524:      # Finished going through the training dataset.  Go to next epoch.
nmt-master/nmt/train.py:525:      hparams.epoch_step = 0
nmt-master/nmt/train.py:526:      utils.print_out(
nmt-master/nmt/train.py:527:          "# Finished an epoch, step %d. Perform external evaluation" %
nmt-master/nmt/train.py:528:          global_step)
nmt-master/nmt/train.py:529:      run_sample_decode(infer_model, infer_sess, model_dir, hparams,
nmt-master/nmt/train.py:530:                        summary_writer, sample_src_data, sample_tgt_data)
nmt-master/nmt/train.py:531:      run_external_eval(infer_model, infer_sess, model_dir, hparams,
nmt-master/nmt/train.py:532:                        summary_writer)
nmt-master/nmt/train.py:534:      if avg_ckpts:
nmt-master/nmt/train.py:535:        run_avg_external_eval(infer_model, infer_sess, model_dir, hparams,
nmt-master/nmt/train.py:536:                              summary_writer, global_step)
nmt-master/nmt/train.py:538:      train_sess.run(
nmt-master/nmt/train.py:539:          train_model.iterator.initializer,
nmt-master/nmt/train.py:540:          feed_dict={train_model.skip_count_placeholder: 0})
nmt-master/nmt/train.py:541:      continue
nmt-master/nmt/train.py:543:    # Process step_result, accumulate stats, and write summary
nmt-master/nmt/train.py:544:    global_step, info["learning_rate"], step_summary = update_stats(
nmt-master/nmt/train.py:545:        stats, start_time, step_result)
nmt-master/nmt/train.py:546:    summary_writer.add_summary(step_summary, global_step)
nmt-master/nmt/train.py:548:    # Once in a while, we print statistics.
nmt-master/nmt/train.py:549:    if global_step - last_stats_step >= steps_per_stats:
nmt-master/nmt/train.py:550:      last_stats_step = global_step
nmt-master/nmt/train.py:551:      is_overflow = process_stats(
nmt-master/nmt/train.py:552:          stats, info, global_step, steps_per_stats, log_f)
nmt-master/nmt/train.py:553:      print_step_info("  ", global_step, info, get_best_results(hparams),
nmt-master/nmt/train.py:554:                      log_f)
nmt-master/nmt/train.py:555:      if is_overflow:
nmt-master/nmt/train.py:556:        break
nmt-master/nmt/train.py:558:      # Reset statistics
nmt-master/nmt/train.py:559:      stats = init_stats()
nmt-master/nmt/train.py:561:    if global_step - last_eval_step >= steps_per_eval:
nmt-master/nmt/train.py:562:      last_eval_step = global_step
nmt-master/nmt/train.py:563:      utils.print_out("# Save eval, global step %d" % global_step)
nmt-master/nmt/train.py:564:      add_info_summaries(summary_writer, global_step, info)
nmt-master/nmt/train.py:566:      # Save checkpoint
nmt-master/nmt/train.py:567:      loaded_train_model.saver.save(
nmt-master/nmt/train.py:568:          train_sess,
nmt-master/nmt/train.py:569:          os.path.join(out_dir, "translate.ckpt"),
nmt-master/nmt/train.py:570:          global_step=global_step)
nmt-master/nmt/train.py:572:      # Evaluate on dev/test
nmt-master/nmt/train.py:573:      run_sample_decode(infer_model, infer_sess,
nmt-master/nmt/train.py:574:                        model_dir, hparams, summary_writer, sample_src_data,
nmt-master/nmt/train.py:575:                        sample_tgt_data)
nmt-master/nmt/train.py:576:      run_internal_eval(
nmt-master/nmt/train.py:577:          eval_model, eval_sess, model_dir, hparams, summary_writer)
nmt-master/nmt/train.py:579:    if global_step - last_external_eval_step >= steps_per_external_eval:
nmt-master/nmt/train.py:580:      last_external_eval_step = global_step
nmt-master/nmt/train.py:582:      # Save checkpoint
nmt-master/nmt/train.py:583:      loaded_train_model.saver.save(
nmt-master/nmt/train.py:584:          train_sess,
nmt-master/nmt/train.py:585:          os.path.join(out_dir, "translate.ckpt"),
nmt-master/nmt/train.py:586:          global_step=global_step)
nmt-master/nmt/train.py:587:      run_sample_decode(infer_model, infer_sess,
nmt-master/nmt/train.py:588:                        model_dir, hparams, summary_writer, sample_src_data,
nmt-master/nmt/train.py:589:                        sample_tgt_data)
nmt-master/nmt/train.py:590:      run_external_eval(
nmt-master/nmt/train.py:591:          infer_model, infer_sess, model_dir,
nmt-master/nmt/train.py:592:          hparams, summary_writer)
nmt-master/nmt/train.py:594:      if avg_ckpts:
nmt-master/nmt/train.py:595:        run_avg_external_eval(infer_model, infer_sess, model_dir, hparams,
nmt-master/nmt/train.py:596:                              summary_writer, global_step)
nmt-master/nmt/train.py:598:  # Done training
nmt-master/nmt/train.py:599:  loaded_train_model.saver.save(
nmt-master/nmt/train.py:600:      train_sess,
nmt-master/nmt/train.py:601:      os.path.join(out_dir, "translate.ckpt"),
nmt-master/nmt/train.py:602:      global_step=global_step)
nmt-master/nmt/train.py:604:  (result_summary, _, final_eval_metrics) = (
nmt-master/nmt/train.py:605:      run_full_eval(
nmt-master/nmt/train.py:606:          model_dir, infer_model, infer_sess, eval_model, eval_sess, hparams,
nmt-master/nmt/train.py:607:          summary_writer, sample_src_data, sample_tgt_data, avg_ckpts))
nmt-master/nmt/train.py:608:  print_step_info("# Final, ", global_step, info, result_summary, log_f)
nmt-master/nmt/train.py:609:  utils.print_time("# Done training!", start_train_time)
nmt-master/nmt/train.py:611:  summary_writer.close()
nmt-master/nmt/train.py:613:  utils.print_out("# Start evaluating saved best models.")
nmt-master/nmt/train.py:614:  for metric in hparams.metrics:
nmt-master/nmt/train.py:615:    best_model_dir = getattr(hparams, "best_" + metric + "_dir")
nmt-master/nmt/train.py:616:    summary_writer = tf.summary.FileWriter(
nmt-master/nmt/train.py:617:        os.path.join(best_model_dir, summary_name), infer_model.graph)
nmt-master/nmt/train.py:618:    result_summary, best_global_step, _ = run_full_eval(
nmt-master/nmt/train.py:619:        best_model_dir, infer_model, infer_sess, eval_model, eval_sess, hparams,
nmt-master/nmt/train.py:620:        summary_writer, sample_src_data, sample_tgt_data)
nmt-master/nmt/train.py:621:    print_step_info("# Best %s, " % metric, best_global_step, info,
nmt-master/nmt/train.py:622:                    result_summary, log_f)
nmt-master/nmt/train.py:623:    summary_writer.close()
nmt-master/nmt/train.py:625:    if avg_ckpts:
nmt-master/nmt/train.py:626:      best_model_dir = getattr(hparams, "avg_best_" + metric + "_dir")
nmt-master/nmt/train.py:627:      summary_writer = tf.summary.FileWriter(
nmt-master/nmt/train.py:628:          os.path.join(best_model_dir, summary_name), infer_model.graph)
nmt-master/nmt/train.py:629:      result_summary, best_global_step, _ = run_full_eval(
nmt-master/nmt/train.py:630:          best_model_dir, infer_model, infer_sess, eval_model, eval_sess,
nmt-master/nmt/train.py:631:          hparams, summary_writer, sample_src_data, sample_tgt_data)
nmt-master/nmt/train.py:632:      print_step_info("# Averaged Best %s, " % metric, best_global_step, info,
nmt-master/nmt/train.py:633:                      result_summary, log_f)
nmt-master/nmt/train.py:634:      summary_writer.close()
nmt-master/nmt/train.py:636:  return final_eval_metrics, global_step
nmt-master/nmt/train.py:639:def _format_results(name, ppl, scores, metrics):
nmt-master/nmt/train.py:640:  """Format results."""
nmt-master/nmt/train.py:641:  result_str = ""
nmt-master/nmt/train.py:642:  if ppl:
nmt-master/nmt/train.py:643:    result_str = "%s ppl %.2f" % (name, ppl)
nmt-master/nmt/train.py:644:  if scores:
nmt-master/nmt/train.py:645:    for metric in metrics:
nmt-master/nmt/train.py:646:      if result_str:
nmt-master/nmt/train.py:647:        result_str += ", %s %s %.1f" % (name, metric, scores[metric])
nmt-master/nmt/train.py:648:      else:
nmt-master/nmt/train.py:649:        result_str = "%s %s %.1f" % (name, metric, scores[metric])
nmt-master/nmt/train.py:650:  return result_str
nmt-master/nmt/train.py:653:def get_best_results(hparams):
nmt-master/nmt/train.py:654:  """Summary of the current best results."""
nmt-master/nmt/train.py:655:  tokens = []
nmt-master/nmt/train.py:656:  for metric in hparams.metrics:
nmt-master/nmt/train.py:657:    tokens.append("%s %.2f" % (metric, getattr(hparams, "best_" + metric)))
nmt-master/nmt/train.py:658:  return ", ".join(tokens)
nmt-master/nmt/train.py:661:def _internal_eval(model, global_step, sess, iterator, iterator_feed_dict,
nmt-master/nmt/train.py:662:                   summary_writer, label):
nmt-master/nmt/train.py:663:  """Computing perplexity."""
nmt-master/nmt/train.py:664:  sess.run(iterator.initializer, feed_dict=iterator_feed_dict)
nmt-master/nmt/train.py:665:  ppl = model_helper.compute_perplexity(model, sess, label)
nmt-master/nmt/train.py:666:  utils.add_summary(summary_writer, global_step, "%s_ppl" % label, ppl)
nmt-master/nmt/train.py:667:  return ppl
nmt-master/nmt/train.py:670:def _sample_decode(model, global_step, sess, hparams, iterator, src_data,
nmt-master/nmt/train.py:671:                   tgt_data, iterator_src_placeholder,
nmt-master/nmt/train.py:672:                   iterator_batch_size_placeholder, summary_writer):
nmt-master/nmt/train.py:673:  """Pick a sentence and decode."""
nmt-master/nmt/train.py:674:  decode_id = random.randint(0, len(src_data) - 1)
nmt-master/nmt/train.py:675:  utils.print_out("  # %d" % decode_id)
nmt-master/nmt/train.py:677:  iterator_feed_dict = {
nmt-master/nmt/train.py:678:      iterator_src_placeholder: [src_data[decode_id]],
nmt-master/nmt/train.py:679:      iterator_batch_size_placeholder: 1,
nmt-master/nmt/train.py:680:  }
nmt-master/nmt/train.py:681:  sess.run(iterator.initializer, feed_dict=iterator_feed_dict)
nmt-master/nmt/train.py:683:  nmt_outputs, attention_summary = model.decode(sess)
nmt-master/nmt/train.py:685:  if hparams.infer_mode == "beam_search":
nmt-master/nmt/train.py:686:    # get the top translation.
nmt-master/nmt/train.py:687:    nmt_outputs = nmt_outputs[0]
nmt-master/nmt/train.py:689:  translation = nmt_utils.get_translation(
nmt-master/nmt/train.py:690:      nmt_outputs,
nmt-master/nmt/train.py:691:      sent_id=0,
nmt-master/nmt/train.py:692:      tgt_eos=hparams.eos,
nmt-master/nmt/train.py:693:      subword_option=hparams.subword_option)
nmt-master/nmt/train.py:694:  utils.print_out("    src: %s" % src_data[decode_id])
nmt-master/nmt/train.py:695:  utils.print_out("    ref: %s" % tgt_data[decode_id])
nmt-master/nmt/train.py:696:  utils.print_out(b"    nmt: " + translation)
nmt-master/nmt/train.py:698:  # Summary
nmt-master/nmt/train.py:699:  if attention_summary is not None:
nmt-master/nmt/train.py:700:    summary_writer.add_summary(attention_summary, global_step)
nmt-master/nmt/train.py:703:def _external_eval(model, global_step, sess, hparams, iterator,
nmt-master/nmt/train.py:704:                   iterator_feed_dict, tgt_file, label, summary_writer,
nmt-master/nmt/train.py:705:                   save_on_best, avg_ckpts=False):
nmt-master/nmt/train.py:706:  """External evaluation such as BLEU and ROUGE scores."""
nmt-master/nmt/train.py:707:  out_dir = hparams.out_dir
nmt-master/nmt/train.py:708:  decode = global_step > 0
nmt-master/nmt/train.py:710:  if avg_ckpts:
nmt-master/nmt/train.py:711:    label = "avg_" + label
nmt-master/nmt/train.py:713:  if decode:
nmt-master/nmt/train.py:714:    utils.print_out("# External evaluation, global step %d" % global_step)
nmt-master/nmt/train.py:716:  sess.run(iterator.initializer, feed_dict=iterator_feed_dict)
nmt-master/nmt/train.py:718:  output = os.path.join(out_dir, "output_%s" % label)
nmt-master/nmt/train.py:719:  scores = nmt_utils.decode_and_evaluate(
nmt-master/nmt/train.py:720:      label,
nmt-master/nmt/train.py:721:      model,
nmt-master/nmt/train.py:722:      sess,
nmt-master/nmt/train.py:723:      output,
nmt-master/nmt/train.py:724:      ref_file=tgt_file,
nmt-master/nmt/train.py:725:      metrics=hparams.metrics,
nmt-master/nmt/train.py:726:      subword_option=hparams.subword_option,
nmt-master/nmt/train.py:727:      beam_width=hparams.beam_width,
nmt-master/nmt/train.py:728:      tgt_eos=hparams.eos,
nmt-master/nmt/train.py:729:      decode=decode,
nmt-master/nmt/train.py:730:      infer_mode=hparams.infer_mode)
nmt-master/nmt/train.py:731:  # Save on best metrics
nmt-master/nmt/train.py:732:  if decode:
nmt-master/nmt/train.py:733:    for metric in hparams.metrics:
nmt-master/nmt/train.py:734:      if avg_ckpts:
nmt-master/nmt/train.py:735:        best_metric_label = "avg_best_" + metric
nmt-master/nmt/train.py:736:      else:
nmt-master/nmt/train.py:737:        best_metric_label = "best_" + metric
nmt-master/nmt/train.py:739:      utils.add_summary(summary_writer, global_step, "%s_%s" % (label, metric),
nmt-master/nmt/train.py:740:                        scores[metric])
nmt-master/nmt/train.py:741:      # metric: larger is better
nmt-master/nmt/train.py:742:      if save_on_best and scores[metric] > getattr(hparams, best_metric_label):
nmt-master/nmt/train.py:743:        setattr(hparams, best_metric_label, scores[metric])
nmt-master/nmt/train.py:744:        model.saver.save(
nmt-master/nmt/train.py:745:            sess,
nmt-master/nmt/train.py:746:            os.path.join(
nmt-master/nmt/train.py:747:                getattr(hparams, best_metric_label + "_dir"), "translate.ckpt"),
nmt-master/nmt/train.py:748:            global_step=model.global_step)
nmt-master/nmt/train.py:749:    utils.save_hparams(out_dir, hparams)
nmt-master/nmt/train.py:750:  return scores
nmt-master/nmt/utils/common_test_utils.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/utils/common_test_utils.py:2:#
nmt-master/nmt/utils/common_test_utils.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/utils/common_test_utils.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/utils/common_test_utils.py:5:# You may obtain a copy of the License at
nmt-master/nmt/utils/common_test_utils.py:6:#
nmt-master/nmt/utils/common_test_utils.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/utils/common_test_utils.py:8:#
nmt-master/nmt/utils/common_test_utils.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/utils/common_test_utils.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/utils/common_test_utils.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/utils/common_test_utils.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/utils/common_test_utils.py:13:# limitations under the License.
nmt-master/nmt/utils/common_test_utils.py:14:# ==============================================================================
nmt-master/nmt/utils/common_test_utils.py:16:"""Common utility functions for tests."""
nmt-master/nmt/utils/common_test_utils.py:18:from __future__ import absolute_import
nmt-master/nmt/utils/common_test_utils.py:19:from __future__ import division
nmt-master/nmt/utils/common_test_utils.py:20:from __future__ import print_function
nmt-master/nmt/utils/common_test_utils.py:22:import tensorflow as tf
nmt-master/nmt/utils/common_test_utils.py:24:from tensorflow.python.ops import lookup_ops
nmt-master/nmt/utils/common_test_utils.py:26:from ..utils import iterator_utils
nmt-master/nmt/utils/common_test_utils.py:27:from ..utils import standard_hparams_utils
nmt-master/nmt/utils/common_test_utils.py:30:def create_test_hparams(unit_type="lstm",
nmt-master/nmt/utils/common_test_utils.py:31:                        encoder_type="uni",
nmt-master/nmt/utils/common_test_utils.py:32:                        num_layers=4,
nmt-master/nmt/utils/common_test_utils.py:33:                        attention="",
nmt-master/nmt/utils/common_test_utils.py:34:                        attention_architecture=None,
nmt-master/nmt/utils/common_test_utils.py:35:                        use_residual=False,
nmt-master/nmt/utils/common_test_utils.py:36:                        inference_indices=None,
nmt-master/nmt/utils/common_test_utils.py:37:                        num_translations_per_input=1,
nmt-master/nmt/utils/common_test_utils.py:38:                        beam_width=0,
nmt-master/nmt/utils/common_test_utils.py:39:                        init_op="uniform"):
nmt-master/nmt/utils/common_test_utils.py:40:  """Create training and inference test hparams."""
nmt-master/nmt/utils/common_test_utils.py:41:  num_residual_layers = 0
nmt-master/nmt/utils/common_test_utils.py:42:  if use_residual:
nmt-master/nmt/utils/common_test_utils.py:43:    # TODO(rzhao): Put num_residual_layers computation logic into
nmt-master/nmt/utils/common_test_utils.py:44:    # `model_utils.py`, so we can also test it here.
nmt-master/nmt/utils/common_test_utils.py:45:    num_residual_layers = 2
nmt-master/nmt/utils/common_test_utils.py:47:  standard_hparams = standard_hparams_utils.create_standard_hparams()
nmt-master/nmt/utils/common_test_utils.py:49:  # Networks
nmt-master/nmt/utils/common_test_utils.py:50:  standard_hparams.num_units = 5
nmt-master/nmt/utils/common_test_utils.py:51:  standard_hparams.num_encoder_layers = num_layers
nmt-master/nmt/utils/common_test_utils.py:52:  standard_hparams.num_decoder_layers = num_layers
nmt-master/nmt/utils/common_test_utils.py:53:  standard_hparams.dropout = 0.5
nmt-master/nmt/utils/common_test_utils.py:54:  standard_hparams.unit_type = unit_type
nmt-master/nmt/utils/common_test_utils.py:55:  standard_hparams.encoder_type = encoder_type
nmt-master/nmt/utils/common_test_utils.py:56:  standard_hparams.residual = use_residual
nmt-master/nmt/utils/common_test_utils.py:57:  standard_hparams.num_residual_layers = num_residual_layers
nmt-master/nmt/utils/common_test_utils.py:59:  # Attention mechanisms
nmt-master/nmt/utils/common_test_utils.py:60:  standard_hparams.attention = attention
nmt-master/nmt/utils/common_test_utils.py:61:  standard_hparams.attention_architecture = attention_architecture
nmt-master/nmt/utils/common_test_utils.py:63:  # Train
nmt-master/nmt/utils/common_test_utils.py:64:  standard_hparams.init_op = init_op
nmt-master/nmt/utils/common_test_utils.py:65:  standard_hparams.num_train_steps = 1
nmt-master/nmt/utils/common_test_utils.py:66:  standard_hparams.decay_scheme = ""
nmt-master/nmt/utils/common_test_utils.py:68:  # Infer
nmt-master/nmt/utils/common_test_utils.py:69:  standard_hparams.tgt_max_len_infer = 100
nmt-master/nmt/utils/common_test_utils.py:70:  standard_hparams.beam_width = beam_width
nmt-master/nmt/utils/common_test_utils.py:71:  standard_hparams.num_translations_per_input = num_translations_per_input
nmt-master/nmt/utils/common_test_utils.py:73:  # Misc
nmt-master/nmt/utils/common_test_utils.py:74:  standard_hparams.forget_bias = 0.0
nmt-master/nmt/utils/common_test_utils.py:75:  standard_hparams.random_seed = 3
nmt-master/nmt/utils/common_test_utils.py:76:  standard_hparams.language_model = False
nmt-master/nmt/utils/common_test_utils.py:78:  # Vocab
nmt-master/nmt/utils/common_test_utils.py:79:  standard_hparams.src_vocab_size = 5
nmt-master/nmt/utils/common_test_utils.py:80:  standard_hparams.tgt_vocab_size = 5
nmt-master/nmt/utils/common_test_utils.py:81:  standard_hparams.eos = "</s>"
nmt-master/nmt/utils/common_test_utils.py:82:  standard_hparams.sos = "<s>"
nmt-master/nmt/utils/common_test_utils.py:83:  standard_hparams.src_vocab_file = ""
nmt-master/nmt/utils/common_test_utils.py:84:  standard_hparams.tgt_vocab_file = ""
nmt-master/nmt/utils/common_test_utils.py:85:  standard_hparams.src_embed_file = ""
nmt-master/nmt/utils/common_test_utils.py:86:  standard_hparams.tgt_embed_file = ""
nmt-master/nmt/utils/common_test_utils.py:88:  # For inference.py test
nmt-master/nmt/utils/common_test_utils.py:89:  standard_hparams.subword_option = "bpe"
nmt-master/nmt/utils/common_test_utils.py:90:  standard_hparams.src = "src"
nmt-master/nmt/utils/common_test_utils.py:91:  standard_hparams.tgt = "tgt"
nmt-master/nmt/utils/common_test_utils.py:92:  standard_hparams.src_max_len = 400
nmt-master/nmt/utils/common_test_utils.py:93:  standard_hparams.tgt_eos_id = 0
nmt-master/nmt/utils/common_test_utils.py:94:  standard_hparams.inference_indices = inference_indices
nmt-master/nmt/utils/common_test_utils.py:95:  return standard_hparams
nmt-master/nmt/utils/common_test_utils.py:98:def create_test_iterator(hparams, mode):
nmt-master/nmt/utils/common_test_utils.py:99:  """Create test iterator."""
nmt-master/nmt/utils/common_test_utils.py:100:  src_vocab_table = lookup_ops.index_table_from_tensor(
nmt-master/nmt/utils/common_test_utils.py:101:      tf.constant([hparams.eos, "a", "b", "c", "d"]))
nmt-master/nmt/utils/common_test_utils.py:102:  tgt_vocab_mapping = tf.constant([hparams.sos, hparams.eos, "a", "b", "c"])
nmt-master/nmt/utils/common_test_utils.py:103:  tgt_vocab_table = lookup_ops.index_table_from_tensor(tgt_vocab_mapping)
nmt-master/nmt/utils/common_test_utils.py:104:  if mode == tf.contrib.learn.ModeKeys.INFER:
nmt-master/nmt/utils/common_test_utils.py:105:    reverse_tgt_vocab_table = lookup_ops.index_to_string_table_from_tensor(
nmt-master/nmt/utils/common_test_utils.py:106:        tgt_vocab_mapping)
nmt-master/nmt/utils/common_test_utils.py:108:  src_dataset = tf.data.Dataset.from_tensor_slices(
nmt-master/nmt/utils/common_test_utils.py:109:      tf.constant(["a a b b c", "a b b"]))
nmt-master/nmt/utils/common_test_utils.py:111:  if mode != tf.contrib.learn.ModeKeys.INFER:
nmt-master/nmt/utils/common_test_utils.py:112:    tgt_dataset = tf.data.Dataset.from_tensor_slices(
nmt-master/nmt/utils/common_test_utils.py:113:        tf.constant(["a b c b c", "a b c b"]))
nmt-master/nmt/utils/common_test_utils.py:114:    return (
nmt-master/nmt/utils/common_test_utils.py:115:        iterator_utils.get_iterator(
nmt-master/nmt/utils/common_test_utils.py:116:            src_dataset=src_dataset,
nmt-master/nmt/utils/common_test_utils.py:117:            tgt_dataset=tgt_dataset,
nmt-master/nmt/utils/common_test_utils.py:118:            src_vocab_table=src_vocab_table,
nmt-master/nmt/utils/common_test_utils.py:119:            tgt_vocab_table=tgt_vocab_table,
nmt-master/nmt/utils/common_test_utils.py:120:            batch_size=hparams.batch_size,
nmt-master/nmt/utils/common_test_utils.py:121:            sos=hparams.sos,
nmt-master/nmt/utils/common_test_utils.py:122:            eos=hparams.eos,
nmt-master/nmt/utils/common_test_utils.py:123:            random_seed=hparams.random_seed,
nmt-master/nmt/utils/common_test_utils.py:124:            num_buckets=hparams.num_buckets),
nmt-master/nmt/utils/common_test_utils.py:125:        src_vocab_table,
nmt-master/nmt/utils/common_test_utils.py:126:        tgt_vocab_table)
nmt-master/nmt/utils/common_test_utils.py:127:  else:
nmt-master/nmt/utils/common_test_utils.py:128:    return (
nmt-master/nmt/utils/common_test_utils.py:129:        iterator_utils.get_infer_iterator(
nmt-master/nmt/utils/common_test_utils.py:130:            src_dataset=src_dataset,
nmt-master/nmt/utils/common_test_utils.py:131:            src_vocab_table=src_vocab_table,
nmt-master/nmt/utils/common_test_utils.py:132:            eos=hparams.eos,
nmt-master/nmt/utils/common_test_utils.py:133:            batch_size=hparams.batch_size),
nmt-master/nmt/utils/common_test_utils.py:134:        src_vocab_table,
nmt-master/nmt/utils/common_test_utils.py:135:        tgt_vocab_table,
nmt-master/nmt/utils/common_test_utils.py:136:        reverse_tgt_vocab_table)
nmt-master/nmt/utils/evaluation_utils.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/utils/evaluation_utils.py:2:#
nmt-master/nmt/utils/evaluation_utils.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/utils/evaluation_utils.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/utils/evaluation_utils.py:5:# You may obtain a copy of the License at
nmt-master/nmt/utils/evaluation_utils.py:6:#
nmt-master/nmt/utils/evaluation_utils.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/utils/evaluation_utils.py:8:#
nmt-master/nmt/utils/evaluation_utils.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/utils/evaluation_utils.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/utils/evaluation_utils.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/utils/evaluation_utils.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/utils/evaluation_utils.py:13:# limitations under the License.
nmt-master/nmt/utils/evaluation_utils.py:14:# ==============================================================================
nmt-master/nmt/utils/evaluation_utils.py:16:"""Utility for evaluating various tasks, e.g., translation & summarization."""
nmt-master/nmt/utils/evaluation_utils.py:17:import codecs
nmt-master/nmt/utils/evaluation_utils.py:18:import os
nmt-master/nmt/utils/evaluation_utils.py:19:import re
nmt-master/nmt/utils/evaluation_utils.py:20:import subprocess
nmt-master/nmt/utils/evaluation_utils.py:22:import tensorflow as tf
nmt-master/nmt/utils/evaluation_utils.py:24:from ..scripts import bleu
nmt-master/nmt/utils/evaluation_utils.py:25:from ..scripts import rouge
nmt-master/nmt/utils/evaluation_utils.py:28:__all__ = ["evaluate"]
nmt-master/nmt/utils/evaluation_utils.py:31:def evaluate(ref_file, trans_file, metric, subword_option=None):
nmt-master/nmt/utils/evaluation_utils.py:32:  """Pick a metric and evaluate depending on task."""
nmt-master/nmt/utils/evaluation_utils.py:33:  # BLEU scores for translation task
nmt-master/nmt/utils/evaluation_utils.py:34:  if metric.lower() == "bleu":
nmt-master/nmt/utils/evaluation_utils.py:35:    evaluation_score = _bleu(ref_file, trans_file,
nmt-master/nmt/utils/evaluation_utils.py:36:                             subword_option=subword_option)
nmt-master/nmt/utils/evaluation_utils.py:37:  # ROUGE scores for summarization tasks
nmt-master/nmt/utils/evaluation_utils.py:38:  elif metric.lower() == "rouge":
nmt-master/nmt/utils/evaluation_utils.py:39:    evaluation_score = _rouge(ref_file, trans_file,
nmt-master/nmt/utils/evaluation_utils.py:40:                              subword_option=subword_option)
nmt-master/nmt/utils/evaluation_utils.py:41:  elif metric.lower() == "accuracy":
nmt-master/nmt/utils/evaluation_utils.py:42:    evaluation_score = _accuracy(ref_file, trans_file)
nmt-master/nmt/utils/evaluation_utils.py:43:  elif metric.lower() == "word_accuracy":
nmt-master/nmt/utils/evaluation_utils.py:44:    evaluation_score = _word_accuracy(ref_file, trans_file)
nmt-master/nmt/utils/evaluation_utils.py:45:  else:
nmt-master/nmt/utils/evaluation_utils.py:46:    raise ValueError("Unknown metric %s" % metric)
nmt-master/nmt/utils/evaluation_utils.py:48:  return evaluation_score
nmt-master/nmt/utils/evaluation_utils.py:51:def _clean(sentence, subword_option):
nmt-master/nmt/utils/evaluation_utils.py:52:  """Clean and handle BPE or SPM outputs."""
nmt-master/nmt/utils/evaluation_utils.py:53:  sentence = sentence.strip()
nmt-master/nmt/utils/evaluation_utils.py:55:  # BPE
nmt-master/nmt/utils/evaluation_utils.py:56:  if subword_option == "bpe":
nmt-master/nmt/utils/evaluation_utils.py:57:    sentence = re.sub("@@ ", "", sentence)
nmt-master/nmt/utils/evaluation_utils.py:59:  # SPM
nmt-master/nmt/utils/evaluation_utils.py:60:  elif subword_option == "spm":
nmt-master/nmt/utils/evaluation_utils.py:61:    sentence = u"".join(sentence.split()).replace(u"\u2581", u" ").lstrip()
nmt-master/nmt/utils/evaluation_utils.py:63:  return sentence
nmt-master/nmt/utils/evaluation_utils.py:66:# Follow //transconsole/localization/machine_translation/metrics/bleu_calc.py
nmt-master/nmt/utils/evaluation_utils.py:67:def _bleu(ref_file, trans_file, subword_option=None):
nmt-master/nmt/utils/evaluation_utils.py:68:  """Compute BLEU scores and handling BPE."""
nmt-master/nmt/utils/evaluation_utils.py:69:  max_order = 4
nmt-master/nmt/utils/evaluation_utils.py:70:  smooth = False
nmt-master/nmt/utils/evaluation_utils.py:72:  ref_files = [ref_file]
nmt-master/nmt/utils/evaluation_utils.py:73:  reference_text = []
nmt-master/nmt/utils/evaluation_utils.py:74:  for reference_filename in ref_files:
nmt-master/nmt/utils/evaluation_utils.py:75:    with codecs.getreader("utf-8")(
nmt-master/nmt/utils/evaluation_utils.py:76:        tf.gfile.GFile(reference_filename, "rb")) as fh:
nmt-master/nmt/utils/evaluation_utils.py:77:      reference_text.append(fh.readlines())
nmt-master/nmt/utils/evaluation_utils.py:79:  per_segment_references = []
nmt-master/nmt/utils/evaluation_utils.py:80:  for references in zip(*reference_text):
nmt-master/nmt/utils/evaluation_utils.py:81:    reference_list = []
nmt-master/nmt/utils/evaluation_utils.py:82:    for reference in references:
nmt-master/nmt/utils/evaluation_utils.py:83:      reference = _clean(reference, subword_option)
nmt-master/nmt/utils/evaluation_utils.py:84:      reference_list.append(reference.split(" "))
nmt-master/nmt/utils/evaluation_utils.py:85:    per_segment_references.append(reference_list)
nmt-master/nmt/utils/evaluation_utils.py:87:  translations = []
nmt-master/nmt/utils/evaluation_utils.py:88:  with codecs.getreader("utf-8")(tf.gfile.GFile(trans_file, "rb")) as fh:
nmt-master/nmt/utils/evaluation_utils.py:89:    for line in fh:
nmt-master/nmt/utils/evaluation_utils.py:90:      line = _clean(line, subword_option=None)
nmt-master/nmt/utils/evaluation_utils.py:91:      translations.append(line.split(" "))
nmt-master/nmt/utils/evaluation_utils.py:93:  # bleu_score, precisions, bp, ratio, translation_length, reference_length
nmt-master/nmt/utils/evaluation_utils.py:94:  bleu_score, _, _, _, _, _ = bleu.compute_bleu(
nmt-master/nmt/utils/evaluation_utils.py:95:      per_segment_references, translations, max_order, smooth)
nmt-master/nmt/utils/evaluation_utils.py:96:  return 100 * bleu_score
nmt-master/nmt/utils/evaluation_utils.py:99:def _rouge(ref_file, summarization_file, subword_option=None):
nmt-master/nmt/utils/evaluation_utils.py:100:  """Compute ROUGE scores and handling BPE."""
nmt-master/nmt/utils/evaluation_utils.py:102:  references = []
nmt-master/nmt/utils/evaluation_utils.py:103:  with codecs.getreader("utf-8")(tf.gfile.GFile(ref_file, "rb")) as fh:
nmt-master/nmt/utils/evaluation_utils.py:104:    for line in fh:
nmt-master/nmt/utils/evaluation_utils.py:105:      references.append(_clean(line, subword_option))
nmt-master/nmt/utils/evaluation_utils.py:107:  hypotheses = []
nmt-master/nmt/utils/evaluation_utils.py:108:  with codecs.getreader("utf-8")(
nmt-master/nmt/utils/evaluation_utils.py:109:      tf.gfile.GFile(summarization_file, "rb")) as fh:
nmt-master/nmt/utils/evaluation_utils.py:110:    for line in fh:
nmt-master/nmt/utils/evaluation_utils.py:111:      hypotheses.append(_clean(line, subword_option=None))
nmt-master/nmt/utils/evaluation_utils.py:113:  rouge_score_map = rouge.rouge(hypotheses, references)
nmt-master/nmt/utils/evaluation_utils.py:114:  return 100 * rouge_score_map["rouge_l/f_score"]
nmt-master/nmt/utils/evaluation_utils.py:117:def _accuracy(label_file, pred_file):
nmt-master/nmt/utils/evaluation_utils.py:118:  """Compute accuracy, each line contains a label."""
nmt-master/nmt/utils/evaluation_utils.py:120:  with codecs.getreader("utf-8")(tf.gfile.GFile(label_file, "rb")) as label_fh:
nmt-master/nmt/utils/evaluation_utils.py:121:    with codecs.getreader("utf-8")(tf.gfile.GFile(pred_file, "rb")) as pred_fh:
nmt-master/nmt/utils/evaluation_utils.py:122:      count = 0.0
nmt-master/nmt/utils/evaluation_utils.py:123:      match = 0.0
nmt-master/nmt/utils/evaluation_utils.py:124:      for label in label_fh:
nmt-master/nmt/utils/evaluation_utils.py:125:        label = label.strip()
nmt-master/nmt/utils/evaluation_utils.py:126:        pred = pred_fh.readline().strip()
nmt-master/nmt/utils/evaluation_utils.py:127:        if label == pred:
nmt-master/nmt/utils/evaluation_utils.py:128:          match += 1
nmt-master/nmt/utils/evaluation_utils.py:129:        count += 1
nmt-master/nmt/utils/evaluation_utils.py:130:  return 100 * match / count
nmt-master/nmt/utils/evaluation_utils.py:133:def _word_accuracy(label_file, pred_file):
nmt-master/nmt/utils/evaluation_utils.py:134:  """Compute accuracy on per word basis."""
nmt-master/nmt/utils/evaluation_utils.py:136:  with codecs.getreader("utf-8")(tf.gfile.GFile(label_file, "rb")) as label_fh:
nmt-master/nmt/utils/evaluation_utils.py:137:    with codecs.getreader("utf-8")(tf.gfile.GFile(pred_file, "rb")) as pred_fh:
nmt-master/nmt/utils/evaluation_utils.py:138:      total_acc, total_count = 0., 0.
nmt-master/nmt/utils/evaluation_utils.py:139:      for sentence in label_fh:
nmt-master/nmt/utils/evaluation_utils.py:140:        labels = sentence.strip().split(" ")
nmt-master/nmt/utils/evaluation_utils.py:141:        preds = pred_fh.readline().strip().split(" ")
nmt-master/nmt/utils/evaluation_utils.py:142:        match = 0.0
nmt-master/nmt/utils/evaluation_utils.py:143:        for pos in range(min(len(labels), len(preds))):
nmt-master/nmt/utils/evaluation_utils.py:144:          label = labels[pos]
nmt-master/nmt/utils/evaluation_utils.py:145:          pred = preds[pos]
nmt-master/nmt/utils/evaluation_utils.py:146:          if label == pred:
nmt-master/nmt/utils/evaluation_utils.py:147:            match += 1
nmt-master/nmt/utils/evaluation_utils.py:148:        total_acc += 100 * match / max(len(labels), len(preds))
nmt-master/nmt/utils/evaluation_utils.py:149:        total_count += 1
nmt-master/nmt/utils/evaluation_utils.py:150:  return total_acc / total_count
nmt-master/nmt/utils/evaluation_utils.py:153:def _moses_bleu(multi_bleu_script, tgt_test, trans_file, subword_option=None):
nmt-master/nmt/utils/evaluation_utils.py:154:  """Compute BLEU scores using Moses multi-bleu.perl script."""
nmt-master/nmt/utils/evaluation_utils.py:156:  # TODO(thangluong): perform rewrite using python
nmt-master/nmt/utils/evaluation_utils.py:157:  # BPE
nmt-master/nmt/utils/evaluation_utils.py:158:  if subword_option == "bpe":
nmt-master/nmt/utils/evaluation_utils.py:159:    debpe_tgt_test = tgt_test + ".debpe"
nmt-master/nmt/utils/evaluation_utils.py:160:    if not os.path.exists(debpe_tgt_test):
nmt-master/nmt/utils/evaluation_utils.py:161:      # TODO(thangluong): not use shell=True, can be a security hazard
nmt-master/nmt/utils/evaluation_utils.py:162:      subprocess.call("cp %s %s" % (tgt_test, debpe_tgt_test), shell=True)
nmt-master/nmt/utils/evaluation_utils.py:163:      subprocess.call("sed s/@@ //g %s" % (debpe_tgt_test),
nmt-master/nmt/utils/evaluation_utils.py:164:                      shell=True)
nmt-master/nmt/utils/evaluation_utils.py:165:    tgt_test = debpe_tgt_test
nmt-master/nmt/utils/evaluation_utils.py:166:  elif subword_option == "spm":
nmt-master/nmt/utils/evaluation_utils.py:167:    despm_tgt_test = tgt_test + ".despm"
nmt-master/nmt/utils/evaluation_utils.py:168:    if not os.path.exists(despm_tgt_test):
nmt-master/nmt/utils/evaluation_utils.py:169:      subprocess.call("cp %s %s" % (tgt_test, despm_tgt_test))
nmt-master/nmt/utils/evaluation_utils.py:170:      subprocess.call("sed s/ //g %s" % (despm_tgt_test))
nmt-master/nmt/utils/evaluation_utils.py:171:      subprocess.call(u"sed s/^\u2581/g %s" % (despm_tgt_test))
nmt-master/nmt/utils/evaluation_utils.py:172:      subprocess.call(u"sed s/\u2581/ /g %s" % (despm_tgt_test))
nmt-master/nmt/utils/evaluation_utils.py:173:    tgt_test = despm_tgt_test
nmt-master/nmt/utils/evaluation_utils.py:174:  cmd = "%s %s < %s" % (multi_bleu_script, tgt_test, trans_file)
nmt-master/nmt/utils/evaluation_utils.py:176:  # subprocess
nmt-master/nmt/utils/evaluation_utils.py:177:  # TODO(thangluong): not use shell=True, can be a security hazard
nmt-master/nmt/utils/evaluation_utils.py:178:  bleu_output = subprocess.check_output(cmd, shell=True)
nmt-master/nmt/utils/evaluation_utils.py:180:  # extract BLEU score
nmt-master/nmt/utils/evaluation_utils.py:181:  m = re.search("BLEU = (.+?),", bleu_output)
nmt-master/nmt/utils/evaluation_utils.py:182:  bleu_score = float(m.group(1))
nmt-master/nmt/utils/evaluation_utils.py:184:  return bleu_score
nmt-master/nmt/utils/evaluation_utils_test.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/utils/evaluation_utils_test.py:2:#
nmt-master/nmt/utils/evaluation_utils_test.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/utils/evaluation_utils_test.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/utils/evaluation_utils_test.py:5:# You may obtain a copy of the License at
nmt-master/nmt/utils/evaluation_utils_test.py:6:#
nmt-master/nmt/utils/evaluation_utils_test.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/utils/evaluation_utils_test.py:8:#
nmt-master/nmt/utils/evaluation_utils_test.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/utils/evaluation_utils_test.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/utils/evaluation_utils_test.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/utils/evaluation_utils_test.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/utils/evaluation_utils_test.py:13:# limitations under the License.
nmt-master/nmt/utils/evaluation_utils_test.py:14:# ==============================================================================
nmt-master/nmt/utils/evaluation_utils_test.py:16:"""Tests for evaluation_utils.py."""
nmt-master/nmt/utils/evaluation_utils_test.py:18:from __future__ import absolute_import
nmt-master/nmt/utils/evaluation_utils_test.py:19:from __future__ import division
nmt-master/nmt/utils/evaluation_utils_test.py:20:from __future__ import print_function
nmt-master/nmt/utils/evaluation_utils_test.py:22:import tensorflow as tf
nmt-master/nmt/utils/evaluation_utils_test.py:24:from ..utils import evaluation_utils
nmt-master/nmt/utils/evaluation_utils_test.py:27:class EvaluationUtilsTest(tf.test.TestCase):
nmt-master/nmt/utils/evaluation_utils_test.py:29:  def testEvaluate(self):
nmt-master/nmt/utils/evaluation_utils_test.py:30:    output = "nmt/testdata/deen_output"
nmt-master/nmt/utils/evaluation_utils_test.py:31:    ref_bpe = "nmt/testdata/deen_ref_bpe"
nmt-master/nmt/utils/evaluation_utils_test.py:32:    ref_spm = "nmt/testdata/deen_ref_spm"
nmt-master/nmt/utils/evaluation_utils_test.py:34:    expected_bleu_score = 22.5855084573
nmt-master/nmt/utils/evaluation_utils_test.py:35:    expected_rouge_score = 50.8429782599
nmt-master/nmt/utils/evaluation_utils_test.py:37:    bpe_bleu_score = evaluation_utils.evaluate(
nmt-master/nmt/utils/evaluation_utils_test.py:38:        ref_bpe, output, "bleu", "bpe")
nmt-master/nmt/utils/evaluation_utils_test.py:39:    bpe_rouge_score = evaluation_utils.evaluate(
nmt-master/nmt/utils/evaluation_utils_test.py:40:        ref_bpe, output, "rouge", "bpe")
nmt-master/nmt/utils/evaluation_utils_test.py:42:    self.assertAlmostEqual(expected_bleu_score, bpe_bleu_score)
nmt-master/nmt/utils/evaluation_utils_test.py:43:    self.assertAlmostEqual(expected_rouge_score, bpe_rouge_score)
nmt-master/nmt/utils/evaluation_utils_test.py:45:    spm_bleu_score = evaluation_utils.evaluate(
nmt-master/nmt/utils/evaluation_utils_test.py:46:        ref_spm, output, "bleu", "spm")
nmt-master/nmt/utils/evaluation_utils_test.py:47:    spm_rouge_score = evaluation_utils.evaluate(
nmt-master/nmt/utils/evaluation_utils_test.py:48:        ref_spm, output, "rouge", "spm")
nmt-master/nmt/utils/evaluation_utils_test.py:50:    self.assertAlmostEqual(expected_rouge_score, spm_rouge_score)
nmt-master/nmt/utils/evaluation_utils_test.py:51:    self.assertAlmostEqual(expected_bleu_score, spm_bleu_score)
nmt-master/nmt/utils/evaluation_utils_test.py:53:  def testAccuracy(self):
nmt-master/nmt/utils/evaluation_utils_test.py:54:    pred_output = "nmt/testdata/pred_output"
nmt-master/nmt/utils/evaluation_utils_test.py:55:    label_ref = "nmt/testdata/label_ref"
nmt-master/nmt/utils/evaluation_utils_test.py:57:    expected_accuracy_score = 60.00
nmt-master/nmt/utils/evaluation_utils_test.py:59:    accuracy_score = evaluation_utils.evaluate(
nmt-master/nmt/utils/evaluation_utils_test.py:60:        label_ref, pred_output, "accuracy")
nmt-master/nmt/utils/evaluation_utils_test.py:61:    self.assertAlmostEqual(expected_accuracy_score, accuracy_score)
nmt-master/nmt/utils/evaluation_utils_test.py:63:  def testWordAccuracy(self):
nmt-master/nmt/utils/evaluation_utils_test.py:64:    pred_output = "nmt/testdata/pred_output"
nmt-master/nmt/utils/evaluation_utils_test.py:65:    label_ref = "nmt/testdata/label_ref"
nmt-master/nmt/utils/evaluation_utils_test.py:67:    expected_word_accuracy_score = 60.00
nmt-master/nmt/utils/evaluation_utils_test.py:69:    word_accuracy_score = evaluation_utils.evaluate(
nmt-master/nmt/utils/evaluation_utils_test.py:70:        label_ref, pred_output, "word_accuracy")
nmt-master/nmt/utils/evaluation_utils_test.py:71:    self.assertAlmostEqual(expected_word_accuracy_score, word_accuracy_score)
nmt-master/nmt/utils/evaluation_utils_test.py:74:if __name__ == "__main__":
nmt-master/nmt/utils/evaluation_utils_test.py:75:  tf.test.main()
nmt-master/nmt/utils/iterator_utils.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/utils/iterator_utils.py:2:#
nmt-master/nmt/utils/iterator_utils.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/utils/iterator_utils.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/utils/iterator_utils.py:5:# You may obtain a copy of the License at
nmt-master/nmt/utils/iterator_utils.py:6:#
nmt-master/nmt/utils/iterator_utils.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/utils/iterator_utils.py:8:#
nmt-master/nmt/utils/iterator_utils.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/utils/iterator_utils.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/utils/iterator_utils.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/utils/iterator_utils.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/utils/iterator_utils.py:13:# limitations under the License.
nmt-master/nmt/utils/iterator_utils.py:14:# ==============================================================================
nmt-master/nmt/utils/iterator_utils.py:15:"""For loading data into NMT models."""
nmt-master/nmt/utils/iterator_utils.py:16:from __future__ import print_function
nmt-master/nmt/utils/iterator_utils.py:18:import collections
nmt-master/nmt/utils/iterator_utils.py:20:import tensorflow as tf
nmt-master/nmt/utils/iterator_utils.py:22:from ..utils import vocab_utils
nmt-master/nmt/utils/iterator_utils.py:25:__all__ = ["BatchedInput", "get_iterator", "get_infer_iterator"]
nmt-master/nmt/utils/iterator_utils.py:28:# NOTE(ebrevdo): When we subclass this, instances' __dict__ becomes empty.
nmt-master/nmt/utils/iterator_utils.py:29:class BatchedInput(
nmt-master/nmt/utils/iterator_utils.py:30:    collections.namedtuple("BatchedInput",
nmt-master/nmt/utils/iterator_utils.py:31:                           ("initializer", "source", "target_input",
nmt-master/nmt/utils/iterator_utils.py:32:                            "target_output", "source_sequence_length",
nmt-master/nmt/utils/iterator_utils.py:33:                            "target_sequence_length"))):
nmt-master/nmt/utils/iterator_utils.py:34:  pass
nmt-master/nmt/utils/iterator_utils.py:37:def get_infer_iterator(src_dataset,
nmt-master/nmt/utils/iterator_utils.py:38:                       src_vocab_table,
nmt-master/nmt/utils/iterator_utils.py:39:                       batch_size,
nmt-master/nmt/utils/iterator_utils.py:40:                       eos,
nmt-master/nmt/utils/iterator_utils.py:41:                       src_max_len=None,
nmt-master/nmt/utils/iterator_utils.py:42:                       use_char_encode=False):
nmt-master/nmt/utils/iterator_utils.py:43:  if use_char_encode:
nmt-master/nmt/utils/iterator_utils.py:44:    src_eos_id = vocab_utils.EOS_CHAR_ID
nmt-master/nmt/utils/iterator_utils.py:45:  else:
nmt-master/nmt/utils/iterator_utils.py:46:    src_eos_id = tf.cast(src_vocab_table.lookup(tf.constant(eos)), tf.int32)
nmt-master/nmt/utils/iterator_utils.py:47:  src_dataset = src_dataset.map(lambda src: tf.string_split([src]).values)
nmt-master/nmt/utils/iterator_utils.py:49:  if src_max_len:
nmt-master/nmt/utils/iterator_utils.py:50:    src_dataset = src_dataset.map(lambda src: src[:src_max_len])
nmt-master/nmt/utils/iterator_utils.py:52:  if use_char_encode:
nmt-master/nmt/utils/iterator_utils.py:53:    # Convert the word strings to character ids
nmt-master/nmt/utils/iterator_utils.py:54:    src_dataset = src_dataset.map(
nmt-master/nmt/utils/iterator_utils.py:55:        lambda src: tf.reshape(vocab_utils.tokens_to_bytes(src), [-1]))
nmt-master/nmt/utils/iterator_utils.py:56:  else:
nmt-master/nmt/utils/iterator_utils.py:57:    # Convert the word strings to ids
nmt-master/nmt/utils/iterator_utils.py:58:    src_dataset = src_dataset.map(
nmt-master/nmt/utils/iterator_utils.py:59:        lambda src: tf.cast(src_vocab_table.lookup(src), tf.int32))
nmt-master/nmt/utils/iterator_utils.py:61:  # Add in the word counts.
nmt-master/nmt/utils/iterator_utils.py:62:  if use_char_encode:
nmt-master/nmt/utils/iterator_utils.py:63:    src_dataset = src_dataset.map(
nmt-master/nmt/utils/iterator_utils.py:64:        lambda src: (src,
nmt-master/nmt/utils/iterator_utils.py:65:                     tf.to_int32(
nmt-master/nmt/utils/iterator_utils.py:66:                         tf.size(src) / vocab_utils.DEFAULT_CHAR_MAXLEN)))
nmt-master/nmt/utils/iterator_utils.py:67:  else:
nmt-master/nmt/utils/iterator_utils.py:68:    src_dataset = src_dataset.map(lambda src: (src, tf.size(src)))
nmt-master/nmt/utils/iterator_utils.py:70:  def batching_func(x):
nmt-master/nmt/utils/iterator_utils.py:71:    return x.padded_batch(
nmt-master/nmt/utils/iterator_utils.py:72:        batch_size,
nmt-master/nmt/utils/iterator_utils.py:73:        # The entry is the source line rows;
nmt-master/nmt/utils/iterator_utils.py:74:        # this has unknown-length vectors.  The last entry is
nmt-master/nmt/utils/iterator_utils.py:75:        # the source row size; this is a scalar.
nmt-master/nmt/utils/iterator_utils.py:76:        padded_shapes=(
nmt-master/nmt/utils/iterator_utils.py:77:            tf.TensorShape([None]),  # src
nmt-master/nmt/utils/iterator_utils.py:78:            tf.TensorShape([])),  # src_len
nmt-master/nmt/utils/iterator_utils.py:79:        # Pad the source sequences with eos tokens.
nmt-master/nmt/utils/iterator_utils.py:80:        # (Though notice we don't generally need to do this since
nmt-master/nmt/utils/iterator_utils.py:81:        # later on we will be masking out calculations past the true sequence.
nmt-master/nmt/utils/iterator_utils.py:82:        padding_values=(
nmt-master/nmt/utils/iterator_utils.py:83:            src_eos_id,  # src
nmt-master/nmt/utils/iterator_utils.py:84:            0))  # src_len -- unused
nmt-master/nmt/utils/iterator_utils.py:86:  batched_dataset = batching_func(src_dataset)
nmt-master/nmt/utils/iterator_utils.py:87:  batched_iter = batched_dataset.make_initializable_iterator()
nmt-master/nmt/utils/iterator_utils.py:88:  (src_ids, src_seq_len) = batched_iter.get_next()
nmt-master/nmt/utils/iterator_utils.py:89:  return BatchedInput(
nmt-master/nmt/utils/iterator_utils.py:90:      initializer=batched_iter.initializer,
nmt-master/nmt/utils/iterator_utils.py:91:      source=src_ids,
nmt-master/nmt/utils/iterator_utils.py:92:      target_input=None,
nmt-master/nmt/utils/iterator_utils.py:93:      target_output=None,
nmt-master/nmt/utils/iterator_utils.py:94:      source_sequence_length=src_seq_len,
nmt-master/nmt/utils/iterator_utils.py:95:      target_sequence_length=None)
nmt-master/nmt/utils/iterator_utils.py:98:def get_iterator(src_dataset,
nmt-master/nmt/utils/iterator_utils.py:99:                 tgt_dataset,
nmt-master/nmt/utils/iterator_utils.py:100:                 src_vocab_table,
nmt-master/nmt/utils/iterator_utils.py:101:                 tgt_vocab_table,
nmt-master/nmt/utils/iterator_utils.py:102:                 batch_size,
nmt-master/nmt/utils/iterator_utils.py:103:                 sos,
nmt-master/nmt/utils/iterator_utils.py:104:                 eos,
nmt-master/nmt/utils/iterator_utils.py:105:                 random_seed,
nmt-master/nmt/utils/iterator_utils.py:106:                 num_buckets,
nmt-master/nmt/utils/iterator_utils.py:107:                 src_max_len=None,
nmt-master/nmt/utils/iterator_utils.py:108:                 tgt_max_len=None,
nmt-master/nmt/utils/iterator_utils.py:109:                 num_parallel_calls=4,
nmt-master/nmt/utils/iterator_utils.py:110:                 output_buffer_size=None,
nmt-master/nmt/utils/iterator_utils.py:111:                 skip_count=None,
nmt-master/nmt/utils/iterator_utils.py:112:                 num_shards=1,
nmt-master/nmt/utils/iterator_utils.py:113:                 shard_index=0,
nmt-master/nmt/utils/iterator_utils.py:114:                 reshuffle_each_iteration=True,
nmt-master/nmt/utils/iterator_utils.py:115:                 use_char_encode=False):
nmt-master/nmt/utils/iterator_utils.py:116:  if not output_buffer_size:
nmt-master/nmt/utils/iterator_utils.py:117:    output_buffer_size = batch_size * 1000
nmt-master/nmt/utils/iterator_utils.py:119:  if use_char_encode:
nmt-master/nmt/utils/iterator_utils.py:120:    src_eos_id = vocab_utils.EOS_CHAR_ID
nmt-master/nmt/utils/iterator_utils.py:121:  else:
nmt-master/nmt/utils/iterator_utils.py:122:    src_eos_id = tf.cast(src_vocab_table.lookup(tf.constant(eos)), tf.int32)
nmt-master/nmt/utils/iterator_utils.py:124:  tgt_sos_id = tf.cast(tgt_vocab_table.lookup(tf.constant(sos)), tf.int32)
nmt-master/nmt/utils/iterator_utils.py:125:  tgt_eos_id = tf.cast(tgt_vocab_table.lookup(tf.constant(eos)), tf.int32)
nmt-master/nmt/utils/iterator_utils.py:127:  src_tgt_dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset))
nmt-master/nmt/utils/iterator_utils.py:129:  src_tgt_dataset = src_tgt_dataset.shard(num_shards, shard_index)
nmt-master/nmt/utils/iterator_utils.py:130:  if skip_count is not None:
nmt-master/nmt/utils/iterator_utils.py:131:    src_tgt_dataset = src_tgt_dataset.skip(skip_count)
nmt-master/nmt/utils/iterator_utils.py:133:  src_tgt_dataset = src_tgt_dataset.shuffle(
nmt-master/nmt/utils/iterator_utils.py:134:      output_buffer_size, random_seed, reshuffle_each_iteration)
nmt-master/nmt/utils/iterator_utils.py:136:  src_tgt_dataset = src_tgt_dataset.map(
nmt-master/nmt/utils/iterator_utils.py:137:      lambda src, tgt: (
nmt-master/nmt/utils/iterator_utils.py:138:          tf.string_split([src]).values, tf.string_split([tgt]).values),
nmt-master/nmt/utils/iterator_utils.py:139:      num_parallel_calls=num_parallel_calls).prefetch(output_buffer_size)
nmt-master/nmt/utils/iterator_utils.py:141:  # Filter zero length input sequences.
nmt-master/nmt/utils/iterator_utils.py:142:  src_tgt_dataset = src_tgt_dataset.filter(
nmt-master/nmt/utils/iterator_utils.py:143:      lambda src, tgt: tf.logical_and(tf.size(src) > 0, tf.size(tgt) > 0))
nmt-master/nmt/utils/iterator_utils.py:145:  if src_max_len:
nmt-master/nmt/utils/iterator_utils.py:146:    src_tgt_dataset = src_tgt_dataset.map(
nmt-master/nmt/utils/iterator_utils.py:147:        lambda src, tgt: (src[:src_max_len], tgt),
nmt-master/nmt/utils/iterator_utils.py:148:        num_parallel_calls=num_parallel_calls).prefetch(output_buffer_size)
nmt-master/nmt/utils/iterator_utils.py:149:  if tgt_max_len:
nmt-master/nmt/utils/iterator_utils.py:150:    src_tgt_dataset = src_tgt_dataset.map(
nmt-master/nmt/utils/iterator_utils.py:151:        lambda src, tgt: (src, tgt[:tgt_max_len]),
nmt-master/nmt/utils/iterator_utils.py:152:        num_parallel_calls=num_parallel_calls).prefetch(output_buffer_size)
nmt-master/nmt/utils/iterator_utils.py:154:  # Convert the word strings to ids.  Word strings that are not in the
nmt-master/nmt/utils/iterator_utils.py:155:  # vocab get the lookup table's default_value integer.
nmt-master/nmt/utils/iterator_utils.py:156:  if use_char_encode:
nmt-master/nmt/utils/iterator_utils.py:157:    src_tgt_dataset = src_tgt_dataset.map(
nmt-master/nmt/utils/iterator_utils.py:158:        lambda src, tgt: (tf.reshape(vocab_utils.tokens_to_bytes(src), [-1]),
nmt-master/nmt/utils/iterator_utils.py:159:                          tf.cast(tgt_vocab_table.lookup(tgt), tf.int32)),
nmt-master/nmt/utils/iterator_utils.py:160:        num_parallel_calls=num_parallel_calls)
nmt-master/nmt/utils/iterator_utils.py:161:  else:
nmt-master/nmt/utils/iterator_utils.py:162:    src_tgt_dataset = src_tgt_dataset.map(
nmt-master/nmt/utils/iterator_utils.py:163:        lambda src, tgt: (tf.cast(src_vocab_table.lookup(src), tf.int32),
nmt-master/nmt/utils/iterator_utils.py:164:                          tf.cast(tgt_vocab_table.lookup(tgt), tf.int32)),
nmt-master/nmt/utils/iterator_utils.py:165:        num_parallel_calls=num_parallel_calls)
nmt-master/nmt/utils/iterator_utils.py:167:  src_tgt_dataset = src_tgt_dataset.prefetch(output_buffer_size)
nmt-master/nmt/utils/iterator_utils.py:168:  # Create a tgt_input prefixed with <sos> and a tgt_output suffixed with <eos>.
nmt-master/nmt/utils/iterator_utils.py:169:  src_tgt_dataset = src_tgt_dataset.map(
nmt-master/nmt/utils/iterator_utils.py:170:      lambda src, tgt: (src,
nmt-master/nmt/utils/iterator_utils.py:171:                        tf.concat(([tgt_sos_id], tgt), 0),
nmt-master/nmt/utils/iterator_utils.py:172:                        tf.concat((tgt, [tgt_eos_id]), 0)),
nmt-master/nmt/utils/iterator_utils.py:173:      num_parallel_calls=num_parallel_calls).prefetch(output_buffer_size)
nmt-master/nmt/utils/iterator_utils.py:174:  # Add in sequence lengths.
nmt-master/nmt/utils/iterator_utils.py:175:  if use_char_encode:
nmt-master/nmt/utils/iterator_utils.py:176:    src_tgt_dataset = src_tgt_dataset.map(
nmt-master/nmt/utils/iterator_utils.py:177:        lambda src, tgt_in, tgt_out: (
nmt-master/nmt/utils/iterator_utils.py:178:            src, tgt_in, tgt_out,
nmt-master/nmt/utils/iterator_utils.py:179:            tf.to_int32(tf.size(src) / vocab_utils.DEFAULT_CHAR_MAXLEN),
nmt-master/nmt/utils/iterator_utils.py:180:            tf.size(tgt_in)),
nmt-master/nmt/utils/iterator_utils.py:181:        num_parallel_calls=num_parallel_calls)
nmt-master/nmt/utils/iterator_utils.py:182:  else:
nmt-master/nmt/utils/iterator_utils.py:183:    src_tgt_dataset = src_tgt_dataset.map(
nmt-master/nmt/utils/iterator_utils.py:184:        lambda src, tgt_in, tgt_out: (
nmt-master/nmt/utils/iterator_utils.py:185:            src, tgt_in, tgt_out, tf.size(src), tf.size(tgt_in)),
nmt-master/nmt/utils/iterator_utils.py:186:        num_parallel_calls=num_parallel_calls)
nmt-master/nmt/utils/iterator_utils.py:188:  src_tgt_dataset = src_tgt_dataset.prefetch(output_buffer_size)
nmt-master/nmt/utils/iterator_utils.py:190:  # Bucket by source sequence length (buckets for lengths 0-9, 10-19, ...)
nmt-master/nmt/utils/iterator_utils.py:191:  def batching_func(x):
nmt-master/nmt/utils/iterator_utils.py:192:    return x.padded_batch(
nmt-master/nmt/utils/iterator_utils.py:193:        batch_size,
nmt-master/nmt/utils/iterator_utils.py:194:        # The first three entries are the source and target line rows;
nmt-master/nmt/utils/iterator_utils.py:195:        # these have unknown-length vectors.  The last two entries are
nmt-master/nmt/utils/iterator_utils.py:196:        # the source and target row sizes; these are scalars.
nmt-master/nmt/utils/iterator_utils.py:197:        padded_shapes=(
nmt-master/nmt/utils/iterator_utils.py:198:            tf.TensorShape([None]),  # src
nmt-master/nmt/utils/iterator_utils.py:199:            tf.TensorShape([None]),  # tgt_input
nmt-master/nmt/utils/iterator_utils.py:200:            tf.TensorShape([None]),  # tgt_output
nmt-master/nmt/utils/iterator_utils.py:201:            tf.TensorShape([]),  # src_len
nmt-master/nmt/utils/iterator_utils.py:202:            tf.TensorShape([])),  # tgt_len
nmt-master/nmt/utils/iterator_utils.py:203:        # Pad the source and target sequences with eos tokens.
nmt-master/nmt/utils/iterator_utils.py:204:        # (Though notice we don't generally need to do this since
nmt-master/nmt/utils/iterator_utils.py:205:        # later on we will be masking out calculations past the true sequence.
nmt-master/nmt/utils/iterator_utils.py:206:        padding_values=(
nmt-master/nmt/utils/iterator_utils.py:207:            src_eos_id,  # src
nmt-master/nmt/utils/iterator_utils.py:208:            tgt_eos_id,  # tgt_input
nmt-master/nmt/utils/iterator_utils.py:209:            tgt_eos_id,  # tgt_output
nmt-master/nmt/utils/iterator_utils.py:210:            0,  # src_len -- unused
nmt-master/nmt/utils/iterator_utils.py:211:            0))  # tgt_len -- unused
nmt-master/nmt/utils/iterator_utils.py:213:  if num_buckets > 1:
nmt-master/nmt/utils/iterator_utils.py:215:    def key_func(unused_1, unused_2, unused_3, src_len, tgt_len):
nmt-master/nmt/utils/iterator_utils.py:216:      # Calculate bucket_width by maximum source sequence length.
nmt-master/nmt/utils/iterator_utils.py:217:      # Pairs with length [0, bucket_width) go to bucket 0, length
nmt-master/nmt/utils/iterator_utils.py:218:      # [bucket_width, 2 * bucket_width) go to bucket 1, etc.  Pairs with length
nmt-master/nmt/utils/iterator_utils.py:219:      # over ((num_bucket-1) * bucket_width) words all go into the last bucket.
nmt-master/nmt/utils/iterator_utils.py:220:      if src_max_len:
nmt-master/nmt/utils/iterator_utils.py:221:        bucket_width = (src_max_len + num_buckets - 1) // num_buckets
nmt-master/nmt/utils/iterator_utils.py:222:      else:
nmt-master/nmt/utils/iterator_utils.py:223:        bucket_width = 10
nmt-master/nmt/utils/iterator_utils.py:225:      # Bucket sentence pairs by the length of their source sentence and target
nmt-master/nmt/utils/iterator_utils.py:226:      # sentence.
nmt-master/nmt/utils/iterator_utils.py:227:      bucket_id = tf.maximum(src_len // bucket_width, tgt_len // bucket_width)
nmt-master/nmt/utils/iterator_utils.py:228:      return tf.to_int64(tf.minimum(num_buckets, bucket_id))
nmt-master/nmt/utils/iterator_utils.py:230:    def reduce_func(unused_key, windowed_data):
nmt-master/nmt/utils/iterator_utils.py:231:      return batching_func(windowed_data)
nmt-master/nmt/utils/iterator_utils.py:233:    batched_dataset = src_tgt_dataset.apply(
nmt-master/nmt/utils/iterator_utils.py:234:        tf.contrib.data.group_by_window(
nmt-master/nmt/utils/iterator_utils.py:235:            key_func=key_func, reduce_func=reduce_func, window_size=batch_size))
nmt-master/nmt/utils/iterator_utils.py:237:  else:
nmt-master/nmt/utils/iterator_utils.py:238:    batched_dataset = batching_func(src_tgt_dataset)
nmt-master/nmt/utils/iterator_utils.py:239:  batched_iter = batched_dataset.make_initializable_iterator()
nmt-master/nmt/utils/iterator_utils.py:240:  (src_ids, tgt_input_ids, tgt_output_ids, src_seq_len,
nmt-master/nmt/utils/iterator_utils.py:241:   tgt_seq_len) = (batched_iter.get_next())
nmt-master/nmt/utils/iterator_utils.py:242:  return BatchedInput(
nmt-master/nmt/utils/iterator_utils.py:243:      initializer=batched_iter.initializer,
nmt-master/nmt/utils/iterator_utils.py:244:      source=src_ids,
nmt-master/nmt/utils/iterator_utils.py:245:      target_input=tgt_input_ids,
nmt-master/nmt/utils/iterator_utils.py:246:      target_output=tgt_output_ids,
nmt-master/nmt/utils/iterator_utils.py:247:      source_sequence_length=src_seq_len,
nmt-master/nmt/utils/iterator_utils.py:248:      target_sequence_length=tgt_seq_len)
nmt-master/nmt/utils/iterator_utils_test.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/utils/iterator_utils_test.py:2:#
nmt-master/nmt/utils/iterator_utils_test.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/utils/iterator_utils_test.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/utils/iterator_utils_test.py:5:# You may obtain a copy of the License at
nmt-master/nmt/utils/iterator_utils_test.py:6:#
nmt-master/nmt/utils/iterator_utils_test.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/utils/iterator_utils_test.py:8:#
nmt-master/nmt/utils/iterator_utils_test.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/utils/iterator_utils_test.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/utils/iterator_utils_test.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/utils/iterator_utils_test.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/utils/iterator_utils_test.py:13:# limitations under the License.
nmt-master/nmt/utils/iterator_utils_test.py:14:# ==============================================================================
nmt-master/nmt/utils/iterator_utils_test.py:16:"""Tests for iterator_utils.py"""
nmt-master/nmt/utils/iterator_utils_test.py:18:from __future__ import absolute_import
nmt-master/nmt/utils/iterator_utils_test.py:19:from __future__ import division
nmt-master/nmt/utils/iterator_utils_test.py:20:from __future__ import print_function
nmt-master/nmt/utils/iterator_utils_test.py:22:import tensorflow as tf
nmt-master/nmt/utils/iterator_utils_test.py:24:from tensorflow.python.ops import lookup_ops
nmt-master/nmt/utils/iterator_utils_test.py:26:from ..utils import iterator_utils
nmt-master/nmt/utils/iterator_utils_test.py:29:class IteratorUtilsTest(tf.test.TestCase):
nmt-master/nmt/utils/iterator_utils_test.py:31:  def testGetIterator(self):
nmt-master/nmt/utils/iterator_utils_test.py:32:    tf.set_random_seed(1)
nmt-master/nmt/utils/iterator_utils_test.py:33:    tgt_vocab_table = src_vocab_table = lookup_ops.index_table_from_tensor(
nmt-master/nmt/utils/iterator_utils_test.py:34:        tf.constant(["a", "b", "c", "eos", "sos"]))
nmt-master/nmt/utils/iterator_utils_test.py:35:    src_dataset = tf.data.Dataset.from_tensor_slices(
nmt-master/nmt/utils/iterator_utils_test.py:36:        tf.constant(["f e a g", "c c a", "d", "c a"]))
nmt-master/nmt/utils/iterator_utils_test.py:37:    tgt_dataset = tf.data.Dataset.from_tensor_slices(
nmt-master/nmt/utils/iterator_utils_test.py:38:        tf.constant(["c c", "a b", "", "b c"]))
nmt-master/nmt/utils/iterator_utils_test.py:39:    hparams = tf.contrib.training.HParams(
nmt-master/nmt/utils/iterator_utils_test.py:40:        random_seed=3,
nmt-master/nmt/utils/iterator_utils_test.py:41:        num_buckets=5,
nmt-master/nmt/utils/iterator_utils_test.py:42:        eos="eos",
nmt-master/nmt/utils/iterator_utils_test.py:43:        sos="sos")
nmt-master/nmt/utils/iterator_utils_test.py:44:    batch_size = 2
nmt-master/nmt/utils/iterator_utils_test.py:45:    src_max_len = 3
nmt-master/nmt/utils/iterator_utils_test.py:46:    iterator = iterator_utils.get_iterator(
nmt-master/nmt/utils/iterator_utils_test.py:47:        src_dataset=src_dataset,
nmt-master/nmt/utils/iterator_utils_test.py:48:        tgt_dataset=tgt_dataset,
nmt-master/nmt/utils/iterator_utils_test.py:49:        src_vocab_table=src_vocab_table,
nmt-master/nmt/utils/iterator_utils_test.py:50:        tgt_vocab_table=tgt_vocab_table,
nmt-master/nmt/utils/iterator_utils_test.py:51:        batch_size=batch_size,
nmt-master/nmt/utils/iterator_utils_test.py:52:        sos=hparams.sos,
nmt-master/nmt/utils/iterator_utils_test.py:53:        eos=hparams.eos,
nmt-master/nmt/utils/iterator_utils_test.py:54:        random_seed=hparams.random_seed,
nmt-master/nmt/utils/iterator_utils_test.py:55:        num_buckets=hparams.num_buckets,
nmt-master/nmt/utils/iterator_utils_test.py:56:        src_max_len=src_max_len,
nmt-master/nmt/utils/iterator_utils_test.py:57:        reshuffle_each_iteration=False)
nmt-master/nmt/utils/iterator_utils_test.py:58:    table_initializer = tf.tables_initializer()
nmt-master/nmt/utils/iterator_utils_test.py:59:    source = iterator.source
nmt-master/nmt/utils/iterator_utils_test.py:60:    target_input = iterator.target_input
nmt-master/nmt/utils/iterator_utils_test.py:61:    target_output = iterator.target_output
nmt-master/nmt/utils/iterator_utils_test.py:62:    src_seq_len = iterator.source_sequence_length
nmt-master/nmt/utils/iterator_utils_test.py:63:    tgt_seq_len = iterator.target_sequence_length
nmt-master/nmt/utils/iterator_utils_test.py:64:    self.assertEqual([None, None], source.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:65:    self.assertEqual([None, None], target_input.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:66:    self.assertEqual([None, None], target_output.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:67:    self.assertEqual([None], src_seq_len.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:68:    self.assertEqual([None], tgt_seq_len.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:69:    with self.test_session() as sess:
nmt-master/nmt/utils/iterator_utils_test.py:70:      sess.run(table_initializer)
nmt-master/nmt/utils/iterator_utils_test.py:71:      sess.run(iterator.initializer)
nmt-master/nmt/utils/iterator_utils_test.py:73:      (source_v, src_len_v, target_input_v, target_output_v, tgt_len_v) = (
nmt-master/nmt/utils/iterator_utils_test.py:74:          sess.run((source, src_seq_len, target_input, target_output,
nmt-master/nmt/utils/iterator_utils_test.py:75:                    tgt_seq_len)))
nmt-master/nmt/utils/iterator_utils_test.py:76:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:77:          [[2, 0, 3],   # c a eos -- eos is padding
nmt-master/nmt/utils/iterator_utils_test.py:78:          [-1, -1, 0]], # "f" == unknown, "e" == unknown, a
nmt-master/nmt/utils/iterator_utils_test.py:79:          source_v)
nmt-master/nmt/utils/iterator_utils_test.py:80:      self.assertAllEqual([2, 3], src_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:81:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:82:          [[4, 1, 2],   # sos b c
nmt-master/nmt/utils/iterator_utils_test.py:83:           [4, 2, 2]],  # sos c c
nmt-master/nmt/utils/iterator_utils_test.py:84:          target_input_v)
nmt-master/nmt/utils/iterator_utils_test.py:85:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:86:          [[1, 2, 3],   # b c eos
nmt-master/nmt/utils/iterator_utils_test.py:87:           [2, 2, 3]],  # c c eos
nmt-master/nmt/utils/iterator_utils_test.py:88:          target_output_v)
nmt-master/nmt/utils/iterator_utils_test.py:89:      self.assertAllEqual([3, 3], tgt_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:91:      (source_v, src_len_v, target_input_v, target_output_v, tgt_len_v) = (
nmt-master/nmt/utils/iterator_utils_test.py:92:          sess.run((source, src_seq_len, target_input, target_output,
nmt-master/nmt/utils/iterator_utils_test.py:93:                    tgt_seq_len)))
nmt-master/nmt/utils/iterator_utils_test.py:94:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:95:          [[2, 2, 0]],  # c c a
nmt-master/nmt/utils/iterator_utils_test.py:96:          source_v)
nmt-master/nmt/utils/iterator_utils_test.py:97:      self.assertAllEqual([3], src_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:98:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:99:          [[4, 0, 1]],  # sos a b
nmt-master/nmt/utils/iterator_utils_test.py:100:          target_input_v)
nmt-master/nmt/utils/iterator_utils_test.py:101:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:102:          [[0, 1, 3]],  # a b eos
nmt-master/nmt/utils/iterator_utils_test.py:103:          target_output_v)
nmt-master/nmt/utils/iterator_utils_test.py:104:      self.assertAllEqual([3], tgt_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:106:      with self.assertRaisesOpError("End of sequence"):
nmt-master/nmt/utils/iterator_utils_test.py:107:        sess.run(source)
nmt-master/nmt/utils/iterator_utils_test.py:109:  def testGetIteratorWithShard(self):
nmt-master/nmt/utils/iterator_utils_test.py:110:    tf.set_random_seed(1)
nmt-master/nmt/utils/iterator_utils_test.py:111:    tgt_vocab_table = src_vocab_table = lookup_ops.index_table_from_tensor(
nmt-master/nmt/utils/iterator_utils_test.py:112:        tf.constant(["a", "b", "c", "eos", "sos"]))
nmt-master/nmt/utils/iterator_utils_test.py:113:    src_dataset = tf.data.Dataset.from_tensor_slices(
nmt-master/nmt/utils/iterator_utils_test.py:114:        tf.constant(["c c a", "f e a g", "d", "c a"]))
nmt-master/nmt/utils/iterator_utils_test.py:115:    tgt_dataset = tf.data.Dataset.from_tensor_slices(
nmt-master/nmt/utils/iterator_utils_test.py:116:        tf.constant(["a b", "c c", "", "b c"]))
nmt-master/nmt/utils/iterator_utils_test.py:117:    hparams = tf.contrib.training.HParams(
nmt-master/nmt/utils/iterator_utils_test.py:118:        random_seed=3,
nmt-master/nmt/utils/iterator_utils_test.py:119:        num_buckets=5,
nmt-master/nmt/utils/iterator_utils_test.py:120:        eos="eos",
nmt-master/nmt/utils/iterator_utils_test.py:121:        sos="sos")
nmt-master/nmt/utils/iterator_utils_test.py:122:    batch_size = 2
nmt-master/nmt/utils/iterator_utils_test.py:123:    src_max_len = 3
nmt-master/nmt/utils/iterator_utils_test.py:124:    iterator = iterator_utils.get_iterator(
nmt-master/nmt/utils/iterator_utils_test.py:125:        src_dataset=src_dataset,
nmt-master/nmt/utils/iterator_utils_test.py:126:        tgt_dataset=tgt_dataset,
nmt-master/nmt/utils/iterator_utils_test.py:127:        src_vocab_table=src_vocab_table,
nmt-master/nmt/utils/iterator_utils_test.py:128:        tgt_vocab_table=tgt_vocab_table,
nmt-master/nmt/utils/iterator_utils_test.py:129:        batch_size=batch_size,
nmt-master/nmt/utils/iterator_utils_test.py:130:        sos=hparams.sos,
nmt-master/nmt/utils/iterator_utils_test.py:131:        eos=hparams.eos,
nmt-master/nmt/utils/iterator_utils_test.py:132:        random_seed=hparams.random_seed,
nmt-master/nmt/utils/iterator_utils_test.py:133:        num_buckets=hparams.num_buckets,
nmt-master/nmt/utils/iterator_utils_test.py:134:        src_max_len=src_max_len,
nmt-master/nmt/utils/iterator_utils_test.py:135:        num_shards=2,
nmt-master/nmt/utils/iterator_utils_test.py:136:        shard_index=1,
nmt-master/nmt/utils/iterator_utils_test.py:137:        reshuffle_each_iteration=False)
nmt-master/nmt/utils/iterator_utils_test.py:138:    table_initializer = tf.tables_initializer()
nmt-master/nmt/utils/iterator_utils_test.py:139:    source = iterator.source
nmt-master/nmt/utils/iterator_utils_test.py:140:    target_input = iterator.target_input
nmt-master/nmt/utils/iterator_utils_test.py:141:    target_output = iterator.target_output
nmt-master/nmt/utils/iterator_utils_test.py:142:    src_seq_len = iterator.source_sequence_length
nmt-master/nmt/utils/iterator_utils_test.py:143:    tgt_seq_len = iterator.target_sequence_length
nmt-master/nmt/utils/iterator_utils_test.py:144:    self.assertEqual([None, None], source.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:145:    self.assertEqual([None, None], target_input.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:146:    self.assertEqual([None, None], target_output.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:147:    self.assertEqual([None], src_seq_len.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:148:    self.assertEqual([None], tgt_seq_len.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:149:    with self.test_session() as sess:
nmt-master/nmt/utils/iterator_utils_test.py:150:      sess.run(table_initializer)
nmt-master/nmt/utils/iterator_utils_test.py:151:      sess.run(iterator.initializer)
nmt-master/nmt/utils/iterator_utils_test.py:153:      (source_v, src_len_v, target_input_v, target_output_v, tgt_len_v) = (
nmt-master/nmt/utils/iterator_utils_test.py:154:          sess.run((source, src_seq_len, target_input, target_output,
nmt-master/nmt/utils/iterator_utils_test.py:155:                    tgt_seq_len)))
nmt-master/nmt/utils/iterator_utils_test.py:156:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:157:          [[2, 0, 3],     # c a eos -- eos is padding
nmt-master/nmt/utils/iterator_utils_test.py:158:           [-1, -1, 0]],  # "f" == unknown, "e" == unknown, a
nmt-master/nmt/utils/iterator_utils_test.py:159:          source_v)
nmt-master/nmt/utils/iterator_utils_test.py:160:      self.assertAllEqual([2, 3], src_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:161:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:162:          [[4, 1, 2],   # sos b c
nmt-master/nmt/utils/iterator_utils_test.py:163:           [4, 2, 2]],  # sos c c
nmt-master/nmt/utils/iterator_utils_test.py:164:          target_input_v)
nmt-master/nmt/utils/iterator_utils_test.py:165:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:166:          [[1, 2, 3],   # b c eos
nmt-master/nmt/utils/iterator_utils_test.py:167:           [2, 2, 3]],  # c c eos
nmt-master/nmt/utils/iterator_utils_test.py:168:          target_output_v)
nmt-master/nmt/utils/iterator_utils_test.py:169:      self.assertAllEqual([3, 3], tgt_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:171:      with self.assertRaisesOpError("End of sequence"):
nmt-master/nmt/utils/iterator_utils_test.py:172:        sess.run(source)
nmt-master/nmt/utils/iterator_utils_test.py:174:  def testGetIteratorWithSkipCount(self):
nmt-master/nmt/utils/iterator_utils_test.py:175:    tf.set_random_seed(1)
nmt-master/nmt/utils/iterator_utils_test.py:176:    tgt_vocab_table = src_vocab_table = lookup_ops.index_table_from_tensor(
nmt-master/nmt/utils/iterator_utils_test.py:177:        tf.constant(["a", "b", "c", "eos", "sos"]))
nmt-master/nmt/utils/iterator_utils_test.py:178:    src_dataset = tf.data.Dataset.from_tensor_slices(
nmt-master/nmt/utils/iterator_utils_test.py:179:        tf.constant(["c a", "c c a", "d", "f e a g"]))
nmt-master/nmt/utils/iterator_utils_test.py:180:    tgt_dataset = tf.data.Dataset.from_tensor_slices(
nmt-master/nmt/utils/iterator_utils_test.py:181:        tf.constant(["b c", "a b", "", "c c"]))
nmt-master/nmt/utils/iterator_utils_test.py:182:    hparams = tf.contrib.training.HParams(
nmt-master/nmt/utils/iterator_utils_test.py:183:        random_seed=3,
nmt-master/nmt/utils/iterator_utils_test.py:184:        num_buckets=5,
nmt-master/nmt/utils/iterator_utils_test.py:185:        eos="eos",
nmt-master/nmt/utils/iterator_utils_test.py:186:        sos="sos")
nmt-master/nmt/utils/iterator_utils_test.py:187:    batch_size = 2
nmt-master/nmt/utils/iterator_utils_test.py:188:    src_max_len = 3
nmt-master/nmt/utils/iterator_utils_test.py:189:    skip_count = tf.placeholder(shape=(), dtype=tf.int64)
nmt-master/nmt/utils/iterator_utils_test.py:190:    iterator = iterator_utils.get_iterator(
nmt-master/nmt/utils/iterator_utils_test.py:191:        src_dataset=src_dataset,
nmt-master/nmt/utils/iterator_utils_test.py:192:        tgt_dataset=tgt_dataset,
nmt-master/nmt/utils/iterator_utils_test.py:193:        src_vocab_table=src_vocab_table,
nmt-master/nmt/utils/iterator_utils_test.py:194:        tgt_vocab_table=tgt_vocab_table,
nmt-master/nmt/utils/iterator_utils_test.py:195:        batch_size=batch_size,
nmt-master/nmt/utils/iterator_utils_test.py:196:        sos=hparams.sos,
nmt-master/nmt/utils/iterator_utils_test.py:197:        eos=hparams.eos,
nmt-master/nmt/utils/iterator_utils_test.py:198:        random_seed=hparams.random_seed,
nmt-master/nmt/utils/iterator_utils_test.py:199:        num_buckets=hparams.num_buckets,
nmt-master/nmt/utils/iterator_utils_test.py:200:        src_max_len=src_max_len,
nmt-master/nmt/utils/iterator_utils_test.py:201:        skip_count=skip_count,
nmt-master/nmt/utils/iterator_utils_test.py:202:        reshuffle_each_iteration=False)
nmt-master/nmt/utils/iterator_utils_test.py:203:    table_initializer = tf.tables_initializer()
nmt-master/nmt/utils/iterator_utils_test.py:204:    source = iterator.source
nmt-master/nmt/utils/iterator_utils_test.py:205:    target_input = iterator.target_input
nmt-master/nmt/utils/iterator_utils_test.py:206:    target_output = iterator.target_output
nmt-master/nmt/utils/iterator_utils_test.py:207:    src_seq_len = iterator.source_sequence_length
nmt-master/nmt/utils/iterator_utils_test.py:208:    tgt_seq_len = iterator.target_sequence_length
nmt-master/nmt/utils/iterator_utils_test.py:209:    self.assertEqual([None, None], source.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:210:    self.assertEqual([None, None], target_input.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:211:    self.assertEqual([None, None], target_output.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:212:    self.assertEqual([None], src_seq_len.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:213:    self.assertEqual([None], tgt_seq_len.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:214:    with self.test_session() as sess:
nmt-master/nmt/utils/iterator_utils_test.py:215:      sess.run(table_initializer)
nmt-master/nmt/utils/iterator_utils_test.py:216:      sess.run(iterator.initializer, feed_dict={skip_count: 3})
nmt-master/nmt/utils/iterator_utils_test.py:218:      (source_v, src_len_v, target_input_v, target_output_v, tgt_len_v) = (
nmt-master/nmt/utils/iterator_utils_test.py:219:          sess.run((source, src_seq_len, target_input, target_output,
nmt-master/nmt/utils/iterator_utils_test.py:220:                    tgt_seq_len)))
nmt-master/nmt/utils/iterator_utils_test.py:221:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:222:          [[-1, -1, 0]], # "f" == unknown, "e" == unknown, a
nmt-master/nmt/utils/iterator_utils_test.py:223:          source_v)
nmt-master/nmt/utils/iterator_utils_test.py:224:      self.assertAllEqual([3], src_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:225:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:226:          [[4, 2, 2]],   # sos c c
nmt-master/nmt/utils/iterator_utils_test.py:227:          target_input_v)
nmt-master/nmt/utils/iterator_utils_test.py:228:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:229:          [[2, 2, 3]],   # c c eos
nmt-master/nmt/utils/iterator_utils_test.py:230:          target_output_v)
nmt-master/nmt/utils/iterator_utils_test.py:231:      self.assertAllEqual([3], tgt_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:233:      with self.assertRaisesOpError("End of sequence"):
nmt-master/nmt/utils/iterator_utils_test.py:234:        sess.run(source)
nmt-master/nmt/utils/iterator_utils_test.py:236:      # Re-init iterator with skip_count=0.
nmt-master/nmt/utils/iterator_utils_test.py:237:      sess.run(iterator.initializer, feed_dict={skip_count: 0})
nmt-master/nmt/utils/iterator_utils_test.py:239:      (source_v, src_len_v, target_input_v, target_output_v, tgt_len_v) = (
nmt-master/nmt/utils/iterator_utils_test.py:240:          sess.run((source, src_seq_len, target_input, target_output,
nmt-master/nmt/utils/iterator_utils_test.py:241:                    tgt_seq_len)))
nmt-master/nmt/utils/iterator_utils_test.py:242:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:243:          [[-1, -1, 0],  # "f" == unknown, "e" == unknown, a
nmt-master/nmt/utils/iterator_utils_test.py:244:           [2, 0, 3]],   # c a eos -- eos is padding
nmt-master/nmt/utils/iterator_utils_test.py:245:          source_v)
nmt-master/nmt/utils/iterator_utils_test.py:246:      self.assertAllEqual([3, 2], src_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:247:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:248:          [[4, 2, 2],   # sos c c
nmt-master/nmt/utils/iterator_utils_test.py:249:           [4, 1, 2]],  # sos b c
nmt-master/nmt/utils/iterator_utils_test.py:250:          target_input_v)
nmt-master/nmt/utils/iterator_utils_test.py:251:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:252:          [[2, 2, 3],   # c c eos
nmt-master/nmt/utils/iterator_utils_test.py:253:           [1, 2, 3]],  # b c eos
nmt-master/nmt/utils/iterator_utils_test.py:254:          target_output_v)
nmt-master/nmt/utils/iterator_utils_test.py:255:      self.assertAllEqual([3, 3], tgt_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:257:      (source_v, src_len_v, target_input_v, target_output_v, tgt_len_v) = (
nmt-master/nmt/utils/iterator_utils_test.py:258:          sess.run((source, src_seq_len, target_input, target_output,
nmt-master/nmt/utils/iterator_utils_test.py:259:                    tgt_seq_len)))
nmt-master/nmt/utils/iterator_utils_test.py:260:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:261:          [[2, 2, 0]],  # c c a
nmt-master/nmt/utils/iterator_utils_test.py:262:          source_v)
nmt-master/nmt/utils/iterator_utils_test.py:263:      self.assertAllEqual([3], src_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:264:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:265:          [[4, 0, 1]],  # sos a b
nmt-master/nmt/utils/iterator_utils_test.py:266:          target_input_v)
nmt-master/nmt/utils/iterator_utils_test.py:267:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:268:          [[0, 1, 3]],  # a b eos
nmt-master/nmt/utils/iterator_utils_test.py:269:          target_output_v)
nmt-master/nmt/utils/iterator_utils_test.py:270:      self.assertAllEqual([3], tgt_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:272:      with self.assertRaisesOpError("End of sequence"):
nmt-master/nmt/utils/iterator_utils_test.py:273:        sess.run(source)
nmt-master/nmt/utils/iterator_utils_test.py:276:  def testGetInferIterator(self):
nmt-master/nmt/utils/iterator_utils_test.py:277:    src_vocab_table = lookup_ops.index_table_from_tensor(
nmt-master/nmt/utils/iterator_utils_test.py:278:        tf.constant(["a", "b", "c", "eos", "sos"]))
nmt-master/nmt/utils/iterator_utils_test.py:279:    src_dataset = tf.data.Dataset.from_tensor_slices(
nmt-master/nmt/utils/iterator_utils_test.py:280:        tf.constant(["c c a", "c a", "d", "f e a g"]))
nmt-master/nmt/utils/iterator_utils_test.py:281:    hparams = tf.contrib.training.HParams(
nmt-master/nmt/utils/iterator_utils_test.py:282:        random_seed=3,
nmt-master/nmt/utils/iterator_utils_test.py:283:        eos="eos",
nmt-master/nmt/utils/iterator_utils_test.py:284:        sos="sos")
nmt-master/nmt/utils/iterator_utils_test.py:285:    batch_size = 2
nmt-master/nmt/utils/iterator_utils_test.py:286:    src_max_len = 3
nmt-master/nmt/utils/iterator_utils_test.py:287:    iterator = iterator_utils.get_infer_iterator(
nmt-master/nmt/utils/iterator_utils_test.py:288:        src_dataset=src_dataset,
nmt-master/nmt/utils/iterator_utils_test.py:289:        src_vocab_table=src_vocab_table,
nmt-master/nmt/utils/iterator_utils_test.py:290:        batch_size=batch_size,
nmt-master/nmt/utils/iterator_utils_test.py:291:        eos=hparams.eos,
nmt-master/nmt/utils/iterator_utils_test.py:292:        src_max_len=src_max_len)
nmt-master/nmt/utils/iterator_utils_test.py:293:    table_initializer = tf.tables_initializer()
nmt-master/nmt/utils/iterator_utils_test.py:294:    source = iterator.source
nmt-master/nmt/utils/iterator_utils_test.py:295:    seq_len = iterator.source_sequence_length
nmt-master/nmt/utils/iterator_utils_test.py:296:    self.assertEqual([None, None], source.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:297:    self.assertEqual([None], seq_len.shape.as_list())
nmt-master/nmt/utils/iterator_utils_test.py:298:    with self.test_session() as sess:
nmt-master/nmt/utils/iterator_utils_test.py:299:      sess.run(table_initializer)
nmt-master/nmt/utils/iterator_utils_test.py:300:      sess.run(iterator.initializer)
nmt-master/nmt/utils/iterator_utils_test.py:302:      (source_v, seq_len_v) = sess.run((source, seq_len))
nmt-master/nmt/utils/iterator_utils_test.py:303:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:304:          [[2, 2, 0],   # c c a
nmt-master/nmt/utils/iterator_utils_test.py:305:           [2, 0, 3]],  # c a eos
nmt-master/nmt/utils/iterator_utils_test.py:306:          source_v)
nmt-master/nmt/utils/iterator_utils_test.py:307:      self.assertAllEqual([3, 2], seq_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:309:      (source_v, seq_len_v) = sess.run((source, seq_len))
nmt-master/nmt/utils/iterator_utils_test.py:310:      self.assertAllEqual(
nmt-master/nmt/utils/iterator_utils_test.py:311:          [[-1, 3, 3],    # "d" == unknown, eos eos
nmt-master/nmt/utils/iterator_utils_test.py:312:           [-1, -1, 0]],  # "f" == unknown, "e" == unknown, a
nmt-master/nmt/utils/iterator_utils_test.py:313:          source_v)
nmt-master/nmt/utils/iterator_utils_test.py:314:      self.assertAllEqual([1, 3], seq_len_v)
nmt-master/nmt/utils/iterator_utils_test.py:316:      with self.assertRaisesOpError("End of sequence"):
nmt-master/nmt/utils/iterator_utils_test.py:317:        sess.run((source, seq_len))
nmt-master/nmt/utils/iterator_utils_test.py:320:if __name__ == "__main__":
nmt-master/nmt/utils/iterator_utils_test.py:321:  tf.test.main()
nmt-master/nmt/utils/misc_utils.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/utils/misc_utils.py:2:#
nmt-master/nmt/utils/misc_utils.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/utils/misc_utils.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/utils/misc_utils.py:5:# You may obtain a copy of the License at
nmt-master/nmt/utils/misc_utils.py:6:#
nmt-master/nmt/utils/misc_utils.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/utils/misc_utils.py:8:#
nmt-master/nmt/utils/misc_utils.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/utils/misc_utils.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/utils/misc_utils.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/utils/misc_utils.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/utils/misc_utils.py:13:# limitations under the License.
nmt-master/nmt/utils/misc_utils.py:14:# ==============================================================================
nmt-master/nmt/utils/misc_utils.py:16:"""Generally useful utility functions."""
nmt-master/nmt/utils/misc_utils.py:17:from __future__ import print_function
nmt-master/nmt/utils/misc_utils.py:19:import codecs
nmt-master/nmt/utils/misc_utils.py:20:import collections
nmt-master/nmt/utils/misc_utils.py:21:import json
nmt-master/nmt/utils/misc_utils.py:22:import math
nmt-master/nmt/utils/misc_utils.py:23:import os
nmt-master/nmt/utils/misc_utils.py:24:import sys
nmt-master/nmt/utils/misc_utils.py:25:import time
nmt-master/nmt/utils/misc_utils.py:26:from distutils import version
nmt-master/nmt/utils/misc_utils.py:28:import numpy as np
nmt-master/nmt/utils/misc_utils.py:29:import six
nmt-master/nmt/utils/misc_utils.py:30:import tensorflow as tf
nmt-master/nmt/utils/misc_utils.py:33:def check_tensorflow_version():
nmt-master/nmt/utils/misc_utils.py:34:  # LINT.IfChange
nmt-master/nmt/utils/misc_utils.py:35:  min_tf_version = "1.12.0"
nmt-master/nmt/utils/misc_utils.py:36:  # LINT.ThenChange(<pwd>/nmt/copy.bara.sky)
nmt-master/nmt/utils/misc_utils.py:37:  if (version.LooseVersion(tf.__version__) <
nmt-master/nmt/utils/misc_utils.py:38:      version.LooseVersion(min_tf_version)):
nmt-master/nmt/utils/misc_utils.py:39:    raise EnvironmentError("Tensorflow version must >= %s" % min_tf_version)
nmt-master/nmt/utils/misc_utils.py:42:def safe_exp(value):
nmt-master/nmt/utils/misc_utils.py:43:  """Exponentiation with catching of overflow error."""
nmt-master/nmt/utils/misc_utils.py:44:  try:
nmt-master/nmt/utils/misc_utils.py:45:    ans = math.exp(value)
nmt-master/nmt/utils/misc_utils.py:46:  except OverflowError:
nmt-master/nmt/utils/misc_utils.py:47:    ans = float("inf")
nmt-master/nmt/utils/misc_utils.py:48:  return ans
nmt-master/nmt/utils/misc_utils.py:51:def print_time(s, start_time):
nmt-master/nmt/utils/misc_utils.py:52:  """Take a start time, print elapsed duration, and return a new time."""
nmt-master/nmt/utils/misc_utils.py:53:  print("%s, time %ds, %s." % (s, (time.time() - start_time), time.ctime()))
nmt-master/nmt/utils/misc_utils.py:54:  sys.stdout.flush()
nmt-master/nmt/utils/misc_utils.py:55:  return time.time()
nmt-master/nmt/utils/misc_utils.py:58:def print_out(s, f=None, new_line=True):
nmt-master/nmt/utils/misc_utils.py:59:  """Similar to print but with support to flush and output to a file."""
nmt-master/nmt/utils/misc_utils.py:60:  if isinstance(s, bytes):
nmt-master/nmt/utils/misc_utils.py:61:    s = s.decode("utf-8")
nmt-master/nmt/utils/misc_utils.py:63:  if f:
nmt-master/nmt/utils/misc_utils.py:64:    f.write(s.encode("utf-8"))
nmt-master/nmt/utils/misc_utils.py:65:    if new_line:
nmt-master/nmt/utils/misc_utils.py:66:      f.write(b"\n")
nmt-master/nmt/utils/misc_utils.py:68:  # stdout
nmt-master/nmt/utils/misc_utils.py:69:  if six.PY2:
nmt-master/nmt/utils/misc_utils.py:70:    sys.stdout.write(s.encode("utf-8"))
nmt-master/nmt/utils/misc_utils.py:71:  else:
nmt-master/nmt/utils/misc_utils.py:72:    sys.stdout.buffer.write(s.encode("utf-8"))
nmt-master/nmt/utils/misc_utils.py:74:  if new_line:
nmt-master/nmt/utils/misc_utils.py:75:    sys.stdout.write("\n")
nmt-master/nmt/utils/misc_utils.py:76:  sys.stdout.flush()
nmt-master/nmt/utils/misc_utils.py:79:def print_hparams(hparams, skip_patterns=None, header=None):
nmt-master/nmt/utils/misc_utils.py:80:  """Print hparams, can skip keys based on pattern."""
nmt-master/nmt/utils/misc_utils.py:81:  if header: print_out("%s" % header)
nmt-master/nmt/utils/misc_utils.py:82:  values = hparams.values()
nmt-master/nmt/utils/misc_utils.py:83:  for key in sorted(values.keys()):
nmt-master/nmt/utils/misc_utils.py:84:    if not skip_patterns or all(
nmt-master/nmt/utils/misc_utils.py:85:        [skip_pattern not in key for skip_pattern in skip_patterns]):
nmt-master/nmt/utils/misc_utils.py:86:      print_out("  %s=%s" % (key, str(values[key])))
nmt-master/nmt/utils/misc_utils.py:89:def load_hparams(model_dir):
nmt-master/nmt/utils/misc_utils.py:90:  """Load hparams from an existing model directory."""
nmt-master/nmt/utils/misc_utils.py:91:  hparams_file = os.path.join(model_dir, "hparams")
nmt-master/nmt/utils/misc_utils.py:92:  if tf.gfile.Exists(hparams_file):
nmt-master/nmt/utils/misc_utils.py:93:    print_out("# Loading hparams from %s" % hparams_file)
nmt-master/nmt/utils/misc_utils.py:94:    with codecs.getreader("utf-8")(tf.gfile.GFile(hparams_file, "rb")) as f:
nmt-master/nmt/utils/misc_utils.py:95:      try:
nmt-master/nmt/utils/misc_utils.py:96:        hparams_values = json.load(f)
nmt-master/nmt/utils/misc_utils.py:97:        hparams = tf.contrib.training.HParams(**hparams_values)
nmt-master/nmt/utils/misc_utils.py:98:      except ValueError:
nmt-master/nmt/utils/misc_utils.py:99:        print_out("  can't load hparams file")
nmt-master/nmt/utils/misc_utils.py:100:        return None
nmt-master/nmt/utils/misc_utils.py:101:    return hparams
nmt-master/nmt/utils/misc_utils.py:102:  else:
nmt-master/nmt/utils/misc_utils.py:103:    return None
nmt-master/nmt/utils/misc_utils.py:106:def maybe_parse_standard_hparams(hparams, hparams_path):
nmt-master/nmt/utils/misc_utils.py:107:  """Override hparams values with existing standard hparams config."""
nmt-master/nmt/utils/misc_utils.py:108:  if hparams_path and tf.gfile.Exists(hparams_path):
nmt-master/nmt/utils/misc_utils.py:109:    print_out("# Loading standard hparams from %s" % hparams_path)
nmt-master/nmt/utils/misc_utils.py:110:    with codecs.getreader("utf-8")(tf.gfile.GFile(hparams_path, "rb")) as f:
nmt-master/nmt/utils/misc_utils.py:111:      hparams.parse_json(f.read())
nmt-master/nmt/utils/misc_utils.py:112:  return hparams
nmt-master/nmt/utils/misc_utils.py:115:def save_hparams(out_dir, hparams):
nmt-master/nmt/utils/misc_utils.py:116:  """Save hparams."""
nmt-master/nmt/utils/misc_utils.py:117:  hparams_file = os.path.join(out_dir, "hparams")
nmt-master/nmt/utils/misc_utils.py:118:  print_out("  saving hparams to %s" % hparams_file)
nmt-master/nmt/utils/misc_utils.py:119:  with codecs.getwriter("utf-8")(tf.gfile.GFile(hparams_file, "wb")) as f:
nmt-master/nmt/utils/misc_utils.py:120:    f.write(hparams.to_json(indent=4, sort_keys=True))
nmt-master/nmt/utils/misc_utils.py:123:def debug_tensor(s, msg=None, summarize=10):
nmt-master/nmt/utils/misc_utils.py:124:  """Print the shape and value of a tensor at test time. Return a new tensor."""
nmt-master/nmt/utils/misc_utils.py:125:  if not msg:
nmt-master/nmt/utils/misc_utils.py:126:    msg = s.name
nmt-master/nmt/utils/misc_utils.py:127:  return tf.Print(s, [tf.shape(s), s], msg + " ", summarize=summarize)
nmt-master/nmt/utils/misc_utils.py:130:def add_summary(summary_writer, global_step, tag, value):
nmt-master/nmt/utils/misc_utils.py:131:  """Add a new summary to the current summary_writer.
nmt-master/nmt/utils/misc_utils.py:132:  Useful to log things that are not part of the training graph, e.g., tag=BLEU.
nmt-master/nmt/utils/misc_utils.py:133:  """
nmt-master/nmt/utils/misc_utils.py:134:  summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])
nmt-master/nmt/utils/misc_utils.py:135:  summary_writer.add_summary(summary, global_step)
nmt-master/nmt/utils/misc_utils.py:138:def get_config_proto(log_device_placement=False, allow_soft_placement=True,
nmt-master/nmt/utils/misc_utils.py:139:                     num_intra_threads=0, num_inter_threads=0):
nmt-master/nmt/utils/misc_utils.py:140:  # GPU options:
nmt-master/nmt/utils/misc_utils.py:141:  # https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/index.html
nmt-master/nmt/utils/misc_utils.py:142:  config_proto = tf.ConfigProto(
nmt-master/nmt/utils/misc_utils.py:143:      log_device_placement=log_device_placement,
nmt-master/nmt/utils/misc_utils.py:144:      allow_soft_placement=allow_soft_placement)
nmt-master/nmt/utils/misc_utils.py:145:  config_proto.gpu_options.allow_growth = True
nmt-master/nmt/utils/misc_utils.py:147:  # CPU threads options
nmt-master/nmt/utils/misc_utils.py:148:  if num_intra_threads:
nmt-master/nmt/utils/misc_utils.py:149:    config_proto.intra_op_parallelism_threads = num_intra_threads
nmt-master/nmt/utils/misc_utils.py:150:  if num_inter_threads:
nmt-master/nmt/utils/misc_utils.py:151:    config_proto.inter_op_parallelism_threads = num_inter_threads
nmt-master/nmt/utils/misc_utils.py:153:  return config_proto
nmt-master/nmt/utils/misc_utils.py:156:def format_text(words):
nmt-master/nmt/utils/misc_utils.py:157:  """Convert a sequence words into sentence."""
nmt-master/nmt/utils/misc_utils.py:158:  if (not hasattr(words, "__len__") and  # for numpy array
nmt-master/nmt/utils/misc_utils.py:159:      not isinstance(words, collections.Iterable)):
nmt-master/nmt/utils/misc_utils.py:160:    words = [words]
nmt-master/nmt/utils/misc_utils.py:161:  return b" ".join(words)
nmt-master/nmt/utils/misc_utils.py:164:def format_bpe_text(symbols, delimiter=b"@@"):
nmt-master/nmt/utils/misc_utils.py:165:  """Convert a sequence of bpe words into sentence."""
nmt-master/nmt/utils/misc_utils.py:166:  words = []
nmt-master/nmt/utils/misc_utils.py:167:  word = b""
nmt-master/nmt/utils/misc_utils.py:168:  if isinstance(symbols, str):
nmt-master/nmt/utils/misc_utils.py:169:    symbols = symbols.encode()
nmt-master/nmt/utils/misc_utils.py:170:  delimiter_len = len(delimiter)
nmt-master/nmt/utils/misc_utils.py:171:  for symbol in symbols:
nmt-master/nmt/utils/misc_utils.py:172:    if len(symbol) >= delimiter_len and symbol[-delimiter_len:] == delimiter:
nmt-master/nmt/utils/misc_utils.py:173:      word += symbol[:-delimiter_len]
nmt-master/nmt/utils/misc_utils.py:174:    else:  # end of a word
nmt-master/nmt/utils/misc_utils.py:175:      word += symbol
nmt-master/nmt/utils/misc_utils.py:176:      words.append(word)
nmt-master/nmt/utils/misc_utils.py:177:      word = b""
nmt-master/nmt/utils/misc_utils.py:178:  return b" ".join(words)
nmt-master/nmt/utils/misc_utils.py:181:def format_spm_text(symbols):
nmt-master/nmt/utils/misc_utils.py:182:  """Decode a text in SPM (https://github.com/google/sentencepiece) format."""
nmt-master/nmt/utils/misc_utils.py:183:  return u"".join(format_text(symbols).decode("utf-8").split()).replace(
nmt-master/nmt/utils/misc_utils.py:184:      u"\u2581", u" ").strip().encode("utf-8")
nmt-master/nmt/utils/misc_utils_test.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/utils/misc_utils_test.py:2:#
nmt-master/nmt/utils/misc_utils_test.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/utils/misc_utils_test.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/utils/misc_utils_test.py:5:# You may obtain a copy of the License at
nmt-master/nmt/utils/misc_utils_test.py:6:#
nmt-master/nmt/utils/misc_utils_test.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/utils/misc_utils_test.py:8:#
nmt-master/nmt/utils/misc_utils_test.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/utils/misc_utils_test.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/utils/misc_utils_test.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/utils/misc_utils_test.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/utils/misc_utils_test.py:13:# limitations under the License.
nmt-master/nmt/utils/misc_utils_test.py:14:# ==============================================================================
nmt-master/nmt/utils/misc_utils_test.py:16:"""Tests for vocab_utils."""
nmt-master/nmt/utils/misc_utils_test.py:18:from __future__ import absolute_import
nmt-master/nmt/utils/misc_utils_test.py:19:from __future__ import division
nmt-master/nmt/utils/misc_utils_test.py:20:from __future__ import print_function
nmt-master/nmt/utils/misc_utils_test.py:22:import tensorflow as tf
nmt-master/nmt/utils/misc_utils_test.py:24:from ..utils import misc_utils
nmt-master/nmt/utils/misc_utils_test.py:27:class MiscUtilsTest(tf.test.TestCase):
nmt-master/nmt/utils/misc_utils_test.py:29:  def testFormatBpeText(self):
nmt-master/nmt/utils/misc_utils_test.py:30:    bpe_line = (
nmt-master/nmt/utils/misc_utils_test.py:31:        b"En@@ ough to make already reluc@@ tant men hesitate to take screening"
nmt-master/nmt/utils/misc_utils_test.py:32:        b" tests ."
nmt-master/nmt/utils/misc_utils_test.py:33:    )
nmt-master/nmt/utils/misc_utils_test.py:34:    expected_result = (
nmt-master/nmt/utils/misc_utils_test.py:35:        b"Enough to make already reluctant men hesitate to take screening tests"
nmt-master/nmt/utils/misc_utils_test.py:36:        b" ."
nmt-master/nmt/utils/misc_utils_test.py:37:    )
nmt-master/nmt/utils/misc_utils_test.py:38:    self.assertEqual(expected_result,
nmt-master/nmt/utils/misc_utils_test.py:39:                     misc_utils.format_bpe_text(bpe_line.split(b" ")))
nmt-master/nmt/utils/misc_utils_test.py:41:  def testFormatSPMText(self):
nmt-master/nmt/utils/misc_utils_test.py:42:    spm_line = u"\u2581This \u2581is \u2581a \u2581 te st .".encode("utf-8")
nmt-master/nmt/utils/misc_utils_test.py:43:    expected_result = b"This is a test."
nmt-master/nmt/utils/misc_utils_test.py:44:    self.assertEqual(expected_result,
nmt-master/nmt/utils/misc_utils_test.py:45:                     misc_utils.format_spm_text(spm_line.split(b" ")))
nmt-master/nmt/utils/misc_utils_test.py:48:if __name__ == "__main__":
nmt-master/nmt/utils/misc_utils_test.py:49:  tf.test.main()
nmt-master/nmt/utils/nmt_utils.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/utils/nmt_utils.py:2:#
nmt-master/nmt/utils/nmt_utils.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/utils/nmt_utils.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/utils/nmt_utils.py:5:# You may obtain a copy of the License at
nmt-master/nmt/utils/nmt_utils.py:6:#
nmt-master/nmt/utils/nmt_utils.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/utils/nmt_utils.py:8:#
nmt-master/nmt/utils/nmt_utils.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/utils/nmt_utils.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/utils/nmt_utils.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/utils/nmt_utils.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/utils/nmt_utils.py:13:# limitations under the License.
nmt-master/nmt/utils/nmt_utils.py:14:# ==============================================================================
nmt-master/nmt/utils/nmt_utils.py:16:"""Utility functions specifically for NMT."""
nmt-master/nmt/utils/nmt_utils.py:17:from __future__ import print_function
nmt-master/nmt/utils/nmt_utils.py:19:import codecs
nmt-master/nmt/utils/nmt_utils.py:20:import time
nmt-master/nmt/utils/nmt_utils.py:21:import numpy as np
nmt-master/nmt/utils/nmt_utils.py:22:import tensorflow as tf
nmt-master/nmt/utils/nmt_utils.py:24:from ..utils import evaluation_utils
nmt-master/nmt/utils/nmt_utils.py:25:from ..utils import misc_utils as utils
nmt-master/nmt/utils/nmt_utils.py:27:__all__ = ["decode_and_evaluate", "get_translation"]
nmt-master/nmt/utils/nmt_utils.py:30:def decode_and_evaluate(name,
nmt-master/nmt/utils/nmt_utils.py:31:                        model,
nmt-master/nmt/utils/nmt_utils.py:32:                        sess,
nmt-master/nmt/utils/nmt_utils.py:33:                        trans_file,
nmt-master/nmt/utils/nmt_utils.py:34:                        ref_file,
nmt-master/nmt/utils/nmt_utils.py:35:                        metrics,
nmt-master/nmt/utils/nmt_utils.py:36:                        subword_option,
nmt-master/nmt/utils/nmt_utils.py:37:                        beam_width,
nmt-master/nmt/utils/nmt_utils.py:38:                        tgt_eos,
nmt-master/nmt/utils/nmt_utils.py:39:                        num_translations_per_input=1,
nmt-master/nmt/utils/nmt_utils.py:40:                        decode=True,
nmt-master/nmt/utils/nmt_utils.py:41:                        infer_mode="greedy"):
nmt-master/nmt/utils/nmt_utils.py:42:  """Decode a test set and compute a score according to the evaluation task."""
nmt-master/nmt/utils/nmt_utils.py:43:  # Decode
nmt-master/nmt/utils/nmt_utils.py:44:  if decode:
nmt-master/nmt/utils/nmt_utils.py:45:    utils.print_out("  decoding to output %s" % trans_file)
nmt-master/nmt/utils/nmt_utils.py:47:    start_time = time.time()
nmt-master/nmt/utils/nmt_utils.py:48:    num_sentences = 0
nmt-master/nmt/utils/nmt_utils.py:49:    with codecs.getwriter("utf-8")(
nmt-master/nmt/utils/nmt_utils.py:50:        tf.gfile.GFile(trans_file, mode="wb")) as trans_f:
nmt-master/nmt/utils/nmt_utils.py:51:      trans_f.write("")  # Write empty string to ensure file is created.
nmt-master/nmt/utils/nmt_utils.py:53:      if infer_mode == "greedy":
nmt-master/nmt/utils/nmt_utils.py:54:        num_translations_per_input = 1
nmt-master/nmt/utils/nmt_utils.py:55:      elif infer_mode == "beam_search":
nmt-master/nmt/utils/nmt_utils.py:56:        num_translations_per_input = min(num_translations_per_input, beam_width)
nmt-master/nmt/utils/nmt_utils.py:58:      while True:
nmt-master/nmt/utils/nmt_utils.py:59:        try:
nmt-master/nmt/utils/nmt_utils.py:60:          nmt_outputs, _ = model.decode(sess)
nmt-master/nmt/utils/nmt_utils.py:61:          if infer_mode != "beam_search":
nmt-master/nmt/utils/nmt_utils.py:62:            nmt_outputs = np.expand_dims(nmt_outputs, 0)
nmt-master/nmt/utils/nmt_utils.py:64:          batch_size = nmt_outputs.shape[1]
nmt-master/nmt/utils/nmt_utils.py:65:          num_sentences += batch_size
nmt-master/nmt/utils/nmt_utils.py:67:          for sent_id in range(batch_size):
nmt-master/nmt/utils/nmt_utils.py:68:            for beam_id in range(num_translations_per_input):
nmt-master/nmt/utils/nmt_utils.py:69:              translation = get_translation(
nmt-master/nmt/utils/nmt_utils.py:70:                  nmt_outputs[beam_id],
nmt-master/nmt/utils/nmt_utils.py:71:                  sent_id,
nmt-master/nmt/utils/nmt_utils.py:72:                  tgt_eos=tgt_eos,
nmt-master/nmt/utils/nmt_utils.py:73:                  subword_option=subword_option)
nmt-master/nmt/utils/nmt_utils.py:74:              trans_f.write((translation + b"\n").decode("utf-8"))
nmt-master/nmt/utils/nmt_utils.py:75:        except tf.errors.OutOfRangeError:
nmt-master/nmt/utils/nmt_utils.py:76:          utils.print_time(
nmt-master/nmt/utils/nmt_utils.py:77:              "  done, num sentences %d, num translations per input %d" %
nmt-master/nmt/utils/nmt_utils.py:78:              (num_sentences, num_translations_per_input), start_time)
nmt-master/nmt/utils/nmt_utils.py:79:          break
nmt-master/nmt/utils/nmt_utils.py:81:  # Evaluation
nmt-master/nmt/utils/nmt_utils.py:82:  evaluation_scores = {}
nmt-master/nmt/utils/nmt_utils.py:83:  if ref_file and tf.gfile.Exists(trans_file):
nmt-master/nmt/utils/nmt_utils.py:84:    for metric in metrics:
nmt-master/nmt/utils/nmt_utils.py:85:      score = evaluation_utils.evaluate(
nmt-master/nmt/utils/nmt_utils.py:86:          ref_file,
nmt-master/nmt/utils/nmt_utils.py:87:          trans_file,
nmt-master/nmt/utils/nmt_utils.py:88:          metric,
nmt-master/nmt/utils/nmt_utils.py:89:          subword_option=subword_option)
nmt-master/nmt/utils/nmt_utils.py:90:      evaluation_scores[metric] = score
nmt-master/nmt/utils/nmt_utils.py:91:      utils.print_out("  %s %s: %.1f" % (metric, name, score))
nmt-master/nmt/utils/nmt_utils.py:93:  return evaluation_scores
nmt-master/nmt/utils/nmt_utils.py:96:def get_translation(nmt_outputs, sent_id, tgt_eos, subword_option):
nmt-master/nmt/utils/nmt_utils.py:97:  """Given batch decoding outputs, select a sentence and turn to text."""
nmt-master/nmt/utils/nmt_utils.py:98:  if tgt_eos: tgt_eos = tgt_eos.encode("utf-8")
nmt-master/nmt/utils/nmt_utils.py:99:  # Select a sentence
nmt-master/nmt/utils/nmt_utils.py:100:  output = nmt_outputs[sent_id, :].tolist()
nmt-master/nmt/utils/nmt_utils.py:102:  # If there is an eos symbol in outputs, cut them at that point.
nmt-master/nmt/utils/nmt_utils.py:103:  if tgt_eos and tgt_eos in output:
nmt-master/nmt/utils/nmt_utils.py:104:    output = output[:output.index(tgt_eos)]
nmt-master/nmt/utils/nmt_utils.py:106:  if subword_option == "bpe":  # BPE
nmt-master/nmt/utils/nmt_utils.py:107:    translation = utils.format_bpe_text(output)
nmt-master/nmt/utils/nmt_utils.py:108:  elif subword_option == "spm":  # SPM
nmt-master/nmt/utils/nmt_utils.py:109:    translation = utils.format_spm_text(output)
nmt-master/nmt/utils/nmt_utils.py:110:  else:
nmt-master/nmt/utils/nmt_utils.py:111:    translation = utils.format_text(output)
nmt-master/nmt/utils/nmt_utils.py:113:  return translation
nmt-master/nmt/utils/standard_hparams_utils.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/utils/standard_hparams_utils.py:2:#
nmt-master/nmt/utils/standard_hparams_utils.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/utils/standard_hparams_utils.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/utils/standard_hparams_utils.py:5:# You may obtain a copy of the License at
nmt-master/nmt/utils/standard_hparams_utils.py:6:#
nmt-master/nmt/utils/standard_hparams_utils.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/utils/standard_hparams_utils.py:8:#
nmt-master/nmt/utils/standard_hparams_utils.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/utils/standard_hparams_utils.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/utils/standard_hparams_utils.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/utils/standard_hparams_utils.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/utils/standard_hparams_utils.py:13:# limitations under the License.
nmt-master/nmt/utils/standard_hparams_utils.py:14:# ==============================================================================
nmt-master/nmt/utils/standard_hparams_utils.py:16:"""standard hparams utils."""
nmt-master/nmt/utils/standard_hparams_utils.py:18:from __future__ import absolute_import
nmt-master/nmt/utils/standard_hparams_utils.py:19:from __future__ import division
nmt-master/nmt/utils/standard_hparams_utils.py:20:from __future__ import print_function
nmt-master/nmt/utils/standard_hparams_utils.py:22:import tensorflow as tf
nmt-master/nmt/utils/standard_hparams_utils.py:25:def create_standard_hparams():
nmt-master/nmt/utils/standard_hparams_utils.py:26:  return tf.contrib.training.HParams(
nmt-master/nmt/utils/standard_hparams_utils.py:27:      # Data
nmt-master/nmt/utils/standard_hparams_utils.py:28:      src="",
nmt-master/nmt/utils/standard_hparams_utils.py:29:      tgt="",
nmt-master/nmt/utils/standard_hparams_utils.py:30:      train_prefix="",
nmt-master/nmt/utils/standard_hparams_utils.py:31:      dev_prefix="",
nmt-master/nmt/utils/standard_hparams_utils.py:32:      test_prefix="",
nmt-master/nmt/utils/standard_hparams_utils.py:33:      vocab_prefix="",
nmt-master/nmt/utils/standard_hparams_utils.py:34:      embed_prefix="",
nmt-master/nmt/utils/standard_hparams_utils.py:35:      out_dir="",
nmt-master/nmt/utils/standard_hparams_utils.py:37:      # Networks
nmt-master/nmt/utils/standard_hparams_utils.py:38:      num_units=512,
nmt-master/nmt/utils/standard_hparams_utils.py:39:      num_encoder_layers=2,
nmt-master/nmt/utils/standard_hparams_utils.py:40:      num_decoder_layers=2,
nmt-master/nmt/utils/standard_hparams_utils.py:41:      dropout=0.2,
nmt-master/nmt/utils/standard_hparams_utils.py:42:      unit_type="lstm",
nmt-master/nmt/utils/standard_hparams_utils.py:43:      encoder_type="bi",
nmt-master/nmt/utils/standard_hparams_utils.py:44:      residual=False,
nmt-master/nmt/utils/standard_hparams_utils.py:45:      time_major=True,
nmt-master/nmt/utils/standard_hparams_utils.py:46:      num_embeddings_partitions=0,
nmt-master/nmt/utils/standard_hparams_utils.py:47:      num_enc_emb_partitions=0,
nmt-master/nmt/utils/standard_hparams_utils.py:48:      num_dec_emb_partitions=0,
nmt-master/nmt/utils/standard_hparams_utils.py:50:      # Attention mechanisms
nmt-master/nmt/utils/standard_hparams_utils.py:51:      attention="scaled_luong",
nmt-master/nmt/utils/standard_hparams_utils.py:52:      attention_architecture="standard",
nmt-master/nmt/utils/standard_hparams_utils.py:53:      output_attention=True,
nmt-master/nmt/utils/standard_hparams_utils.py:54:      pass_hidden_state=True,
nmt-master/nmt/utils/standard_hparams_utils.py:56:      # Train
nmt-master/nmt/utils/standard_hparams_utils.py:57:      optimizer="sgd",
nmt-master/nmt/utils/standard_hparams_utils.py:58:      batch_size=128,
nmt-master/nmt/utils/standard_hparams_utils.py:59:      init_op="uniform",
nmt-master/nmt/utils/standard_hparams_utils.py:60:      init_weight=0.1,
nmt-master/nmt/utils/standard_hparams_utils.py:61:      max_gradient_norm=5.0,
nmt-master/nmt/utils/standard_hparams_utils.py:62:      learning_rate=1.0,
nmt-master/nmt/utils/standard_hparams_utils.py:63:      warmup_steps=0,
nmt-master/nmt/utils/standard_hparams_utils.py:64:      warmup_scheme="t2t",
nmt-master/nmt/utils/standard_hparams_utils.py:65:      decay_scheme="luong234",
nmt-master/nmt/utils/standard_hparams_utils.py:66:      colocate_gradients_with_ops=True,
nmt-master/nmt/utils/standard_hparams_utils.py:67:      num_train_steps=12000,
nmt-master/nmt/utils/standard_hparams_utils.py:68:      num_sampled_softmax=0,
nmt-master/nmt/utils/standard_hparams_utils.py:70:      # Data constraints
nmt-master/nmt/utils/standard_hparams_utils.py:71:      num_buckets=5,
nmt-master/nmt/utils/standard_hparams_utils.py:72:      max_train=0,
nmt-master/nmt/utils/standard_hparams_utils.py:73:      src_max_len=50,
nmt-master/nmt/utils/standard_hparams_utils.py:74:      tgt_max_len=50,
nmt-master/nmt/utils/standard_hparams_utils.py:75:      src_max_len_infer=0,
nmt-master/nmt/utils/standard_hparams_utils.py:76:      tgt_max_len_infer=0,
nmt-master/nmt/utils/standard_hparams_utils.py:78:      # Data format
nmt-master/nmt/utils/standard_hparams_utils.py:79:      sos="<s>",
nmt-master/nmt/utils/standard_hparams_utils.py:80:      eos="</s>",
nmt-master/nmt/utils/standard_hparams_utils.py:81:      subword_option="",
nmt-master/nmt/utils/standard_hparams_utils.py:82:      use_char_encode=False,
nmt-master/nmt/utils/standard_hparams_utils.py:83:      check_special_token=True,
nmt-master/nmt/utils/standard_hparams_utils.py:85:      # Misc
nmt-master/nmt/utils/standard_hparams_utils.py:86:      forget_bias=1.0,
nmt-master/nmt/utils/standard_hparams_utils.py:87:      num_gpus=1,
nmt-master/nmt/utils/standard_hparams_utils.py:88:      epoch_step=0,  # record where we were within an epoch.
nmt-master/nmt/utils/standard_hparams_utils.py:89:      steps_per_stats=100,
nmt-master/nmt/utils/standard_hparams_utils.py:90:      steps_per_external_eval=0,
nmt-master/nmt/utils/standard_hparams_utils.py:91:      share_vocab=False,
nmt-master/nmt/utils/standard_hparams_utils.py:92:      metrics=["bleu"],
nmt-master/nmt/utils/standard_hparams_utils.py:93:      log_device_placement=False,
nmt-master/nmt/utils/standard_hparams_utils.py:94:      random_seed=None,
nmt-master/nmt/utils/standard_hparams_utils.py:95:      # only enable beam search during inference when beam_width > 0.
nmt-master/nmt/utils/standard_hparams_utils.py:96:      beam_width=0,
nmt-master/nmt/utils/standard_hparams_utils.py:97:      length_penalty_weight=0.0,
nmt-master/nmt/utils/standard_hparams_utils.py:98:      coverage_penalty_weight=0.0,
nmt-master/nmt/utils/standard_hparams_utils.py:99:      override_loaded_hparams=True,
nmt-master/nmt/utils/standard_hparams_utils.py:100:      num_keep_ckpts=5,
nmt-master/nmt/utils/standard_hparams_utils.py:101:      avg_ckpts=False,
nmt-master/nmt/utils/standard_hparams_utils.py:103:      # For inference
nmt-master/nmt/utils/standard_hparams_utils.py:104:      inference_indices=None,
nmt-master/nmt/utils/standard_hparams_utils.py:105:      infer_batch_size=32,
nmt-master/nmt/utils/standard_hparams_utils.py:106:      sampling_temperature=0.0,
nmt-master/nmt/utils/standard_hparams_utils.py:107:      num_translations_per_input=1,
nmt-master/nmt/utils/standard_hparams_utils.py:108:      infer_mode="greedy",
nmt-master/nmt/utils/standard_hparams_utils.py:110:      # Language model
nmt-master/nmt/utils/standard_hparams_utils.py:111:      language_model=False,
nmt-master/nmt/utils/standard_hparams_utils.py:112:  )
nmt-master/nmt/utils/vocab_utils.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/utils/vocab_utils.py:2:#
nmt-master/nmt/utils/vocab_utils.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/utils/vocab_utils.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/utils/vocab_utils.py:5:# You may obtain a copy of the License at
nmt-master/nmt/utils/vocab_utils.py:6:#
nmt-master/nmt/utils/vocab_utils.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/utils/vocab_utils.py:8:#
nmt-master/nmt/utils/vocab_utils.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/utils/vocab_utils.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/utils/vocab_utils.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/utils/vocab_utils.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/utils/vocab_utils.py:13:# limitations under the License.
nmt-master/nmt/utils/vocab_utils.py:14:# ==============================================================================
nmt-master/nmt/utils/vocab_utils.py:16:"""Utility to handle vocabularies."""
nmt-master/nmt/utils/vocab_utils.py:18:from __future__ import absolute_import
nmt-master/nmt/utils/vocab_utils.py:19:from __future__ import division
nmt-master/nmt/utils/vocab_utils.py:20:from __future__ import print_function
nmt-master/nmt/utils/vocab_utils.py:22:import codecs
nmt-master/nmt/utils/vocab_utils.py:23:import os
nmt-master/nmt/utils/vocab_utils.py:24:import tensorflow as tf
nmt-master/nmt/utils/vocab_utils.py:26:from tensorflow.python.ops import lookup_ops
nmt-master/nmt/utils/vocab_utils.py:28:from ..utils import misc_utils as utils
nmt-master/nmt/utils/vocab_utils.py:30:# word level special token
nmt-master/nmt/utils/vocab_utils.py:31:UNK = "<unk>"
nmt-master/nmt/utils/vocab_utils.py:32:SOS = "<s>"
nmt-master/nmt/utils/vocab_utils.py:33:EOS = "</s>"
nmt-master/nmt/utils/vocab_utils.py:34:UNK_ID = 0
nmt-master/nmt/utils/vocab_utils.py:36:# char ids 0-255 come from utf-8 encoding bytes
nmt-master/nmt/utils/vocab_utils.py:37:# assign 256-300 to special chars
nmt-master/nmt/utils/vocab_utils.py:38:BOS_CHAR_ID = 256  # <begin sentence>
nmt-master/nmt/utils/vocab_utils.py:39:EOS_CHAR_ID = 257  # <end sentence>
nmt-master/nmt/utils/vocab_utils.py:40:BOW_CHAR_ID = 258  # <begin word>
nmt-master/nmt/utils/vocab_utils.py:41:EOW_CHAR_ID = 259  # <end word>
nmt-master/nmt/utils/vocab_utils.py:42:PAD_CHAR_ID = 260  # <padding>
nmt-master/nmt/utils/vocab_utils.py:44:DEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.
nmt-master/nmt/utils/vocab_utils.py:47:def _string_to_bytes(text, max_length):
nmt-master/nmt/utils/vocab_utils.py:48:  """Given string and length, convert to byte seq of at most max_length.
nmt-master/nmt/utils/vocab_utils.py:50:  This process mimics docqa/elmo's preprocessing:
nmt-master/nmt/utils/vocab_utils.py:51:  https://github.com/allenai/document-qa/blob/master/docqa/elmo/data.py
nmt-master/nmt/utils/vocab_utils.py:53:  Note that we make use of BOS_CHAR_ID and EOS_CHAR_ID in iterator_utils.py & 
nmt-master/nmt/utils/vocab_utils.py:54:  our usage differs from docqa/elmo.
nmt-master/nmt/utils/vocab_utils.py:56:  Args:
nmt-master/nmt/utils/vocab_utils.py:57:    text: tf.string tensor of shape []
nmt-master/nmt/utils/vocab_utils.py:58:    max_length: max number of chars for each word.
nmt-master/nmt/utils/vocab_utils.py:60:  Returns:
nmt-master/nmt/utils/vocab_utils.py:61:    A tf.int32 tensor of the byte encoded text.
nmt-master/nmt/utils/vocab_utils.py:62:  """
nmt-master/nmt/utils/vocab_utils.py:63:  byte_ids = tf.to_int32(tf.decode_raw(text, tf.uint8))
nmt-master/nmt/utils/vocab_utils.py:64:  byte_ids = byte_ids[:max_length - 2]
nmt-master/nmt/utils/vocab_utils.py:65:  padding = tf.fill([max_length - tf.shape(byte_ids)[0] - 2], PAD_CHAR_ID)
nmt-master/nmt/utils/vocab_utils.py:66:  byte_ids = tf.concat(
nmt-master/nmt/utils/vocab_utils.py:67:      [[BOW_CHAR_ID], byte_ids, [EOW_CHAR_ID], padding], axis=0)
nmt-master/nmt/utils/vocab_utils.py:68:  tf.logging.info(byte_ids)
nmt-master/nmt/utils/vocab_utils.py:70:  byte_ids = tf.reshape(byte_ids, [max_length])
nmt-master/nmt/utils/vocab_utils.py:71:  tf.logging.info(byte_ids.get_shape().as_list())
nmt-master/nmt/utils/vocab_utils.py:72:  return byte_ids + 1
nmt-master/nmt/utils/vocab_utils.py:75:def tokens_to_bytes(tokens):
nmt-master/nmt/utils/vocab_utils.py:76:  """Given a sequence of strings, map to sequence of bytes.
nmt-master/nmt/utils/vocab_utils.py:78:  Args:
nmt-master/nmt/utils/vocab_utils.py:79:    tokens: A tf.string tensor
nmt-master/nmt/utils/vocab_utils.py:81:  Returns:
nmt-master/nmt/utils/vocab_utils.py:82:    A tensor of shape words.shape + [bytes_per_word] containing byte versions
nmt-master/nmt/utils/vocab_utils.py:83:    of each word.
nmt-master/nmt/utils/vocab_utils.py:84:  """
nmt-master/nmt/utils/vocab_utils.py:85:  bytes_per_word = DEFAULT_CHAR_MAXLEN
nmt-master/nmt/utils/vocab_utils.py:86:  with tf.device("/cpu:0"):
nmt-master/nmt/utils/vocab_utils.py:87:    tf.assert_rank(tokens, 1)
nmt-master/nmt/utils/vocab_utils.py:88:    shape = tf.shape(tokens)
nmt-master/nmt/utils/vocab_utils.py:89:    tf.logging.info(tokens)
nmt-master/nmt/utils/vocab_utils.py:90:    tokens_flat = tf.reshape(tokens, [-1])
nmt-master/nmt/utils/vocab_utils.py:91:    as_bytes_flat = tf.map_fn(
nmt-master/nmt/utils/vocab_utils.py:92:        fn=lambda x: _string_to_bytes(x, max_length=bytes_per_word),
nmt-master/nmt/utils/vocab_utils.py:93:        elems=tokens_flat,
nmt-master/nmt/utils/vocab_utils.py:94:        dtype=tf.int32,
nmt-master/nmt/utils/vocab_utils.py:95:        back_prop=False)
nmt-master/nmt/utils/vocab_utils.py:96:    tf.logging.info(as_bytes_flat)
nmt-master/nmt/utils/vocab_utils.py:97:    as_bytes = tf.reshape(as_bytes_flat, [shape[0], bytes_per_word])
nmt-master/nmt/utils/vocab_utils.py:98:  return as_bytes
nmt-master/nmt/utils/vocab_utils.py:101:def load_vocab(vocab_file):
nmt-master/nmt/utils/vocab_utils.py:102:  vocab = []
nmt-master/nmt/utils/vocab_utils.py:103:  with codecs.getreader("utf-8")(tf.gfile.GFile(vocab_file, "rb")) as f:
nmt-master/nmt/utils/vocab_utils.py:104:    vocab_size = 0
nmt-master/nmt/utils/vocab_utils.py:105:    for word in f:
nmt-master/nmt/utils/vocab_utils.py:106:      vocab_size += 1
nmt-master/nmt/utils/vocab_utils.py:107:      vocab.append(word.strip())
nmt-master/nmt/utils/vocab_utils.py:108:  return vocab, vocab_size
nmt-master/nmt/utils/vocab_utils.py:111:def check_vocab(vocab_file, out_dir, check_special_token=True, sos=None,
nmt-master/nmt/utils/vocab_utils.py:112:                eos=None, unk=None):
nmt-master/nmt/utils/vocab_utils.py:113:  """Check if vocab_file doesn't exist, create from corpus_file."""
nmt-master/nmt/utils/vocab_utils.py:114:  if tf.gfile.Exists(vocab_file):
nmt-master/nmt/utils/vocab_utils.py:115:    utils.print_out("# Vocab file %s exists" % vocab_file)
nmt-master/nmt/utils/vocab_utils.py:116:    vocab, vocab_size = load_vocab(vocab_file)
nmt-master/nmt/utils/vocab_utils.py:117:    if check_special_token:
nmt-master/nmt/utils/vocab_utils.py:118:      # Verify if the vocab starts with unk, sos, eos
nmt-master/nmt/utils/vocab_utils.py:119:      # If not, prepend those tokens & generate a new vocab file
nmt-master/nmt/utils/vocab_utils.py:120:      if not unk: unk = UNK
nmt-master/nmt/utils/vocab_utils.py:121:      if not sos: sos = SOS
nmt-master/nmt/utils/vocab_utils.py:122:      if not eos: eos = EOS
nmt-master/nmt/utils/vocab_utils.py:123:      assert len(vocab) >= 3
nmt-master/nmt/utils/vocab_utils.py:124:      if vocab[0] != unk or vocab[1] != sos or vocab[2] != eos:
nmt-master/nmt/utils/vocab_utils.py:125:        utils.print_out("The first 3 vocab words [%s, %s, %s]"
nmt-master/nmt/utils/vocab_utils.py:126:                        " are not [%s, %s, %s]" %
nmt-master/nmt/utils/vocab_utils.py:127:                        (vocab[0], vocab[1], vocab[2], unk, sos, eos))
nmt-master/nmt/utils/vocab_utils.py:128:        vocab = [unk, sos, eos] + vocab
nmt-master/nmt/utils/vocab_utils.py:129:        vocab_size += 3
nmt-master/nmt/utils/vocab_utils.py:130:        new_vocab_file = os.path.join(out_dir, os.path.basename(vocab_file))
nmt-master/nmt/utils/vocab_utils.py:131:        with codecs.getwriter("utf-8")(
nmt-master/nmt/utils/vocab_utils.py:132:            tf.gfile.GFile(new_vocab_file, "wb")) as f:
nmt-master/nmt/utils/vocab_utils.py:133:          for word in vocab:
nmt-master/nmt/utils/vocab_utils.py:134:            f.write("%s\n" % word)
nmt-master/nmt/utils/vocab_utils.py:135:        vocab_file = new_vocab_file
nmt-master/nmt/utils/vocab_utils.py:136:  else:
nmt-master/nmt/utils/vocab_utils.py:137:    raise ValueError("vocab_file '%s' does not exist." % vocab_file)
nmt-master/nmt/utils/vocab_utils.py:139:  vocab_size = len(vocab)
nmt-master/nmt/utils/vocab_utils.py:140:  return vocab_size, vocab_file
nmt-master/nmt/utils/vocab_utils.py:143:def create_vocab_tables(src_vocab_file, tgt_vocab_file, share_vocab):
nmt-master/nmt/utils/vocab_utils.py:144:  """Creates vocab tables for src_vocab_file and tgt_vocab_file."""
nmt-master/nmt/utils/vocab_utils.py:145:  src_vocab_table = lookup_ops.index_table_from_file(
nmt-master/nmt/utils/vocab_utils.py:146:      src_vocab_file, default_value=UNK_ID)
nmt-master/nmt/utils/vocab_utils.py:147:  if share_vocab:
nmt-master/nmt/utils/vocab_utils.py:148:    tgt_vocab_table = src_vocab_table
nmt-master/nmt/utils/vocab_utils.py:149:  else:
nmt-master/nmt/utils/vocab_utils.py:150:    tgt_vocab_table = lookup_ops.index_table_from_file(
nmt-master/nmt/utils/vocab_utils.py:151:        tgt_vocab_file, default_value=UNK_ID)
nmt-master/nmt/utils/vocab_utils.py:152:  return src_vocab_table, tgt_vocab_table
nmt-master/nmt/utils/vocab_utils.py:155:def load_embed_txt(embed_file):
nmt-master/nmt/utils/vocab_utils.py:156:  """Load embed_file into a python dictionary.
nmt-master/nmt/utils/vocab_utils.py:158:  Note: the embed_file should be a Glove/word2vec formatted txt file. Assuming
nmt-master/nmt/utils/vocab_utils.py:159:  Here is an exampe assuming embed_size=5:
nmt-master/nmt/utils/vocab_utils.py:161:  the -0.071549 0.093459 0.023738 -0.090339 0.056123
nmt-master/nmt/utils/vocab_utils.py:162:  to 0.57346 0.5417 -0.23477 -0.3624 0.4037
nmt-master/nmt/utils/vocab_utils.py:163:  and 0.20327 0.47348 0.050877 0.002103 0.060547
nmt-master/nmt/utils/vocab_utils.py:165:  For word2vec format, the first line will be: <num_words> <emb_size>.
nmt-master/nmt/utils/vocab_utils.py:167:  Args:
nmt-master/nmt/utils/vocab_utils.py:168:    embed_file: file path to the embedding file.
nmt-master/nmt/utils/vocab_utils.py:169:  Returns:
nmt-master/nmt/utils/vocab_utils.py:170:    a dictionary that maps word to vector, and the size of embedding dimensions.
nmt-master/nmt/utils/vocab_utils.py:171:  """
nmt-master/nmt/utils/vocab_utils.py:172:  emb_dict = dict()
nmt-master/nmt/utils/vocab_utils.py:173:  emb_size = None
nmt-master/nmt/utils/vocab_utils.py:175:  is_first_line = True
nmt-master/nmt/utils/vocab_utils.py:176:  with codecs.getreader("utf-8")(tf.gfile.GFile(embed_file, "rb")) as f:
nmt-master/nmt/utils/vocab_utils.py:177:    for line in f:
nmt-master/nmt/utils/vocab_utils.py:178:      tokens = line.rstrip().split(" ")
nmt-master/nmt/utils/vocab_utils.py:179:      if is_first_line:
nmt-master/nmt/utils/vocab_utils.py:180:        is_first_line = False
nmt-master/nmt/utils/vocab_utils.py:181:        if len(tokens) == 2:  # header line
nmt-master/nmt/utils/vocab_utils.py:182:          emb_size = int(tokens[1])
nmt-master/nmt/utils/vocab_utils.py:183:          continue
nmt-master/nmt/utils/vocab_utils.py:184:      word = tokens[0]
nmt-master/nmt/utils/vocab_utils.py:185:      vec = list(map(float, tokens[1:]))
nmt-master/nmt/utils/vocab_utils.py:186:      emb_dict[word] = vec
nmt-master/nmt/utils/vocab_utils.py:187:      if emb_size:
nmt-master/nmt/utils/vocab_utils.py:188:        if emb_size != len(vec):
nmt-master/nmt/utils/vocab_utils.py:189:          utils.print_out(
nmt-master/nmt/utils/vocab_utils.py:190:              "Ignoring %s since embeding size is inconsistent." % word)
nmt-master/nmt/utils/vocab_utils.py:191:          del emb_dict[word]
nmt-master/nmt/utils/vocab_utils.py:192:      else:
nmt-master/nmt/utils/vocab_utils.py:193:        emb_size = len(vec)
nmt-master/nmt/utils/vocab_utils.py:194:  return emb_dict, emb_size
nmt-master/nmt/utils/vocab_utils_test.py:1:# Copyright 2017 Google Inc. All Rights Reserved.
nmt-master/nmt/utils/vocab_utils_test.py:2:#
nmt-master/nmt/utils/vocab_utils_test.py:3:# Licensed under the Apache License, Version 2.0 (the "License");
nmt-master/nmt/utils/vocab_utils_test.py:4:# you may not use this file except in compliance with the License.
nmt-master/nmt/utils/vocab_utils_test.py:5:# You may obtain a copy of the License at
nmt-master/nmt/utils/vocab_utils_test.py:6:#
nmt-master/nmt/utils/vocab_utils_test.py:7:#     http://www.apache.org/licenses/LICENSE-2.0
nmt-master/nmt/utils/vocab_utils_test.py:8:#
nmt-master/nmt/utils/vocab_utils_test.py:9:# Unless required by applicable law or agreed to in writing, software
nmt-master/nmt/utils/vocab_utils_test.py:10:# distributed under the License is distributed on an "AS IS" BASIS,
nmt-master/nmt/utils/vocab_utils_test.py:11:# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
nmt-master/nmt/utils/vocab_utils_test.py:12:# See the License for the specific language governing permissions and
nmt-master/nmt/utils/vocab_utils_test.py:13:# limitations under the License.
nmt-master/nmt/utils/vocab_utils_test.py:14:# ==============================================================================
nmt-master/nmt/utils/vocab_utils_test.py:16:"""Tests for vocab_utils."""
nmt-master/nmt/utils/vocab_utils_test.py:18:from __future__ import absolute_import
nmt-master/nmt/utils/vocab_utils_test.py:19:from __future__ import division
nmt-master/nmt/utils/vocab_utils_test.py:20:from __future__ import print_function
nmt-master/nmt/utils/vocab_utils_test.py:22:import codecs
nmt-master/nmt/utils/vocab_utils_test.py:23:import os
nmt-master/nmt/utils/vocab_utils_test.py:24:import tensorflow as tf
nmt-master/nmt/utils/vocab_utils_test.py:26:from ..utils import vocab_utils
nmt-master/nmt/utils/vocab_utils_test.py:29:class VocabUtilsTest(tf.test.TestCase):
nmt-master/nmt/utils/vocab_utils_test.py:31:  def testCheckVocab(self):
nmt-master/nmt/utils/vocab_utils_test.py:32:    # Create a vocab file
nmt-master/nmt/utils/vocab_utils_test.py:33:    vocab_dir = os.path.join(tf.test.get_temp_dir(), "vocab_dir")
nmt-master/nmt/utils/vocab_utils_test.py:34:    os.makedirs(vocab_dir)
nmt-master/nmt/utils/vocab_utils_test.py:35:    vocab_file = os.path.join(vocab_dir, "vocab_file")
nmt-master/nmt/utils/vocab_utils_test.py:36:    vocab = ["a", "b", "c"]
nmt-master/nmt/utils/vocab_utils_test.py:37:    with codecs.getwriter("utf-8")(tf.gfile.GFile(vocab_file, "wb")) as f:
nmt-master/nmt/utils/vocab_utils_test.py:38:      for word in vocab:
nmt-master/nmt/utils/vocab_utils_test.py:39:        f.write("%s\n" % word)
nmt-master/nmt/utils/vocab_utils_test.py:41:    # Call vocab_utils
nmt-master/nmt/utils/vocab_utils_test.py:42:    out_dir = os.path.join(tf.test.get_temp_dir(), "out_dir")
nmt-master/nmt/utils/vocab_utils_test.py:43:    os.makedirs(out_dir)
nmt-master/nmt/utils/vocab_utils_test.py:44:    vocab_size, new_vocab_file = vocab_utils.check_vocab(
nmt-master/nmt/utils/vocab_utils_test.py:45:        vocab_file, out_dir)
nmt-master/nmt/utils/vocab_utils_test.py:47:    # Assert: we expect the code to add  <unk>, <s>, </s> and
nmt-master/nmt/utils/vocab_utils_test.py:48:    # create a new vocab file
nmt-master/nmt/utils/vocab_utils_test.py:49:    self.assertEqual(len(vocab) + 3, vocab_size)
nmt-master/nmt/utils/vocab_utils_test.py:50:    self.assertEqual(os.path.join(out_dir, "vocab_file"), new_vocab_file)
nmt-master/nmt/utils/vocab_utils_test.py:51:    new_vocab, _ = vocab_utils.load_vocab(new_vocab_file)
nmt-master/nmt/utils/vocab_utils_test.py:52:    self.assertEqual(
nmt-master/nmt/utils/vocab_utils_test.py:53:        [vocab_utils.UNK, vocab_utils.SOS, vocab_utils.EOS] + vocab, new_vocab)
nmt-master/nmt/utils/vocab_utils_test.py:56:if __name__ == "__main__":
nmt-master/nmt/utils/vocab_utils_test.py:57:  tf.test.main()
